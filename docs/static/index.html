<!doctype html>
<html lang="ru">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>CV Time — статическая версия (стр. 1/4)</title>
  <meta name="description" content="Статическая версия зеркала Telegram-канала" />
  <link rel="icon" href="../favicon.ico?v=2026-02-11T17%3A39%3A52Z" sizes="any" />
  <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32.png?v=2026-02-11T17%3A39%3A52Z" />
  <link rel="apple-touch-icon" href="../apple-touch-icon.png?v=2026-02-11T17%3A39%3A52Z" />

  <link rel="stylesheet" href="../style.css" />
  <script src="../metrika.js"></script>
</head>
<body>
  <header class="header">
    <div class="container">
      <div class="title-grid">
        <a class="grid-avatar" href="#" target="_blank" rel="noopener">
          <img id="channelAvatar" class="channel-avatar" src="../assets/channel_avatar.jpg" alt="Аватар канала"  />
        </a>
        <div class="grid-main">
          <div class="title-head">
            <div class="title-left">
              <a class="badge-chip" id="siteTitleWrap" href="#" target="_blank" rel="noopener"><h1 id="siteTitle">CV Time</h1></a>
            </div>
            <div class="hero-actions">
              <a id="subscribeBtn" class="subscribe-btn" href="https://t.me/+JoULEedmHyE5MmYy" target="_blank" rel="noopener" >Подписаться</a>
              <a class="icon-btn" href="../" aria-label="Перейти к динамической версии">↺</a>
              <button id="themeToggle" class="icon-btn" type="button" aria-label="Переключить тему"></button>
            </div>
          </div>
        </div>
        <div class="controls"></div>
      </div>
    </div>
  </header>

  
  <div id="promoBanner" class="promo-banner" hidden>
    <div class="container promo-inner">
      <span class="promo-text"><a href="https://t.me/addlist/5NH3RoVejEI1MGEy">Подпишись на все наши ML каналы. Они классные, отвечаем!</a></span>
      <button id="promoClose" class="promo-close" type="button" aria-label="Скрыть плашку">×</button>
    </div>
  </div>
  

  <main class="container">
    
    <div class="pager static-pager" style="justify-content:center">
      <div class="page-links">
        <a class="nav-link disabled" href="#">←</a>
        <a class="page-link current" href="index.html">1</a> <a class="page-link" href="page-2.html">2</a> <a class="page-link" href="page-3.html">3</a> <a class="page-link" href="page-4.html">4</a>
        <a class="nav-link" href="page-2.html">→</a>
      </div>
    </div>
    
    <div id="posts" class="posts">
      
    <article class="post" data-post-id="238" data-search="emu3.5: native multimodal models are world learners сегодня разбираем работу от команды китайского института искусственного интеллекта, которая продолжает гнуть свою линию и выкатывает очередную модель семейства emu. на этот раз — emu3.5. в отличие от предыдущих работ, здесь авторы прямо говорят, что пытаются построить не просто мультимодальную модель, а некую world model. ниже разберёмся, что под этим понимают. сразу о путанице в названиях. есть emu от meta* — text-image-модель, важная в своё время как ранний пример качественного sft на небольших датасетах. и есть отдельная серия работ emu от авторов этой статьи. например, год назад, у них была работа под названием emu3: next-token prediction is all you need. тогда идея была довольно простой: свести текст, картинки и видео к единой задаче next-token prediction. генерации выглядели сочными, но при внимательном рассмотрении страдали от типичных артефактов дискретизации — текстуры «плыли», мелкие детали разваливались. в emu3.5 амбиции заметно выросли. архитектурно всё по-прежнему прямолинейно: один decoder-only-трансформер на 34b параметров, обучаемый чисто авторегрессионно. самое интересное — в данных. вместо того чтобы опираться в основном на пары картинка-текст, модель обучают преимущественно на чередующихся (interleaved) видео-текстовых последовательностях из интернета. видео нарезают на ключевые кадры, аудио транскрибируют с помощью asr с таймстемпами, а затем всё это склеивают в одну длинную последовательность: в сумме — больше 10 триллионов токенов. так модель учится не отдельным сценам, а событиям во времени: динамике, переходам, причинно-следственным связям. это и есть их практическое определение «world learning». кроме видео используют обычные image-text-данные и большой объём text-only-данных. и это ещё не финал: после претрейна модель доучивают — сначала на гигантском sft (150 млрд сэмплов), а потом через rl-алайнмент, чтобы она вела себя адекватно и по тексту, и по картинкам. все модальности токенизируются в общее дискретное пространство. словарь модели — около 280k токенов, из которых ~150k приходятся на текст, а остальная часть — на визуальные токены. для визуальной части используется собственный токенизатор с repa-подобной стабилизацией через siglip. авторы честно признают, что дискретизация всё равно даёт артефакты, поэтому опционально добавляют диффузионный декодер поверх авторегрессионной генерации. отдельная важная часть — dida (discrete diffusion adaptation). так пробуют решить главную боль авторегрессии: медленную генерацию изображений. на этапе инференса модель временно переводится в режим дискретной диффузии: визуальные токены зашумляются и затем восстанавливаются за несколько итераций. за счёт этого генерация картинок ускоряется примерно в 20 раз без заметной потери качества. на выходе emu3.5 умеет довольно широкий спектр вещей: выдаёт длинные согласованные визуальные нарративы, генерацию историй с картинками, пошаговые визуальные инструкции и даже навигацию по сцене по текстовым командам — как будто внутри есть некоторое представление пространства. в классических задачах text-to-image и image editing модель на уровне сильных закрытых мультимодальных моделей. в итоге, даже если с громким термином world model можно поспорить, сама траектория развития emu выглядит любопытно — продолжим следить за ними. разбор подготовил ❣ сергей кастрюлин cv time ___ компания meta признана экстремистской; её деятельность в россии запрещена. emu3.5: native multimodal models are world learners сегодня разбираем работу от команды китайского института искусственного интеллекта, которая продолжает гнуть свою линию и выкатывает очередную модель семейства emu. на этот раз — emu3.5. в отличие от предыдущих работ, здесь авторы прямо говорят, что пытаются построить не просто мультимодальную модель, а некую world model. ниже разберёмся, что под этим понимают. сразу о путанице в названиях. есть emu от meta * — text-image-модель, важная в своё время как ранний пример качественного sft на небольших датасетах. и есть отдельная серия работ emu от авторов этой статьи. например, год назад, у них была работа под названием emu3: next-token prediction is all you need . тогда идея была довольно простой: свести текст, картинки и видео к единой задаче next-token prediction. генерации выглядели сочными, но при внимательном рассмотрении страдали от типичных артефактов дискретизации — текстуры «плыли», мелкие детали разваливались. в emu3.5 амбиции заметно выросли. архитектурно всё по-прежнему прямолинейно: один decoder-only-трансформер на 34b параметров, обучаемый чисто авторегрессионно. самое интересное — в данных. вместо того чтобы опираться в основном на пары картинка-текст, модель обучают преимущественно на чередующихся (interleaved) видео-текстовых последовательностях из интернета. видео нарезают на ключевые кадры, аудио транскрибируют с помощью asr с таймстемпами, а затем всё это склеивают в одну длинную последовательность: в сумме — больше 10 триллионов токенов. так модель учится не отдельным сценам, а событиям во времени: динамике, переходам, причинно-следственным связям. это и есть их практическое определение «world learning». кроме видео используют обычные image-text-данные и большой объём text-only-данных. и это ещё не финал: после претрейна модель доучивают — сначала на гигантском sft (150 млрд сэмплов), а потом через rl-алайнмент, чтобы она вела себя адекватно и по тексту, и по картинкам. все модальности токенизируются в общее дискретное пространство. словарь модели — около 280k токенов, из которых ~150k приходятся на текст, а остальная часть — на визуальные токены. для визуальной части используется собственный токенизатор с repa-подобной стабилизацией через siglip. авторы честно признают, что дискретизация всё равно даёт артефакты, поэтому опционально добавляют диффузионный декодер поверх авторегрессионной генерации. отдельная важная часть — dida (discrete diffusion adaptation). так пробуют решить главную боль авторегрессии: медленную генерацию изображений. на этапе инференса модель временно переводится в режим дискретной диффузии: визуальные токены зашумляются и затем восстанавливаются за несколько итераций. за счёт этого генерация картинок ускоряется примерно в 20 раз без заметной потери качества. на выходе emu3.5 умеет довольно широкий спектр вещей: выдаёт длинные согласованные визуальные нарративы, генерацию историй с картинками, пошаговые визуальные инструкции и даже навигацию по сцене по текстовым командам — как будто внутри есть некоторое представление пространства. в классических задачах text-to-image и image editing модель на уровне сильных закрытых мультимодальных моделей. в итоге, даже если с громким термином world model можно поспорить, сама траектория развития emu выглядит любопытно — продолжим следить за ними. разбор подготовил ❣ сергей кастрюлин cv time ___ компания meta признана экстремистской; её деятельность в россии запрещена.">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2026-02-10T10:16:14+00:00" href="./posts/238.html">2026-02-10 10:16 UTC</a></div>
      </div>
      <div class="post-body"><strong>Emu3.5: Native Multimodal Models are World Learners</strong><br><strong><br></strong>Сегодня разбираем <a href="https://arxiv.org/abs/2510.26583v1" rel="nofollow noopener noreferrer">работу</a> от команды китайского Института искусственного интеллекта, которая продолжает гнуть свою линию и выкатывает очередную модель семейства Emu. На этот раз — Emu3.5. В отличие от предыдущих работ, здесь авторы прямо говорят, что пытаются построить не просто мультимодальную модель, а некую world model. Ниже разберёмся, что под этим понимают.<br><br>Сразу о путанице в названиях. Есть <a href="https://arxiv.org/abs/2307.05222" rel="nofollow noopener noreferrer">Emu от Meta</a>* — text-image-модель, важная в своё время как ранний пример качественного SFT на небольших датасетах. И есть отдельная серия работ Emu от авторов этой статьи.<br><br>Например, год назад, у них была работа под названием <a href="https://arxiv.org/abs/2409.18869" rel="nofollow noopener noreferrer">Emu3: Next-Token Prediction is All You Need</a>. Тогда идея была довольно простой: свести текст, картинки и видео к единой задаче next-token prediction. Генерации выглядели сочными, но при внимательном рассмотрении страдали от типичных артефактов дискретизации — текстуры «плыли», мелкие детали разваливались. <br><br>В Emu3.5 амбиции заметно выросли. Архитектурно всё по-прежнему прямолинейно: один decoder-only-трансформер на 34B параметров, обучаемый чисто авторегрессионно. Самое интересное — в данных. Вместо того чтобы опираться в основном на пары картинка-текст, модель обучают преимущественно на чередующихся (interleaved) видео-текстовых последовательностях из интернета. Видео нарезают на ключевые кадры, аудио транскрибируют с помощью ASR с таймстемпами, а затем всё это склеивают в одну длинную последовательность: в сумме — больше 10 триллионов токенов.<br><br>Так модель учится не отдельным сценам, а событиям во времени: динамике, переходам, причинно-следственным связям. Это и есть их практическое определение «world learning». Кроме видео используют обычные image-text-данные и большой объём text-only-данных.<br><br>И это ещё не финал: после претрейна модель доучивают — сначала на гигантском SFT (150 млрд сэмплов), а потом через RL-алайнмент, чтобы она вела себя адекватно и по тексту, и по картинкам.<br><br>Все модальности токенизируются в общее дискретное пространство. Словарь модели — около 280k токенов, из которых ~150k приходятся на текст, а остальная часть — на визуальные токены. Для визуальной части используется собственный токенизатор с REPA-подобной стабилизацией через SigLIP. Авторы честно признают, что дискретизация всё равно даёт артефакты, поэтому опционально добавляют диффузионный декодер поверх авторегрессионной генерации.<br><br>Отдельная важная часть — DiDA (Discrete Diffusion Adaptation). Так пробуют решить главную боль авторегрессии: медленную генерацию изображений. На этапе инференса модель временно переводится в режим дискретной диффузии: визуальные токены зашумляются и затем восстанавливаются за несколько итераций. За счёт этого генерация картинок ускоряется примерно в 20 раз без заметной потери качества.<br><br>На выходе Emu3.5 умеет довольно широкий спектр вещей: выдаёт длинные согласованные визуальные нарративы, генерацию историй с картинками, пошаговые визуальные инструкции и даже навигацию по сцене по текстовым командам — как будто внутри есть некоторое представление пространства. В классических задачах text-to-image и image editing модель на уровне сильных закрытых мультимодальных моделей.<br><br>В итоге, даже если с громким термином world model можно поспорить, сама траектория развития Emu выглядит любопытно — продолжим следить за ними.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Сергей Кастрюлин<br></em><a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer">CV Time</a><br><em>___</em><br><em>Компания Meta признана экстремистской; её деятельность в России запрещена.</em><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/238_480.webp" srcset="../assets/media/thumbs/238_480.webp 480w, ../assets/media/238.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="238" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/239_480.webp" srcset="../assets/media/thumbs/239_480.webp 480w, ../assets/media/239.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="238" data-image-index="1" /></div></div>
      <div class="actions">
        <span>490 просмотров · 21 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/238" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/238.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="237" data-search="qwen3-vl-embedding and qwen3-vl-reranker: a unified framework for state-of-the-art multimodal retrieval and ranking [2/2] продолжаем разбирать техрепорт, описывающий новые модели qwen. обучение моделей и результаты обучение моделей делается в несколько этапов, причём довольно нетривиальным образом: модели с этапа x используются для последующей фильтрации данных для этапа x+1, а embedding и reranker на разных этапах выступают учителями друг для друга. — на всех этапах модели обучаются как lora к qwen3-vl, чтобы с большей вероятностью не испортить возможности сильного бэкбона. — на первом этапе (s0) на всём датасете обучается embedding, используя контрастивный infonce-лосс. — на следующем этапе embedding:s0 используется для фильтрации датасета — и на этом фильтре обучается embedder:s1 и reranker. — на последнем этапе снова фильтруется уже reranker, и скоры reranker используются как таргет для дистилляции embedding:s2. — наконец, веса полученной модели усредняются (точнее, сферически интерполируются) с embedding:s1, порождая финальную модель embedding:s3, которая и пошла в релиз. по замерам авторов, их модели опережают все существующие открытые и закрытые модели на мультимодальных бенчмарках. при этом на текстовых задачах есть и более сильные модели — в основном существенно большего размера. использование моделей авторы явно постарались сделать модели production-ready, позаботившись не только о качестве метрик, но и об удобстве использования. во-первых, в модель заложены несколько очень важных свойств для производительности (помимо инференса в один prefill-этап). тренировка проводилась в quantization-aware-режиме — при вычислении лоссов для эмбеддингов, авторы одновременно вычисляли их для квантизованных в int8-эмбеддингов. в результате, полученные эмбеддинги можно квантизовать в int8 (отмасштабировать в интервал [-127, 128] и округлить), хранить и использовать практически потери качества. также в тренировке эмбеддингов использовался подход матрёшки, при котором лоссы применяются не только к эмбеддингам целиком, но и по частям к их первым 32, 64, 128, 256 и 512 элементам. благодаря этому каждый кратный степени двойки «подсрез» эмбеддинга — тоже эмбеддинг (хоть и худшего качества). при работе с большой базой документов можно, например, брать только первые 128 элементов эмбеддинга вместо 1024 и хранить только их. суммарно можно сократить размер эмбеддингов базы документов в 10–50 раз. во-вторых, в силу архитектуры модель очень гибка в применении. и документ, и запрос могут быть не только одним изображением или текстом, но и их произвольной последовательностью. довольно большое окно контекста (32к) токенов позволяет обрабатывать 10–20 страниц изображений вместе с текстом. также интересная фича таких моделей как класса — наличие инструкции. мультимодальные семантические эмбеддинги доступны всем и каждому как минимум с момента релиза clip (5 лет назад!), но способ вычисления эмбеддинга почти всегда был «зашит» в модель. для эмбеддеров на основе llm/vlm можно в инструкции указать, что важно в «кодировании» документов и запросов. например, в случае поиска по картинкам можно инструктировать модель фокусироваться на стиле изображения или, наоборот, на содержимом — и получить эмбеддинги, поиск по которым будет давать разные результаты. в итоге у авторов получилась гибкая и эффективная опенсорсная модель для мультимодального поиска. в отчёте приведено много деталей обучения, а в cookbook — примеров использования. модели такого класса определённо имеют множество применений как в продуктах, так и в рутинных ml-задачах по работе с данными. разбор подготовил ❣ борис зимка cv time qwen3-vl-embedding and qwen3-vl-reranker: a unified framework for state-of-the-art multimodal retrieval and ranking [2/2] продолжаем разбирать техрепорт , описывающий новые модели qwen. обучение моделей и результаты обучение моделей делается в несколько этапов, причём довольно нетривиальным образом: модели с этапа x используются для последующей фильтрации данных для этапа x+1, а embedding и reranker на разных этапах выступают учителями друг для друга. — на всех этапах модели обучаются как lora к qwen3-vl, чтобы с большей вероятностью не испортить возможности сильного бэкбона. — на первом этапе (s0) на всём датасете обучается embedding, используя контрастивный infonce-лосс. — на следующем этапе embedding:s0 используется для фильтрации датасета — и на этом фильтре обучается embedder:s1 и reranker. — на последнем этапе снова фильтруется уже reranker, и скоры reranker используются как таргет для дистилляции embedding:s2. — наконец, веса полученной модели усредняются (точнее, сферически интерполируются) с embedding:s1, порождая финальную модель embedding:s3, которая и пошла в релиз. по замерам авторов, их модели опережают все существующие открытые и закрытые модели на мультимодальных бенчмарках. при этом на текстовых задачах есть и более сильные модели — в основном существенно большего размера. использование моделей авторы явно постарались сделать модели production-ready, позаботившись не только о качестве метрик, но и об удобстве использования. во-первых, в модель заложены несколько очень важных свойств для производительности (помимо инференса в один prefill-этап). тренировка проводилась в quantization-aware-режиме — при вычислении лоссов для эмбеддингов, авторы одновременно вычисляли их для квантизованных в int8-эмбеддингов. в результате, полученные эмбеддинги можно квантизовать в int8 (отмасштабировать в интервал [-127, 128] и округлить), хранить и использовать практически потери качества. также в тренировке эмбеддингов использовался подход матрёшки, при котором лоссы применяются не только к эмбеддингам целиком, но и по частям к их первым 32, 64, 128, 256 и 512 элементам. благодаря этому каждый кратный степени двойки «подсрез» эмбеддинга — тоже эмбеддинг (хоть и худшего качества). при работе с большой базой документов можно, например, брать только первые 128 элементов эмбеддинга вместо 1024 и хранить только их. суммарно можно сократить размер эмбеддингов базы документов в 10–50 раз. во-вторых, в силу архитектуры модель очень гибка в применении. и документ, и запрос могут быть не только одним изображением или текстом, но и их произвольной последовательностью. довольно большое окно контекста (32к) токенов позволяет обрабатывать 10–20 страниц изображений вместе с текстом. также интересная фича таких моделей как класса — наличие инструкции. мультимодальные семантические эмбеддинги доступны всем и каждому как минимум с момента релиза clip (5 лет назад!), но способ вычисления эмбеддинга почти всегда был «зашит» в модель. для эмбеддеров на основе llm/vlm можно в инструкции указать, что важно в «кодировании» документов и запросов. например, в случае поиска по картинкам можно инструктировать модель фокусироваться на стиле изображения или, наоборот, на содержимом — и получить эмбеддинги, поиск по которым будет давать разные результаты. в итоге у авторов получилась гибкая и эффективная опенсорсная модель для мультимодального поиска. в отчёте приведено много деталей обучения, а в cookbook — примеров использования. модели такого класса определённо имеют множество применений как в продуктах, так и в рутинных ml-задачах по работе с данными. разбор подготовил ❣ борис зимка cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2026-02-06T11:03:15+00:00" href="./posts/237.html">2026-02-06 11:03 UTC</a></div>
      </div>
      <div class="post-body"><strong>Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking [2/2]<br></strong><br>Продолжаем разбирать <a href="https://arxiv.org/abs/2601.04720" rel="nofollow noopener noreferrer">техрепорт</a>, описывающий новые модели Qwen.<br><br><strong>Обучение моделей и результаты<br></strong><br>Обучение моделей делается в несколько этапов, причём довольно нетривиальным образом: модели с этапа X используются для последующей фильтрации данных для этапа X+1, а Embedding и Reranker на разных этапах выступают учителями друг для друга.<br><br>— На всех этапах модели обучаются как LoRA к Qwen3-VL, чтобы с большей вероятностью не испортить возможности сильного бэкбона.<br><br>— На первом этапе (s0) на всём датасете обучается Embedding, используя контрастивный InfoNCE-лосс.<br><br>— На следующем этапе Embedding:s0 используется для фильтрации датасета — и на этом фильтре обучается Embedder:s1 и Reranker.<br><br>— На последнем этапе снова фильтруется уже Reranker, и скоры Reranker используются как таргет для дистилляции Embedding:s2.<br><br>— Наконец, веса полученной модели усредняются (точнее, сферически интерполируются) с Embedding:s1, порождая финальную модель Embedding:s3, которая и пошла в релиз.<br><br>По замерам авторов, их модели опережают все существующие открытые и закрытые модели на мультимодальных бенчмарках. При этом на текстовых задачах есть и более сильные модели — в основном существенно большего размера.<br><br><strong>Использование моделей<br></strong> <br>Авторы явно постарались сделать модели production-ready, позаботившись не только о качестве метрик, но и об удобстве использования.<br><br>Во-первых, в модель заложены несколько очень важных свойств для производительности (помимо инференса в один prefill-этап).<br><br>Тренировка проводилась в quantization-aware-режиме — при вычислении лоссов для эмбеддингов, авторы одновременно вычисляли их для квантизованных в int8-эмбеддингов. В результате, полученные эмбеддинги можно квантизовать в int8 (отмасштабировать в интервал  [-127, 128] и округлить), хранить и использовать практически потери качества.<br><br>Также в тренировке эмбеддингов использовался подход матрёшки, при котором лоссы применяются не только к эмбеддингам целиком, но и по частям к их первым 32, 64, 128, 256 и 512 элементам. Благодаря этому каждый кратный степени двойки «подсрез» эмбеддинга — тоже эмбеддинг (хоть и худшего качества). При работе с большой базой документов можно, например, брать только первые 128 элементов эмбеддинга вместо 1024 и хранить только их. Суммарно можно сократить размер эмбеддингов базы документов в 10–50 раз.<br><br>Во-вторых, в силу архитектуры модель очень гибка в применении. И документ, и запрос могут быть не только одним изображением или текстом, но и их произвольной последовательностью. Довольно большое окно контекста (32К) токенов позволяет обрабатывать 10–20 страниц изображений вместе с текстом. <br><br>Также интересная фича таких моделей как класса — наличие инструкции. Мультимодальные семантические эмбеддинги доступны всем и каждому как минимум с момента релиза CLIP (5 лет назад!), но способ вычисления эмбеддинга почти всегда был «зашит» в модель. Для эмбеддеров на основе LLM/VLM можно в инструкции указать, что важно в «кодировании» документов и запросов. Например, в случае поиска по картинкам можно инструктировать модель фокусироваться на стиле изображения или, наоборот, на содержимом — и получить эмбеддинги, поиск по которым будет давать разные результаты.<br><br>В итоге у авторов получилась гибкая и эффективная опенсорсная модель для мультимодального поиска. В отчёте приведено много деталей обучения, а в cookbook — примеров использования. Модели такого класса определённо имеют множество применений как в продуктах, так и в рутинных ML-задачах по работе с данными.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Борис Зимка<br></em><a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/237_480.webp" srcset="../assets/media/thumbs/237_480.webp 480w, ../assets/media/237.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="237" data-image-index="0" /></div></div>
      <div class="actions">
        <span>973 просмотров · 38 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/237" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/237.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="236" data-search="qwen3-vl-embedding and qwen3-vl-reranker: a unified framework for state-of-the-art multimodal retrieval and ranking [1/2] ещё летом 2025-го вышли текстовые qwen3-embedding/reranker. а в январе этого года команда qwen представила новые модели: qwen3-vl-embedding и qwen3-vl-reranker. в техрепорте авторы рассказывают, как им удалось адаптировать vlm для решения задач мультимодального поиска и ранжирования — ключевых тем ml с долгой историей развития и огромным количеством применений. об этом сегодня и поговорим. формулировка задачи если кратко, задача поиска по базе документов — по запросу q среди множества документов d[i] найти подходящие под запрос. в текстовом поиске q и d — текст, а в мультимодальном варианте — q и d могут быть картинками, текстом или их комбинацией, причём модальности q и d могут не совпадать. например, по запросу «пингвины в южной америке» релевантны и статьи википедии, и соответствующие фотографии. модели один из распространённых подходов в решении задачи поиска — разбиение на два этапа: быстрый поиск кандидатов и более сложное ранжирование их между собой для определения лучших. исходя из такой схемы, команда qwen подготовила две модели: 1. qwen3-vl-embedding: модель, предсказывающая для документа или запроса вектор признаков в соответствии с инструкцией. можно считать, `def embedding(instruction: str, query_or_doc: str | image) -&gt; list[float]`. 2. qwen3-vl-reranker: модель, оценивающая согласно инструкции степень соответствия запроса документу от 0 до 1. интерфейс примерно: `def reranker(instruction: str, query: str | image, document: str | image) -&gt; float`. архитектурно модели — почти точные копии vlm: получают на вход токенизированные инструкции и текст, патчи изображений, но имеют модифицированный выход, и инференсятся несколько иначе. reranker выполняет инференс всей vlm целиком, но на выходе в качестве оценки «релевантен ли документ запросу» берётся соотношение вероятностей токенов “yes” и “no”. embedding выполняет инференс до последнего слоя (проекции токена в вероятности вокабуляра) — и hidden state перед этой проекцией возвращается как эмбеддинг. в отличие от полноценных vlm, в embedding и reranker выполняется только этап prefill (обработка входного контекста), и состояние последнего токена промпта возвращается как ответ. стадия decoding (предсказания одного токена за другим) отсутствует, что делает инференс многократно быстрее. обе модели инициализируются qwen3-vl и доступны в двух вариантах: на 2 и 8 миллиардов параметров. данные датасеты для поиска повторяют логику задачи: — одна текстовая инструкция к задаче i; — база мультимодальных документов d[i]; — набор мультимодальных запросов q[j]; — матрица меток r[i, j], определяющих d[i] как релевантный или нерелевантный q[j]. на таком датасете можно обучать как reranker (напрямую классифицировать релевантность пары q-d), так и embedding (оценивая релевантность пары по скалярному произведению эмбеддингов). обучающий корпус embedding и reranker состоит из множества таких датасетов. для каждого из них база документов берётся из реальных данных — эти документы vlm описывает и классифицирует. некачественные фильтруются, распределение датасетов нормализуется, чтобы избежать сильного перекоса в какой-либо домен. затем для документов с помощью vlm генерируют запросы разных типов, причём как релевантные документу, так и hard-negative-примеры — запросы, для которых документ похож на релевантный, но не является таковым. после этого датасеты дополнительно фильтруются уже существующими моделями и неудачные элементы датасета отсеиваются. во второй части разбора поговорим о том, как модели учились, и об их использовании на практике. разбор подготовил ❣ борис зимка cv time qwen3-vl-embedding and qwen3-vl-reranker: a unified framework for state-of-the-art multimodal retrieval and ranking [1/2] ещё летом 2025-го вышли текстовые qwen3-embedding/reranker. а в январе этого года команда qwen представила новые модели: qwen3-vl-embedding и qwen3-vl-reranker. в техрепорте авторы рассказывают, как им удалось адаптировать vlm для решения задач мультимодального поиска и ранжирования — ключевых тем ml с долгой историей развития и огромным количеством применений. об этом сегодня и поговорим. формулировка задачи если кратко, задача поиска по базе документов — по запросу q среди множества документов d[i] найти подходящие под запрос. в текстовом поиске q и d — текст, а в мультимодальном варианте — q и d могут быть картинками, текстом или их комбинацией, причём модальности q и d могут не совпадать. например, по запросу «пингвины в южной америке» релевантны и статьи википедии, и соответствующие фотографии. модели один из распространённых подходов в решении задачи поиска — разбиение на два этапа: быстрый поиск кандидатов и более сложное ранжирование их между собой для определения лучших. исходя из такой схемы, команда qwen подготовила две модели: 1. qwen3-vl-embedding: модель, предсказывающая для документа или запроса вектор признаков в соответствии с инструкцией. можно считать, `def embedding(instruction: str, query_or_doc: str | image) -&amp;gt; list[float]` . 2. qwen3-vl-reranker: модель, оценивающая согласно инструкции степень соответствия запроса документу от 0 до 1. интерфейс примерно: `def reranker(instruction: str, query: str | image, document: str | image) -&amp;gt; float` . архитектурно модели — почти точные копии vlm: получают на вход токенизированные инструкции и текст, патчи изображений, но имеют модифицированный выход, и инференсятся несколько иначе. reranker выполняет инференс всей vlm целиком, но на выходе в качестве оценки «релевантен ли документ запросу» берётся соотношение вероятностей токенов “yes” и “no”. embedding выполняет инференс до последнего слоя (проекции токена в вероятности вокабуляра) — и hidden state перед этой проекцией возвращается как эмбеддинг. в отличие от полноценных vlm, в embedding и reranker выполняется только этап prefill (обработка входного контекста), и состояние последнего токена промпта возвращается как ответ. стадия decoding (предсказания одного токена за другим) отсутствует, что делает инференс многократно быстрее. обе модели инициализируются qwen3-vl и доступны в двух вариантах: на 2 и 8 миллиардов параметров. данные датасеты для поиска повторяют логику задачи: — одна текстовая инструкция к задаче i; — база мультимодальных документов d[i]; — набор мультимодальных запросов q[j]; — матрица меток r[i, j], определяющих d[i] как релевантный или нерелевантный q[j]. на таком датасете можно обучать как reranker (напрямую классифицировать релевантность пары q-d), так и embedding (оценивая релевантность пары по скалярному произведению эмбеддингов). обучающий корпус embedding и reranker состоит из множества таких датасетов. для каждого из них база документов берётся из реальных данных — эти документы vlm описывает и классифицирует. некачественные фильтруются, распределение датасетов нормализуется, чтобы избежать сильного перекоса в какой-либо домен. затем для документов с помощью vlm генерируют запросы разных типов, причём как релевантные документу, так и hard-negative-примеры — запросы, для которых документ похож на релевантный, но не является таковым. после этого датасеты дополнительно фильтруются уже существующими моделями и неудачные элементы датасета отсеиваются. во второй части разбора поговорим о том, как модели учились, и об их использовании на практике. разбор подготовил ❣ борис зимка cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2026-02-03T08:42:01+00:00" href="./posts/236.html">2026-02-03 08:42 UTC</a></div>
      </div>
      <div class="post-body"><strong>Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking [1/2]<br></strong><br>Ещё летом 2025-го вышли текстовые Qwen3-Embedding/Reranker. А в январе этого года команда Qwen представила новые модели: Qwen3-VL-Embedding и Qwen3-VL-Reranker. В <a href="https://arxiv.org/abs/2601.04720" rel="nofollow noopener noreferrer">техрепорте</a> авторы рассказывают, как им удалось адаптировать VLM для решения задач мультимодального поиска и ранжирования — ключевых тем ML с долгой историей развития и огромным количеством применений. Об этом сегодня и поговорим.<br><br><strong>Формулировка задачи</strong><br><br>Если кратко, задача поиска по базе документов — по запросу Q среди множества документов D[i] найти подходящие под запрос. В текстовом поиске Q и D — текст, а в мультимодальном варианте — Q и D могут быть картинками, текстом или их комбинацией, причём модальности Q и D могут не совпадать. Например, по запросу «пингвины в Южной Америке» релевантны и статьи Википедии, и соответствующие фотографии.<br><br><strong>Модели<br></strong><br>Один из распространённых подходов в решении задачи поиска — разбиение на два этапа: быстрый поиск кандидатов и более сложное ранжирование их между собой для определения лучших. Исходя из такой схемы, команда Qwen подготовила две модели:<br><br>1. Qwen3-VL-Embedding: модель, предсказывающая для документа или запроса вектор признаков в соответствии с инструкцией. Можно считать, <code>`def embedding(instruction: str, query_or_doc: str | Image) -&gt; list[float]`</code>.<br><br>2. Qwen3-VL-Reranker: модель, оценивающая согласно инструкции степень соответствия запроса документу от 0 до 1. Интерфейс примерно: <code>`def reranker(instruction: str, query: str | Image, document: str | Image) -&gt; float`</code>.<br><br>Архитектурно модели — почти точные копии VLM: получают на вход токенизированные инструкции и текст, патчи изображений, но имеют модифицированный выход, и инференсятся несколько иначе.<br><br>Reranker выполняет инференс всей VLM целиком, но на выходе в качестве оценки «релевантен ли документ запросу» берётся соотношение вероятностей токенов “yes” и “no”. Embedding выполняет инференс до последнего слоя (проекции токена в вероятности вокабуляра) — и hidden state перед этой проекцией возвращается как эмбеддинг.<br><br>В отличие от полноценных VLM, в Embedding и Reranker выполняется только этап prefill (обработка входного контекста), и состояние последнего токена промпта возвращается как ответ. Стадия decoding (предсказания одного токена за другим) отсутствует, что делает инференс многократно быстрее.<br><br>Обе модели инициализируются Qwen3-VL и доступны в двух вариантах: на 2 и 8 миллиардов параметров.<br><br><strong>Данные</strong><br><br>Датасеты для поиска повторяют логику задачи:<br>— одна текстовая инструкция к задаче I;<br>— база мультимодальных документов D[i];<br>— набор мультимодальных запросов Q[j];<br>— матрица меток R[i, j], определяющих D[i] как релевантный или нерелевантный Q[j].<br><br>На таком датасете можно обучать как Reranker (напрямую классифицировать релевантность пары Q-D), так и Embedding (оценивая релевантность пары по скалярному произведению эмбеддингов).<br><br>Обучающий корпус Embedding и Reranker состоит из множества таких датасетов. Для каждого из них база документов берётся из реальных данных — эти документы VLM описывает и классифицирует. Некачественные фильтруются, распределение датасетов нормализуется, чтобы избежать сильного перекоса в какой-либо домен.<br><br>Затем для документов с помощью VLM генерируют запросы разных типов, причём как релевантные документу, так и hard-negative-примеры — запросы, для которых документ похож на релевантный, но не является таковым.<br><br>После этого датасеты дополнительно фильтруются уже существующими моделями и неудачные элементы датасета отсеиваются.<br><br>Во второй части разбора поговорим о том, как модели учились, и об их использовании на практике.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Борис Зимка<br></em><a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/236_480.webp" srcset="../assets/media/thumbs/236_480.webp 480w, ../assets/media/236.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="236" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 393 просмотров · 29 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/236" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/236.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="235" data-search="ovis-u1 technical report некоторое время назад мы обсуждали mllm. сегодня разберём статью о ещё одной универсальной модели, способной обрабатывать и текст, и изображения. ovis-u1 — модель-швейцарский-нож. в зависимости от инструкции, она может работать и в режиме image-to-text, и в text-to-image. например, можно изменить изображение, описать его или сгенерировать совсем новую картинку по текстовому запросу. архитектуру mllm можно рассмотреть на первой из трёх схем. следите за логикой сверху вниз: 1. сначала ovis-u1 обрабатывает входные данные: токенизирует текст и обрабатывает изображения визуальным энкодером, чтобы составить семантический эмбеддинг, или использует vae-энкодер для составления детализированного представления. 2. полученная последовательность подаётся в трансформер, инициализируемый с qwen3-1.7b. 3. для генерации изображения выходные токены текстов и семантических представлений входной картинки комбинируются с помощью пары трансформерных слоев (авторы называют это refiner’ом, на схеме обозначено как (с)) и, вместе с vae-эмбеддингами, отправляются в «визуальный декодер» на базе mmdit. эта часть инициализируется с нуля. обучение модели происходит в несколько этапов: — сначала предобучается визуальный декодер на задачу text-to-image-генерации. все остальные части при этом заморожены. — следом предобучается адаптер между llm и визуальным энкодером на задачи text-to-image-генерации, а также понимание и редактирование изображений. — потом на тех же данных визуальный энкодер и адаптер обучаются вместе. — на следующей стадии всё, кроме визуального декодера, обучается на задачах понимания изображения. — далее на задаче генерации изображений обучается refiner и визуальный декодер. — на финальном этапе визуальный декодер файнтюнится для задач text-to-image-генерации и редактирования изображений. авторы утверждают, что визуальный декодер на основе диффузии в сочетании с refiner’ом позволяет генерировать изображения почти так же хорошо, как gpt-4o. интересны ещё несколько замеров: — 69,6 баллов в мультимодальном академическом тесте opencompass (что лучше последних современных моделей, такие как ristretto-3b и sail-vl-1.5-2b); — 83,72 балла и 0,89 балла при преобразовании текста в изображение в тестах dpg-bench и geneval; — 4,00 и 6,42 для редактирования изображений в imgedit-bench и gedit-bench-en. разбор подготовил ❣ сергей овчаренко cv time ovis-u1 technical report некоторое время назад мы обсуждали mllm . сегодня разберём статью о ещё одной универсальной модели, способной обрабатывать и текст, и изображения. ovis-u1 — модель-швейцарский-нож. в зависимости от инструкции, она может работать и в режиме image-to-text, и в text-to-image. например, можно изменить изображение, описать его или сгенерировать совсем новую картинку по текстовому запросу. архитектуру mllm можно рассмотреть на первой из трёх схем. следите за логикой сверху вниз: 1. сначала ovis-u1 обрабатывает входные данные: токенизирует текст и обрабатывает изображения визуальным энкодером, чтобы составить семантический эмбеддинг, или использует vae-энкодер для составления детализированного представления. 2. полученная последовательность подаётся в трансформер, инициализируемый с qwen3-1.7b. 3. для генерации изображения выходные токены текстов и семантических представлений входной картинки комбинируются с помощью пары трансформерных слоев (авторы называют это refiner’ом, на схеме обозначено как (с)) и, вместе с vae-эмбеддингами, отправляются в «визуальный декодер» на базе mmdit. эта часть инициализируется с нуля. обучение модели происходит в несколько этапов: — сначала предобучается визуальный декодер на задачу text-to-image-генерации. все остальные части при этом заморожены. — следом предобучается адаптер между llm и визуальным энкодером на задачи text-to-image-генерации, а также понимание и редактирование изображений. — потом на тех же данных визуальный энкодер и адаптер обучаются вместе. — на следующей стадии всё, кроме визуального декодера, обучается на задачах понимания изображения. — далее на задаче генерации изображений обучается refiner и визуальный декодер. — на финальном этапе визуальный декодер файнтюнится для задач text-to-image-генерации и редактирования изображений. авторы утверждают, что визуальный декодер на основе диффузии в сочетании с refiner’ом позволяет генерировать изображения почти так же хорошо, как gpt-4o. интересны ещё несколько замеров: — 69,6 баллов в мультимодальном академическом тесте opencompass (что лучше последних современных моделей, такие как ristretto-3b и sail-vl-1.5-2b); — 83,72 балла и 0,89 балла при преобразовании текста в изображение в тестах dpg-bench и geneval; — 4,00 и 6,42 для редактирования изображений в imgedit-bench и gedit-bench-en. разбор подготовил ❣ сергей овчаренко cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2026-01-27T07:44:01+00:00" href="./posts/235.html">2026-01-27 07:44 UTC</a></div>
      </div>
      <div class="post-body"><strong>Ovis-U1 Technical Report<br></strong><br>Некоторое время назад мы обсуждали <a href="https://t.me/timeforcv/192" rel="nofollow noopener noreferrer">MLLM</a>. Сегодня разберём <a href="https://arxiv.org/abs/2506.23044" rel="nofollow noopener noreferrer">статью</a> о ещё одной универсальной модели, способной обрабатывать и текст, и изображения. <br><br>Ovis-U1 — модель-швейцарский-нож. В зависимости от инструкции, она может работать и в режиме image-to-text, и в text-to-image. Например, можно изменить изображение, описать его или сгенерировать совсем новую картинку по текстовому запросу. Архитектуру MLLM можно рассмотреть на первой из трёх схем.<br><br>Следите за логикой сверху вниз: <br><br>1. Сначала Ovis-U1 обрабатывает входные данные: токенизирует текст и обрабатывает изображения визуальным энкодером, чтобы составить семантический эмбеддинг, или использует VAE-энкодер для составления детализированного представления. <br><br>2. Полученная последовательность подаётся в трансформер, инициализируемый с Qwen3-1.7B.<br><br>3. Для генерации изображения выходные токены текстов и семантических представлений входной картинки комбинируются с помощью пары трансформерных слоев (авторы называют это Refiner’ом, на схеме обозначено как (с)) и, вместе с VAE-эмбеддингами, отправляются в «визуальный декодер» на базе MMDiT. Эта часть инициализируется с нуля.<br><br>Обучение модели происходит в несколько этапов: <br><br>— Сначала предобучается визуальный декодер на задачу text-to-image-генерации. Все остальные части при этом заморожены.<br>— Следом предобучается адаптер между LLM и визуальным энкодером на задачи text-to-image-генерации, а также понимание и редактирование изображений.<br>— Потом на тех же данных визуальный энкодер и адаптер обучаются вместе.<br>— На следующей стадии всё, кроме визуального декодера, обучается на задачах понимания изображения.<br>— Далее на задаче генерации изображений обучается refiner и визуальный декодер.<br>— На финальном этапе визуальный декодер файнтюнится для задач text-to-image-генерации и редактирования изображений.<br><br>Авторы утверждают, что визуальный декодер на основе диффузии в сочетании с Refiner’ом позволяет генерировать изображения почти так же хорошо, как GPT-4o. Интересны ещё несколько замеров:<br><br>— 69,6 баллов в мультимодальном академическом тесте OpenCompass (что лучше последних современных моделей, такие как Ristretto-3B и SAIL-VL-1.5-2B);<br>— 83,72 балла и 0,89 балла при преобразовании текста в изображение в тестах DPG-Bench и GenEval; <br>— 4,00 и 6,42 для редактирования изображений в ImgEdit-Bench и GEdit-Bench-EN.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Сергей Овчаренко<br></em><a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/235_480.webp" srcset="../assets/media/thumbs/235_480.webp 480w, ../assets/media/235.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="235" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 454 просмотров · 21 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/235" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/235.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="230" data-search="decoupled dmd: cfg augmentation as the spear, distribution matching as the shield сегодня разберём статью, авторы которой возвращаются к идее dmd и пытаются понять, что именно заставляет этот метод работать. их главное наблюдение — главную роль в обучении играет не distribution matching, как можно было ожидать, а cfg augmentation. что такое dmd dmd относится к ode-free-дистилляции диффузионных моделей: здесь не важно, по какой траектории происходит сэмплирование, главное — чтобы модель умела выдавать скор-функцию. идея метода в том, чтобы форсить совпадение распределения генератора с распределением реальных данных, оптимизируя kl-дивергенцию между p_{fake} и p_{real}. плотность реальных данных напрямую недоступна, но для обучения достаточно градиента этого лосса. после дифференцирования в выражении появляются скор-функции реальных и фейковых данных: фейковую мы учим, а реальную аппроксимируем замороженной моделью-учителем. поскольку скор-модели плохо работают на незашумлённых изображениях и реальные с фейковыми распределениями часто плохо пересекаются по модам, в dmd скоры считают на зашумлённых данных. это делает их in-distribution и стабилизирует обучение. в итоге реальный скор остаётся замороженным, а фейковый обучается стандартным diffusion loss — это база для всех модификаций dmd. что изменилось в dmd2 в dmd2 авторы разомкнули обучение генератора и оценщика. сделали несколько шагов обучения оценщика на один шаг генератора, и за счёт этого отказались от регрессионного лосса. также был добавлен gan loss как регуляризация: используют не как основной источник сигнала, а именно для стабилизации обучения. основная идея decoupled dmd в новой статье авторы снова смотрят на градиент kl-дивергенции и замечают, что простая conditional-оценка реального скора работает плохо. зато на практике гораздо лучше cfg-оценка. возникает вопрос — это просто удачный трюк или за этим стоит какая-то теория? оказывается, если подставить cfg прямо в формулу kl-лосса, он раскладывается на две части: классический distribution matching и дополнительный член, соответствующий вектору между real conditional и real unconditional скорами. именно эту добавку авторы называют cfg augmentation. из этого разложения следует ключевой вывод статьи: основной обучающий сигнал в dmd даёт cfg augmentation, а distribution matching выступает стабилизирующей регуляризацией. эксперименты и выводы эксперименты подтверждают этот тезис. обучение только на distribution matching быстро ломает семантику, обучение только на cfg augmentation приводит к переобучению. самый стабильный результат получается при совместном использовании обоих компонент лосса. авторы также показывают, что cfg augmentation и distribution matching имеет смысл обучать с разными уровнями шума: больший \tau в cfg-части помогает с высокочастотными деталями, тогда как для distribution matching лучше работает стандартный диапазон шумов. в итоге статья интересна не столько метриками, сколько самим наблюдением: cfg в dmd — это не эвристика, а осмысленный компонент лосса. разбор подготовил ❣ михаил колтаков cv time decoupled dmd: cfg augmentation as the spear, distribution matching as the shield сегодня разберём статью , авторы которой возвращаются к идее dmd и пытаются понять, что именно заставляет этот метод работать. их главное наблюдение — главную роль в обучении играет не distribution matching, как можно было ожидать, а cfg augmentation. что такое dmd dmd относится к ode-free-дистилляции диффузионных моделей: здесь не важно, по какой траектории происходит сэмплирование, главное — чтобы модель умела выдавать скор-функцию. идея метода в том, чтобы форсить совпадение распределения генератора с распределением реальных данных, оптимизируя kl-дивергенцию между p_{fake} и p_{real}. плотность реальных данных напрямую недоступна, но для обучения достаточно градиента этого лосса. после дифференцирования в выражении появляются скор-функции реальных и фейковых данных: фейковую мы учим, а реальную аппроксимируем замороженной моделью-учителем. поскольку скор-модели плохо работают на незашумлённых изображениях и реальные с фейковыми распределениями часто плохо пересекаются по модам, в dmd скоры считают на зашумлённых данных. это делает их in-distribution и стабилизирует обучение. в итоге реальный скор остаётся замороженным, а фейковый обучается стандартным diffusion loss — это база для всех модификаций dmd. что изменилось в dmd2 в dmd2 авторы разомкнули обучение генератора и оценщика. сделали несколько шагов обучения оценщика на один шаг генератора, и за счёт этого отказались от регрессионного лосса. также был добавлен gan loss как регуляризация: используют не как основной источник сигнала, а именно для стабилизации обучения. основная идея decoupled dmd в новой статье авторы снова смотрят на градиент kl-дивергенции и замечают, что простая conditional-оценка реального скора работает плохо. зато на практике гораздо лучше cfg-оценка. возникает вопрос — это просто удачный трюк или за этим стоит какая-то теория? оказывается, если подставить cfg прямо в формулу kl-лосса, он раскладывается на две части: классический distribution matching и дополнительный член, соответствующий вектору между real conditional и real unconditional скорами. именно эту добавку авторы называют cfg augmentation. из этого разложения следует ключевой вывод статьи: основной обучающий сигнал в dmd даёт cfg augmentation, а distribution matching выступает стабилизирующей регуляризацией. эксперименты и выводы эксперименты подтверждают этот тезис. обучение только на distribution matching быстро ломает семантику, обучение только на cfg augmentation приводит к переобучению. самый стабильный результат получается при совместном использовании обоих компонент лосса. авторы также показывают, что cfg augmentation и distribution matching имеет смысл обучать с разными уровнями шума: больший \tau в cfg-части помогает с высокочастотными деталями, тогда как для distribution matching лучше работает стандартный диапазон шумов. в итоге статья интересна не столько метриками, сколько самим наблюдением: cfg в dmd — это не эвристика, а осмысленный компонент лосса. разбор подготовил ❣ михаил колтаков cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2026-01-20T09:32:02+00:00" href="./posts/230.html">2026-01-20 09:32 UTC</a></div>
      </div>
      <div class="post-body"><strong>Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield<br></strong><br>Сегодня разберём <a href="https://arxiv.org/abs/2511.22677" rel="nofollow noopener noreferrer">статью</a>, авторы которой возвращаются к идее DMD и пытаются понять, что именно заставляет этот метод работать. Их главное наблюдение — главную роль в обучении играет не distribution matching, как можно было ожидать, а CFG Augmentation. <br><br><strong>Что такое DMD </strong><br><br><a href="https://arxiv.org/abs/2311.18828" rel="nofollow noopener noreferrer">DMD</a> относится к ODE-free-дистилляции диффузионных моделей: здесь не важно, по какой траектории происходит сэмплирование, главное — чтобы модель умела выдавать скор-функцию.<br><br>Идея метода в том, чтобы форсить совпадение распределения генератора с распределением реальных данных, оптимизируя KL-дивергенцию между P_{fake} и P_{real}. Плотность реальных данных напрямую недоступна, но для обучения достаточно градиента этого лосса. После дифференцирования в выражении появляются скор-функции реальных и фейковых данных: фейковую мы учим, а реальную аппроксимируем замороженной моделью-учителем.<br><br>Поскольку скор-модели плохо работают на незашумлённых изображениях и реальные с фейковыми распределениями часто плохо пересекаются по модам, в DMD скоры считают на зашумлённых данных. Это делает их in-distribution и стабилизирует обучение. В итоге реальный скор остаётся замороженным, а фейковый обучается стандартным diffusion loss — это база для всех модификаций DMD.<br><br><strong>Что изменилось в DMD2<br></strong><br>В <a href="https://arxiv.org/abs/2405.14867" rel="nofollow noopener noreferrer">DMD2</a> авторы разомкнули обучение генератора и оценщика. Сделали несколько шагов обучения оценщика на один шаг генератора, и за счёт этого отказались от регрессионного лосса. Также был добавлен GAN loss как регуляризация: используют не как основной источник сигнала, а именно для стабилизации обучения.<br><br><strong>Основная идея Decoupled DMD<br></strong><br>В новой <a href="https://arxiv.org/abs/2511.22677" rel="nofollow noopener noreferrer">статье</a> авторы снова смотрят на градиент KL-дивергенции и замечают, что простая conditional-оценка реального скора работает плохо. Зато на практике гораздо лучше CFG-оценка. Возникает вопрос — это просто удачный трюк или за этим стоит какая-то теория?<br><br>Оказывается, если подставить CFG прямо в формулу KL-лосса, он раскладывается на две части: классический distribution matching и дополнительный член, соответствующий вектору между real conditional и real unconditional скорами. Именно эту добавку авторы называют CFG Augmentation. Из этого разложения следует ключевой вывод статьи: основной обучающий сигнал в DMD даёт CFG Augmentation, а distribution matching выступает стабилизирующей регуляризацией.<br><br><strong>Эксперименты и выводы</strong><br><br>Эксперименты подтверждают этот тезис. Обучение только на distribution matching быстро ломает семантику, обучение только на CFG Augmentation приводит к переобучению. Самый стабильный результат получается при совместном использовании обоих компонент лосса.<br><br>Авторы также показывают, что CFG Augmentation и distribution matching имеет смысл обучать с разными уровнями шума: больший \tau в CFG-части помогает с высокочастотными деталями, тогда как для distribution matching лучше работает стандартный диапазон шумов.<br><br>В итоге статья интересна не столько метриками, сколько самим наблюдением: CFG в DMD — это не эвристика, а осмысленный компонент лосса.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Михаил Колтаков<br></em><a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/230_480.webp" srcset="../assets/media/thumbs/230_480.webp 480w, ../assets/media/230.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="230" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/231_480.webp" srcset="../assets/media/thumbs/231_480.webp 480w, ../assets/media/231.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="230" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/232_480.webp" srcset="../assets/media/thumbs/232_480.webp 480w, ../assets/media/232.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="230" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/233_480.webp" srcset="../assets/media/thumbs/233_480.webp 480w, ../assets/media/233.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="230" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/234_480.webp" srcset="../assets/media/thumbs/234_480.webp 480w, ../assets/media/234.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="230" data-image-index="4" /></div></div>
      <div class="actions">
        <span>1 437 просмотров · 17 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/230" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/230.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="229" data-search="лучшие статьи 2025 года: выбор авторов сv time. часть 2 хороших статей в прошлом году оказалось слишком много, чтобы уместить их в один пост. во второй части мы собрали не менее интересные работы, которые во многом определяют, как будет выглядеть генерация изображений и видео в 2026-м. why diffusion models don’t memorize: the role of implicit dynamical regularization in training работа, отобранная программным комитетом neurips 2025, как одна из лучших. авторы исследуют причины, по которым диффузионные модели генерируют новые изображения, а не воспроизводят в точности обучающую выборку. для модельных экспериментов берут датасет лиц celeba в низком разрешении и сгенерированный случайной двухслойной сетью. оказывается, что существуют две временные отметки: t_gen и t_mem, между которыми модель умеет создавать качественные примеры и при этом не в точности копировать данные из обучения. причём с увеличением количества данных интервал растёт. вывод: диффузионные модели обладают регуляризацией, которая позволяет им избегать переобучения даже при избыточной параметризации. на практике обучающие выборки очень велики и отметка t_mem недостижима. mean flows for one-step generative modeling группа исследователей из cmu и mit этой весной представила работу, где предложила способ обучения генеративных моделей — такой, чтобы они могли делать качественные генерации за один или мало шагов. в отличие от общепринятого сейчас подхода flow matching, моделирующего мгновенную скорость в точке, mean flow учится воспроизводить усредненную по участку траектории скорость, что даёт более надёжную и точную оценку пути из шума в данные. авторам удалось достичь лучшего качества одношаговой генерации на imagenet на момент выхода публикации. работа получила продолжение в статьях alphaflow и improved mean flows. diffusion transformers with representation autoencoders как известно, сейчас в генерации картинок и видео доминирует латентная диффузия: учат vae, чтобы перевести картинки в более низкоразмерное пространство, и потом — диффузионную модель уже в этом пространстве. авторы предложили вместо vae взять сотовый картиночный энкодер (dino, siglip), доучить к нему декодер и обучать диффузию в пространстве фичей этого энкодера. показывают, что диффузия, обученная в этом пространстве, сильно улучшает качество генерации. вероятно, это будет одно из самых популярных направлений ресёрча на ближайшие полгода-год, как было с repa. back to basics: let denoising generative models denoise исторически диффузионные модели чаще всего обучают предсказывать либо шум, который накладывается на картинку, либо разницу между шумом и чистой картинкой. в работе отмечают, что картинки в высоком разрешении, несмотря на большую размерность, лежат в сильно более низкоразмерном пространстве, и поэтому нейронке гораздо проще предсказывать чистую картинку, чем нечто с шумом, который захватывает всё пространство. исходя из этого, авторы предлагают простейшую диффузионную модель — jit (just image transformer), которая работает напрямую в пиксель-спейсе (без vae) и параметризована на предсказание чистой картинки. по архитектуре это обычный vit с минимальными диффузионными спецификами. показывают, что такая простая модель отлично работает на больших разрешениях, не требует дополнительных наворотов и внешних моделей. при этом по компьюту они даже эффективнее, чем латетные модели с vae. the principles of diffusion models классный учебник по диффузионным моделям от их «создателя» стефано эрмона. в книге куча пояснений, интуиции и обсуждений, которые помогают получить полную картину о том, что мы сейчас знаем про диффузию. покрыты почти все ключевые темы — от самой базы и до последних малошаговых моделей, а-ля meanflow. будет крайне полезным для тех, кто хочет глубоко разобраться с диффузией. статьи отобрали ❣ дмитрий баранчук и денис кузнеделев cv time лучшие статьи 2025 года: выбор авторов сv time. часть 2 хороших статей в прошлом году оказалось слишком много, чтобы уместить их в один пост. во второй части мы собрали не менее интересные работы, которые во многом определяют, как будет выглядеть генерация изображений и видео в 2026-м. why diffusion models don’t memorize: the role of implicit dynamical regularization in training работа, отобранная программным комитетом neurips 2025, как одна из лучших. авторы исследуют причины, по которым диффузионные модели генерируют новые изображения, а не воспроизводят в точности обучающую выборку. для модельных экспериментов берут датасет лиц celeba в низком разрешении и сгенерированный случайной двухслойной сетью. оказывается, что существуют две временные отметки: t_gen и t_mem, между которыми модель умеет создавать качественные примеры и при этом не в точности копировать данные из обучения. причём с увеличением количества данных интервал растёт. вывод: диффузионные модели обладают регуляризацией, которая позволяет им избегать переобучения даже при избыточной параметризации. на практике обучающие выборки очень велики и отметка t_mem недостижима. mean flows for one-step generative modeling группа исследователей из cmu и mit этой весной представила работу, где предложила способ обучения генеративных моделей — такой, чтобы они могли делать качественные генерации за один или мало шагов. в отличие от общепринятого сейчас подхода flow matching, моделирующего мгновенную скорость в точке, mean flow учится воспроизводить усредненную по участку траектории скорость, что даёт более надёжную и точную оценку пути из шума в данные. авторам удалось достичь лучшего качества одношаговой генерации на imagenet на момент выхода публикации. работа получила продолжение в статьях alphaflow и improved mean flows . diffusion transformers with representation autoencoders как известно, сейчас в генерации картинок и видео доминирует латентная диффузия: учат vae, чтобы перевести картинки в более низкоразмерное пространство, и потом — диффузионную модель уже в этом пространстве. авторы предложили вместо vae взять сотовый картиночный энкодер (dino, siglip), доучить к нему декодер и обучать диффузию в пространстве фичей этого энкодера. показывают, что диффузия, обученная в этом пространстве, сильно улучшает качество генерации. вероятно, это будет одно из самых популярных направлений ресёрча на ближайшие полгода-год, как было с repa . back to basics: let denoising generative models denoise исторически диффузионные модели чаще всего обучают предсказывать либо шум, который накладывается на картинку, либо разницу между шумом и чистой картинкой. в работе отмечают, что картинки в высоком разрешении, несмотря на большую размерность, лежат в сильно более низкоразмерном пространстве, и поэтому нейронке гораздо проще предсказывать чистую картинку, чем нечто с шумом, который захватывает всё пространство. исходя из этого, авторы предлагают простейшую диффузионную модель — jit (just image transformer), которая работает напрямую в пиксель-спейсе (без vae) и параметризована на предсказание чистой картинки. по архитектуре это обычный vit с минимальными диффузионными спецификами. показывают, что такая простая модель отлично работает на больших разрешениях, не требует дополнительных наворотов и внешних моделей. при этом по компьюту они даже эффективнее, чем латетные модели с vae. the principles of diffusion models классный учебник по диффузионным моделям от их «создателя» стефано эрмона. в книге куча пояснений, интуиции и обсуждений, которые помогают получить полную картину о том, что мы сейчас знаем про диффузию. покрыты почти все ключевые темы — от самой базы и до последних малошаговых моделей, а-ля meanflow . будет крайне полезным для тех, кто хочет глубоко разобраться с диффузией. статьи отобрали ❣ дмитрий баранчук и денис кузнеделев cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2026-01-16T07:48:02+00:00" href="./posts/229.html">2026-01-16 07:48 UTC</a></div>
      </div>
      <div class="post-body"><strong>Лучшие статьи 2025 года: выбор авторов СV Time. Часть 2<br></strong><br>Хороших статей в прошлом году оказалось слишком много, чтобы уместить их в один пост. Во второй части мы собрали не менее интересные работы, которые во многом определяют, как будет выглядеть генерация изображений и видео в 2026-м.<br><br><a href="https://openreview.net/forum?id=BSZqpqgqM0" rel="nofollow noopener noreferrer"><strong>Why Diffusion Models Don’t Memorize: The Role of Implicit Dynamical Regularization in Training <br></strong></a><br>Работа, отобранная программным комитетом NeurIPS 2025, как одна из лучших. Авторы исследуют причины, по которым диффузионные модели генерируют новые изображения, а не воспроизводят в точности обучающую выборку. Для модельных экспериментов берут датасет лиц Celeba в низком разрешении и сгенерированный случайной двухслойной сетью. Оказывается, что существуют две временные отметки: t_gen и t_mem, между которыми модель умеет создавать качественные примеры и при этом не в точности копировать данные из обучения. Причём с увеличением количества данных интервал растёт. Вывод: диффузионные модели обладают регуляризацией, которая позволяет им избегать переобучения даже при избыточной параметризации. На практике обучающие выборки очень велики и отметка t_mem недостижима.<br><br><a href="https://arxiv.org/abs/2505.13447" rel="nofollow noopener noreferrer"><strong>Mean Flows for One-step Generative Modeling <br></strong></a><br>Группа исследователей из CMU и MIT этой весной представила работу, где предложила способ обучения генеративных моделей — такой, чтобы они могли делать качественные генерации за один или мало шагов. В отличие от общепринятого сейчас подхода Flow Matching, моделирующего мгновенную скорость в точке, Mean Flow учится воспроизводить усредненную по участку траектории скорость, что даёт более надёжную и точную оценку пути из шума в данные. Авторам удалось достичь лучшего качества одношаговой генерации на ImageNet на момент выхода публикации. Работа получила продолжение в статьях <a href="https://arxiv.org/abs/2510.20771" rel="nofollow noopener noreferrer">AlphaFlow</a> и <a href="https://arxiv.org/abs/2512.02012" rel="nofollow noopener noreferrer">Improved Mean Flows</a>.<br><br><a href="https://arxiv.org/abs/2510.11690" rel="nofollow noopener noreferrer"><strong>Diffusion Transformers with Representation Autoencoders<br></strong></a><br>Как известно, сейчас в генерации картинок и видео доминирует латентная диффузия: учат VAE, чтобы перевести картинки в более низкоразмерное пространство, и потом — диффузионную модель уже в этом пространстве. Авторы предложили вместо VAE взять сотовый картиночный энкодер (Dino, Siglip), доучить к нему декодер и обучать диффузию в пространстве фичей этого энкодера. Показывают, что диффузия, обученная в этом пространстве, сильно улучшает качество генерации. Вероятно, это будет одно из самых популярных направлений ресёрча на ближайшие полгода-год, как было с <a href="https://arxiv.org/abs/2410.06940" rel="nofollow noopener noreferrer">REPA</a>. <br><br><a href="https://arxiv.org/abs/2511.13720" rel="nofollow noopener noreferrer"><strong>Back to Basics: Let Denoising Generative Models Denoise <br></strong></a><br>Исторически диффузионные модели чаще всего обучают предсказывать либо шум, который накладывается на картинку, либо разницу между шумом и чистой картинкой. В работе отмечают, что картинки в высоком разрешении, несмотря на большую размерность, лежат в сильно более низкоразмерном пространстве, и поэтому нейронке гораздо проще предсказывать чистую картинку, чем нечто с шумом, который захватывает всё пространство. Исходя из этого, авторы предлагают простейшую диффузионную модель — JiT (Just Image Transformer), которая работает напрямую в пиксель-спейсе (без VAE) и параметризована на предсказание чистой картинки. По архитектуре это обычный ViT с минимальными диффузионными спецификами. Показывают, что такая простая модель отлично работает на больших разрешениях, не требует дополнительных наворотов и внешних моделей. При этом по компьюту они даже эффективнее, чем латетные модели с VAE. <br><br><a href="https://arxiv.org/abs/2510.21890" rel="nofollow noopener noreferrer"><strong>The Principles of Diffusion Models <br></strong></a><br>Классный учебник по диффузионным моделям от их «создателя» Стефано Эрмона. В книге куча пояснений, интуиции и обсуждений, которые помогают получить полную картину о том, что мы сейчас знаем про диффузию. Покрыты почти все ключевые темы — от самой базы и до последних малошаговых моделей, а-ля <a href="https://arxiv.org/abs/2505.13447" rel="nofollow noopener noreferrer">MeanFlow</a>. Будет крайне полезным для тех, кто хочет глубоко разобраться с диффузией.<br><br><em>Статьи отобрали </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Дмитрий Баранчук и Денис Кузнеделев</em><br><a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer">CV Time</a></div>
      <div class="actions">
        <span>1 690 просмотров · 22 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/229" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/229.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="228" data-search="лучшие статьи 2025 года: выбор авторов сv time. часть 1 прошедший год оказался переломным для ai-рынка: монополия американских моделей пошатнулась, а в фокусе оказались китайские команды. они выложили в опенсорс большое количество сильных моделей — от ризонинг до мультимодальных. как заметил один из наших экспертов: «можно сказать, что весь год был китайским — и есть ощущение, что следующий тоже будет». alphaevolve: a coding agent for scientific and algorithmic discovery статья, которая описывает способ решения сложных задач путём применения эволюционного алгоритма поверх llm с большим контекстом. эта модель нашла более оптимальное решение для ряда открытых математических задач, в том числе обнаружила алгоритм перемножения комплекснозначных матриц размера 4x4, который требует меньше операций (скалярного) перемножения, чем алгоритм штрассена 1969 года. этот результат сильнейшие умы человечества не могли получить в течение 56 лет. открытие позволяет ускорить огромное количество вычислений в самых разных технических отраслях. emerging properties in unified multimodal pretraining работа о первой унифицированной мультимодальной модели bagel, выложенной в открытый доступ. модель умеет принимать на вход и выдавать на выходе любые комбинации текста и картинок. это позволяет в рамках одной vlm делать генерацию и редактирование картинок по тексту — возможности, которые раньше публично почти не были доступны. qwen3-vl technical report результат развития линейки моделей qwen-vl. на момент публикации представляет собой state-of-the-art опенсорсную vlm на большинстве мультимодальных бенчмарков. в статье авторы систематизируют ключевые принципы построения современных визуально-языковых моделей и подробно разбирают архитектурные новшества. среди них — усовершенствованный interleaved-mrope для корректного позиционного кодирования пространственно-временных данных, а также интеграция многоуровневых визуальных признаков через механизм deepstack. с этими решениями модель может эффективно работать с длинными контекстами и сложными визуально-текстовыми зависимостями. qwen-image technical report после vlm для распознавания и рассуждений логично посмотреть на вторую половину мультимодальности — генерацию и редактирование контента. здесь у qwen вышла отдельная модель: qwen-image, построенная на трансформерной архитектуре с 3d rope. модель отличается улучшенной генерацией текста и точностью редактирования изображений. также в статье описана структура датасета для мультимодального обучения модели. ui-tars-2 technical report: advancing gui agent with multi-turn reinforcement learning в 2025 году направление визуальных gui-агентов стало активно развиваться, и линейка ui-tars демонстрирует одни из лучших результатов в этом классе задач. в статье основной акцент сделан на тщательном подходе к формированию обучающих данных и на деталях онлайн multi-turn reinforcement learning. авторы подробно описывают асинхронную генерацию траекторий, дизайн reward-системы и использование специализированных доменных моделей для дальнейшего их объединения. такой подход позволяет агенту эффективно осваивать сложные многошаговые сценарии взаимодействия с интерфейсами. emu3.5: native multimodal models are world learners в статье реализован унифицированный подход к обучению предсказания картиночных и текстовых токенов. он позволяет модели лучше улавливать причинно-следственные связи и переносить знания между модальностями, что улучшает результаты в задачах восприятия, рассуждения и генерации. dinov3 статья, в которой описано развитие одного из самых сильных визуальных бэкбонов. такие модели становятся стандартом визуальных бэкбонов; напрямую влияют на качество vlm, ocr, видео- и downstream-задач; масштабируются лучше многих альтернатив; используются как учителя для дистилляции. продолжение следует. статьи отобрали ❣ александр устюжанин, данил кашин и александр шишеня cv time лучшие статьи 2025 года: выбор авторов сv time. часть 1 прошедший год оказался переломным для ai-рынка: монополия американских моделей пошатнулась, а в фокусе оказались китайские команды. они выложили в опенсорс большое количество сильных моделей — от ризонинг до мультимодальных. как заметил один из наших экспертов: «можно сказать, что весь год был китайским — и есть ощущение, что следующий тоже будет». alphaevolve: a coding agent for scientific and algorithmic discovery статья, которая описывает способ решения сложных задач путём применения эволюционного алгоритма поверх llm с большим контекстом. эта модель нашла более оптимальное решение для ряда открытых математических задач, в том числе обнаружила алгоритм перемножения комплекснозначных матриц размера 4x4, который требует меньше операций (скалярного) перемножения, чем алгоритм штрассена 1969 года. этот результат сильнейшие умы человечества не могли получить в течение 56 лет. открытие позволяет ускорить огромное количество вычислений в самых разных технических отраслях. emerging properties in unified multimodal pretraining работа о первой унифицированной мультимодальной модели bagel, выложенной в открытый доступ. модель умеет принимать на вход и выдавать на выходе любые комбинации текста и картинок. это позволяет в рамках одной vlm делать генерацию и редактирование картинок по тексту — возможности, которые раньше публично почти не были доступны. qwen3-vl technical report результат развития линейки моделей qwen-vl. на момент публикации представляет собой state-of-the-art опенсорсную vlm на большинстве мультимодальных бенчмарков. в статье авторы систематизируют ключевые принципы построения современных визуально-языковых моделей и подробно разбирают архитектурные новшества. среди них — усовершенствованный interleaved-mrope для корректного позиционного кодирования пространственно-временных данных, а также интеграция многоуровневых визуальных признаков через механизм deepstack. с этими решениями модель может эффективно работать с длинными контекстами и сложными визуально-текстовыми зависимостями. qwen-image technical report после vlm для распознавания и рассуждений логично посмотреть на вторую половину мультимодальности — генерацию и редактирование контента. здесь у qwen вышла отдельная модель: qwen-image, построенная на трансформерной архитектуре с 3d rope. модель отличается улучшенной генерацией текста и точностью редактирования изображений. также в статье описана структура датасета для мультимодального обучения модели. ui-tars-2 technical report: advancing gui agent with multi-turn reinforcement learning в 2025 году направление визуальных gui-агентов стало активно развиваться, и линейка ui-tars демонстрирует одни из лучших результатов в этом классе задач. в статье основной акцент сделан на тщательном подходе к формированию обучающих данных и на деталях онлайн multi-turn reinforcement learning. авторы подробно описывают асинхронную генерацию траекторий, дизайн reward-системы и использование специализированных доменных моделей для дальнейшего их объединения. такой подход позволяет агенту эффективно осваивать сложные многошаговые сценарии взаимодействия с интерфейсами. emu3.5: native multimodal models are world learners в статье реализован унифицированный подход к обучению предсказания картиночных и текстовых токенов. он позволяет модели лучше улавливать причинно-следственные связи и переносить знания между модальностями, что улучшает результаты в задачах восприятия, рассуждения и генерации. dinov3 статья, в которой описано развитие одного из самых сильных визуальных бэкбонов. такие модели становятся стандартом визуальных бэкбонов; напрямую влияют на качество vlm, ocr, видео- и downstream-задач; масштабируются лучше многих альтернатив; используются как учителя для дистилляции. продолжение следует. статьи отобрали ❣ александр устюжанин, данил кашин и александр шишеня cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2026-01-13T08:24:01+00:00" href="./posts/228.html">2026-01-13 08:24 UTC</a></div>
      </div>
      <div class="post-body"><strong>Лучшие статьи 2025 года: выбор авторов СV Time. Часть 1<br></strong><br>Прошедший год оказался переломным для AI-рынка: монополия американских моделей пошатнулась, а в фокусе оказались китайские команды. Они выложили в опенсорс большое количество сильных моделей — от ризонинг до мультимодальных. Как заметил один из наших экспертов: «Можно сказать, что весь год был китайским — и есть ощущение, что следующий тоже будет».<br><br><a href="https://arxiv.org/abs/2506.13131" rel="nofollow noopener noreferrer"><strong>AlphaEvolve: A coding agent for scientific and algorithmic discovery</strong></a><br><br>Статья, которая описывает способ решения сложных задач путём применения эволюционного алгоритма поверх LLM с большим контекстом. Эта модель нашла более оптимальное решение для ряда открытых математических задач, в том числе обнаружила алгоритм перемножения комплекснозначных матриц размера 4x4, который требует меньше операций (скалярного) перемножения, чем алгоритм Штрассена 1969 года. Этот результат сильнейшие умы человечества не могли получить в течение 56 лет. Открытие позволяет ускорить огромное количество вычислений в самых разных технических отраслях.<br><br><a href="https://arxiv.org/abs/2505.14683v1" rel="nofollow noopener noreferrer"><strong>Emerging Properties in Unified Multimodal Pretraining</strong></a><br><br>Работа о первой унифицированной мультимодальной модели Bagel, выложенной в открытый доступ. Модель умеет принимать на вход и выдавать на выходе любые комбинации текста и картинок. Это позволяет в рамках одной VLM делать генерацию и редактирование картинок по тексту — возможности, которые раньше публично почти не были доступны.<br><br><a href="https://arxiv.org/abs/2511.21631" rel="nofollow noopener noreferrer"><strong>Qwen3-VL Technical Report</strong></a><br><br>Результат развития линейки моделей Qwen-VL. На момент публикации представляет собой state-of-the-art опенсорсную VLM на большинстве мультимодальных бенчмарков. В статье авторы систематизируют ключевые принципы построения современных визуально-языковых моделей и подробно разбирают архитектурные новшества. Среди них — усовершенствованный interleaved-MRoPE для корректного позиционного кодирования пространственно-временных данных, а также интеграция многоуровневых визуальных признаков через механизм DeepStack. С этими решениями модель может эффективно работать с длинными контекстами и сложными визуально-текстовыми зависимостями.<br><br><a href="https://arxiv.org/abs/2508.02324" rel="nofollow noopener noreferrer"><strong>Qwen-Image Technical Report<br></strong></a><br>После VLM для распознавания и рассуждений логично посмотреть на вторую половину мультимодальности — генерацию и редактирование контента. Здесь у Qwen вышла отдельная модель: Qwen-Image, построенная на трансформерной архитектуре с 3D RoPE. Модель отличается улучшенной генерацией текста и точностью редактирования изображений. Также в статье описана структура датасета для мультимодального обучения модели.<br><br><a href="https://arxiv.org/abs/2509.02544" rel="nofollow noopener noreferrer"><strong>UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning</strong></a><br><br>В 2025 году направление визуальных GUI-агентов стало активно развиваться, и линейка UI-TARS демонстрирует одни из лучших результатов в этом классе задач. В статье основной акцент сделан на тщательном подходе к формированию обучающих данных и на деталях онлайн multi-turn reinforcement learning. Авторы подробно описывают асинхронную генерацию траекторий, дизайн reward-системы и использование специализированных доменных моделей для дальнейшего их объединения. Такой подход позволяет агенту эффективно осваивать сложные многошаговые сценарии взаимодействия с интерфейсами.<br><br><a href="https://arxiv.org/abs/2510.26583" rel="nofollow noopener noreferrer"><strong>Emu3.5: Native Multimodal Models are World Learners</strong></a><br><br>В статье реализован унифицированный подход к обучению предсказания картиночных и текстовых токенов. Он позволяет модели лучше улавливать причинно-следственные связи и переносить знания между модальностями, что улучшает результаты в задачах восприятия, рассуждения и генерации.<br><br><a href="https://arxiv.org/abs/2508.10104" rel="nofollow noopener noreferrer"><strong>DINOv3<br></strong></a><br>Статья, в которой описано развитие одного из самых сильных визуальных бэкбонов. Такие модели становятся стандартом визуальных бэкбонов; напрямую влияют на качество VLM, OCR, видео- и downstream-задач; масштабируются лучше многих альтернатив; используются как учителя для дистилляции.<br><br>Продолжение следует.<br><br><em>Статьи отобрали </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Александр Устюжанин, Данил Кашин и Александр Шишеня<br></em><a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer">CV Time</a></div>
      <div class="actions">
        <span>1 430 просмотров · 28 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/228" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/228.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="227" data-search="🎉итоги года в cv time: посты, которые читали чаще всего пока все постепенно уходят в мандариново-выходной режим, мы решили подвести итоги года, собрав самые популярные публикации в канале за 2025-й. это уже стало праздничной традицией, которую мы рады разделить с вами, дорогие читатели, и заодно — поздравить вас с наступающим новым годом! а если считаете, что в топе чего-то не хватает, приходите обсуждать в комментарии. yandex alchemist: открытый датасет для буста text-to-image генерации пост, в котором исследователи yandex research подробно рассказали, как получить датасет уровня alchemist, имея лишь сырой набор интернет-данных. интересное (и даже эксклюзивное) дополнение от авторов к основной статье. кстати, в этом году работа успела съездить на neurips 2025. эволюция florence: от генеративных моделей к mllm в этом посте егор шестопалов сравнил сразу две статьи о семействе моделей florence. и пусть по прошествии времени можно сказать, что идея использовать в качестве энкодера в vlm florence-2 не прижилась, зато разбор получился полезным и собрал свою порцию просмотров. главные инсайты cv week из первых рук карточки, на которых инженеры из яндекса рассказывают самое интересное об онлайн-интенсиве по компьютерному зрению, организованном вместе со школой анализа данных. рекомендуем полистать, если хотите вспомнить, как это было. а для ностальгии на максималках можно заглянуть ещё и на этот лендинг. foundationstereo: zero-shot stereo matching леонид штанько разобрал статью nvidia о восстановлении глубины по стереопаре — двум изображениям, снятым близко расположенными камерами. камеры смотрят в одном направлении, поэтому каждая 3d-точка оказывается примерно на одной строке в обоих кадрах, но в разных местах. это упрощает поиск соответствий между пикселями и позволяет восстановить глубину сцены. ключевые идеи работы вы найдёте в нашем посте. improving the diffusability of autoencoders завершаем подборку разбором от сергея кастрюлина на тему diffusability латентного пространства. авторы статьи выясняют, насколько легко диффузионной модели учиться на латентах автоэнкодера. проблема локальная, но зато в статье есть понятная идея и измеримый эффект. если ещё не читали, приглашаем ознакомиться. надеемся, что наступающий год принесёт индустрии, научному сообществу и нам с вами ещё больше вдохновляющих работ на тему компьютерного зрения. а мы будем и дальше держать вас в курсе самого полезного и интересного! cv time 🎉 итоги года в cv time: посты, которые читали чаще всего пока все постепенно уходят в мандариново-выходной режим, мы решили подвести итоги года, собрав самые популярные публикации в канале за 2025-й. это уже стало праздничной традицией, которую мы рады разделить с вами, дорогие читатели, и заодно — поздравить вас с наступающим новым годом! а если считаете, что в топе чего-то не хватает, приходите обсуждать в комментарии. yandex alchemist: открытый датасет для буста text-to-image генерации пост, в котором исследователи yandex research подробно рассказали, как получить датасет уровня alchemist, имея лишь сырой набор интернет-данных. интересное (и даже эксклюзивное) дополнение от авторов к основной статье . кстати, в этом году работа успела съездить на neurips 2025. эволюция florence: от генеративных моделей к mllm в этом посте егор шестопалов сравнил сразу две статьи о семействе моделей florence. и пусть по прошествии времени можно сказать, что идея использовать в качестве энкодера в vlm florence-2 не прижилась, зато разбор получился полезным и собрал свою порцию просмотров. главные инсайты cv week из первых рук карточки, на которых инженеры из яндекса рассказывают самое интересное об онлайн-интенсиве по компьютерному зрению, организованном вместе со школой анализа данных. рекомендуем полистать, если хотите вспомнить, как это было. а для ностальгии на максималках можно заглянуть ещё и на этот лендинг. foundationstereo: zero-shot stereo matching леонид штанько разобрал статью nvidia о восстановлении глубины по стереопаре — двум изображениям, снятым близко расположенными камерами. камеры смотрят в одном направлении, поэтому каждая 3d-точка оказывается примерно на одной строке в обоих кадрах, но в разных местах. это упрощает поиск соответствий между пикселями и позволяет восстановить глубину сцены. ключевые идеи работы вы найдёте в нашем посте. improving the diffusability of autoencoders завершаем подборку разбором от сергея кастрюлина на тему diffusability латентного пространства. авторы статьи выясняют, насколько легко диффузионной модели учиться на латентах автоэнкодера. проблема локальная, но зато в статье есть понятная идея и измеримый эффект. если ещё не читали, приглашаем ознакомиться. надеемся, что наступающий год принесёт индустрии, научному сообществу и нам с вами ещё больше вдохновляющих работ на тему компьютерного зрения. а мы будем и дальше держать вас в курсе самого полезного и интересного! cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-12-29T12:15:10+00:00" href="./posts/227.html">2025-12-29 12:15 UTC</a></div>
      </div>
      <div class="post-body"><tg-emoji emoji-id="5341780233399848004">🎉</tg-emoji><strong>Итоги года в CV Time: посты, которые читали чаще всего</strong><br><br>Пока все постепенно уходят в мандариново-выходной режим, мы решили подвести итоги года, собрав самые популярные публикации в канале за 2025-й. Это уже стало праздничной традицией, которую мы рады разделить с вами, дорогие читатели, и заодно — поздравить вас с наступающим Новым годом! А если считаете, что в топе чего-то не хватает, приходите обсуждать в комментарии.<br><br><a href="https://t.me/timeforcv/140" rel="nofollow noopener noreferrer"><strong>Yandex Alchemist: открытый датасет для буста text-to-image генерации<br></strong></a><br>Пост, в котором исследователи Yandex Research подробно рассказали, как получить датасет уровня Alchemist, имея лишь сырой набор интернет-данных. Интересное (и даже эксклюзивное) дополнение от авторов к <a href="https://arxiv.org/abs/2505.19297v1" rel="nofollow noopener noreferrer">основной статье</a>. Кстати, в этом году работа успела <a href="https://t.me/MLunderhood/235" rel="nofollow noopener noreferrer">съездить</a> на NeurIPS 2025.<br><br><a href="https://t.me/timeforcv/180" rel="nofollow noopener noreferrer"><strong>Эволюция Florence: от генеративных моделей к MLLM<br></strong></a><br>В этом посте Егор Шестопалов сравнил сразу две статьи о семействе моделей Florence. И пусть по прошествии времени можно сказать, что идея использовать в качестве энкодера в VLM Florence-2 не прижилась, зато разбор получился полезным и собрал свою порцию просмотров.<br><br><a href="https://t.me/timeforcv/65" rel="nofollow noopener noreferrer"><strong>Главные инсайты CV Week из первых рук<br></strong></a><br>Карточки, на которых инженеры из Яндекса рассказывают самое интересное об онлайн-интенсиве по компьютерному зрению, организованном вместе со Школой анализа данных. Рекомендуем полистать, если хотите вспомнить, как это было. А для ностальгии на максималках можно заглянуть ещё и на этот лендинг.<br><br><a href="https://t.me/timeforcv/85" rel="nofollow noopener noreferrer"><strong>FoundationStereo: Zero-Shot Stereo Matching</strong></a><br><br>Леонид Штанько разобрал статью NVIDIA о восстановлении глубины по стереопаре — двум изображениям, снятым близко расположенными камерами. Камеры смотрят в одном направлении, поэтому каждая 3D-точка оказывается примерно на одной строке в обоих кадрах, но в разных местах. Это упрощает поиск соответствий между пикселями и позволяет восстановить глубину сцены. Ключевые идеи работы вы найдёте в нашем посте.<br><br><a href="https://t.me/timeforcv/143" rel="nofollow noopener noreferrer"><strong>Improving the Diffusability of Autoencoders<br></strong></a><br>Завершаем подборку разбором от Сергея Кастрюлина на тему diffusability латентного пространства. Авторы статьи выясняют, насколько легко диффузионной модели учиться на латентах автоэнкодера. Проблема локальная, но зато в статье есть понятная идея и измеримый эффект. Если ещё не читали, приглашаем ознакомиться.<br><br>Надеемся, что наступающий год принесёт индустрии, научному сообществу и нам с вами ещё больше вдохновляющих работ на тему компьютерного зрения. А мы будем и дальше держать вас в курсе самого полезного и интересного! <br><br><a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer">CV Time</a></div>
      <div class="actions">
        <span>1 751 просмотров · 31 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/227" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/227.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="226" data-search="современные нейросетевые модели для глобального прогноза погоды недавно вышла модель weathernext-2 от google, и мы решили рассказать не только о ней, но и в целом о задаче и моделях глобального прогноза погоды. глобальный прогноз погоды — это задача прогноза эволюции всей земной атмосферы на несколько дней вперёд. классический подход — численный прогноз погоды (nwp), в котором численно решается система связанных дифференциальных уравнений гидродинамики. такие расчёты выполняются на суперкомпьютерах более часа, поэтому прогноз на ближайшие часы нельзя получить мгновенно. из-за хаотичной природы атмосферных процессов применяется ансамблирование: прогноз запускают с немного различающихся начальных условий, получая десятки возможных сценариев. ансамблевый прогноз — наиболее точный, он позволяет оценить вероятности событий. к 2025 году сформировались базовые требования к dl-моделям глобального прогноза: — пространственное разрешение не грубее 0,25°по широтам и долготам (~28×28 км); — соответствие спектров (проверка физичности); — наличие осадков и желательно метрик, отличных от mae/rmse; — поддержка ансамблей. ключевым фактором развития dl-подходов стало усвоение данных. современные техники ассимиляции позволили пересобрать архив наблюдений с 1940 года, получив era5 — самый полный и согласованный датасет состояния атмосферы на сетке 0,25°. доступность большого числа качественных данных — благодатная почва для dl-подхода. стандартный вход dl-моделей — около 72 карт (приземные переменные, переменные по уровням давления и статические поля). обзор основных моделей за последние годы появились dl-модели глобального прогноза: pangu weather, graphcast, aurora, gencast. все они используют era5 и авторегрессионно транслируют состояние атмосферы в будущее. pangu weather показала, что «картиночная» модель может воспроизводить крупномасштабную динамику, но ансамбли через шум в начальных условиях оказались некачественными. graphcast использует графовую архитектуру на икосаэдрической сетке и задаёт планку качества для детерминистских моделей. gencast расширил этот подход, применив диффузию для получения ансамблей, что позволило уменьшить «мыло» и лучше моделировать экстремумы, но ценой более медленного инференса. при этом выяснилось, что стандартных метрик (lw-rmse и acc) недостаточно: многие модели не проходят проверку на физичность по спектрам. несоответствие спектров означает, что модель не улавливает вариации энергии на мелких масштабах, и неэффективно использует высокое разрешение. weathernext-2 weathernext-2 — третья итерация модели google. это вероятностная модель, которая напрямую оптимизируется по crps и строит ансамбли без диффузии. ключевая идея — декомпозиция неопределённости: — эпистемическая неопределённость моделируется deep-ансамблем (четыре модели с разными сидами); — алеаторическая неопределённость моделируется через функциональные возмущения: для каждого члена ансамбля и шага сэмплируется один глобальный 32-мерный шумовой вектор, который через conditional layer norm подаётся во все слои модели. архитектура сохраняет подход graphcast: переход grid→mesh, граф-трансформер на mesh и обратное отображение. глобальный низкоразмерный шум, применяемый ко всем слоям и пространственным точкам, задаёт согласованную пространственную вариативность. модель работает с шагом шесть часов и делает полный 15-дневный прогноз ансамбля менее чем за минуту на одном tpu, что значительно быстрее gencast. по метрикам crps и rmse среднего ансамбля weathernext-2 превосходит gencast и приближается к численным ансамблям. про осадки в статье сообщается скупо, спектры лучше, чем у gencast, но хуже, чем у fourcastnetv3. в целом weathernext-2 показывает, что можно получить быстрый ансамбль без диффузии и существенно улучшить качество по сравнению с предыдущими нейромоделями. при этом ключевые вопросы о соответствии спектров и корректной работе с осадками остаются. разбор подготовил ❣ павел анисимов cv time современные нейросетевые модели для глобального прогноза погоды недавно вышла модель weathernext-2 от google, и мы решили рассказать не только о ней, но и в целом о задаче и моделях глобального прогноза погоды. глобальный прогноз погоды — это задача прогноза эволюции всей земной атмосферы на несколько дней вперёд. классический подход — численный прогноз погоды (nwp), в котором численно решается система связанных дифференциальных уравнений гидродинамики. такие расчёты выполняются на суперкомпьютерах более часа, поэтому прогноз на ближайшие часы нельзя получить мгновенно. из-за хаотичной природы атмосферных процессов применяется ансамблирование: прогноз запускают с немного различающихся начальных условий, получая десятки возможных сценариев. ансамблевый прогноз — наиболее точный, он позволяет оценить вероятности событий. к 2025 году сформировались базовые требования к dl-моделям глобального прогноза: — пространственное разрешение не грубее 0,25°по широтам и долготам (~28×28 км); — соответствие спектров (проверка физичности); — наличие осадков и желательно метрик, отличных от mae/rmse; — поддержка ансамблей. ключевым фактором развития dl-подходов стало усвоение данных. современные техники ассимиляции позволили пересобрать архив наблюдений с 1940 года, получив era5 — самый полный и согласованный датасет состояния атмосферы на сетке 0,25°. доступность большого числа качественных данных — благодатная почва для dl-подхода. стандартный вход dl-моделей — около 72 карт (приземные переменные, переменные по уровням давления и статические поля). обзор основных моделей за последние годы появились dl-модели глобального прогноза: pangu weather, graphcast, aurora, gencast. все они используют era5 и авторегрессионно транслируют состояние атмосферы в будущее. pangu weather показала, что «картиночная» модель может воспроизводить крупномасштабную динамику, но ансамбли через шум в начальных условиях оказались некачественными. graphcast использует графовую архитектуру на икосаэдрической сетке и задаёт планку качества для детерминистских моделей. gencast расширил этот подход, применив диффузию для получения ансамблей, что позволило уменьшить «мыло» и лучше моделировать экстремумы, но ценой более медленного инференса. при этом выяснилось, что стандартных метрик (lw-rmse и acc) недостаточно: многие модели не проходят проверку на физичность по спектрам. несоответствие спектров означает, что модель не улавливает вариации энергии на мелких масштабах, и неэффективно использует высокое разрешение. weathernext-2 weathernext-2 — третья итерация модели google. это вероятностная модель, которая напрямую оптимизируется по crps и строит ансамбли без диффузии. ключевая идея — декомпозиция неопределённости: — эпистемическая неопределённость моделируется deep-ансамблем (четыре модели с разными сидами); — алеаторическая неопределённость моделируется через функциональные возмущения: для каждого члена ансамбля и шага сэмплируется один глобальный 32-мерный шумовой вектор, который через conditional layer norm подаётся во все слои модели. архитектура сохраняет подход graphcast: переход grid→mesh, граф-трансформер на mesh и обратное отображение. глобальный низкоразмерный шум, применяемый ко всем слоям и пространственным точкам, задаёт согласованную пространственную вариативность. модель работает с шагом шесть часов и делает полный 15-дневный прогноз ансамбля менее чем за минуту на одном tpu, что значительно быстрее gencast. по метрикам crps и rmse среднего ансамбля weathernext-2 превосходит gencast и приближается к численным ансамблям. про осадки в статье сообщается скупо, спектры лучше, чем у gencast, но хуже, чем у fourcastnetv3. в целом weathernext-2 показывает, что можно получить быстрый ансамбль без диффузии и существенно улучшить качество по сравнению с предыдущими нейромоделями. при этом ключевые вопросы о соответствии спектров и корректной работе с осадками остаются. разбор подготовил ❣ павел анисимов cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-12-24T11:17:33+00:00" href="./posts/226.html">2025-12-24 11:17 UTC</a></div>
      </div>
      <div class="post-body"><strong>Современные нейросетевые модели для глобального прогноза погоды</strong><br><br>Недавно вышла модель <a href="https://deepmind.google/science/weathernext/" rel="nofollow noopener noreferrer"><strong>WeatherNext-2</strong></a> от Google, и мы решили рассказать не только о ней, но и в целом о задаче и моделях глобального прогноза погоды.<br><br>Глобальный прогноз погоды — это задача прогноза эволюции всей земной атмосферы на несколько дней вперёд. Классический подход — численный прогноз погоды (NWP), в котором численно решается система связанных дифференциальных уравнений гидродинамики. Такие расчёты выполняются на суперкомпьютерах более часа, поэтому прогноз на ближайшие часы нельзя получить мгновенно.<br><br>Из-за хаотичной природы атмосферных процессов применяется ансамблирование: прогноз запускают с немного различающихся начальных условий, получая десятки возможных сценариев. Ансамблевый прогноз — наиболее точный, он позволяет оценить вероятности событий.<br><br>К 2025 году сформировались базовые требования к DL-моделям глобального прогноза:<br><br>— пространственное разрешение не грубее 0,25°по широтам и долготам (~28×28 км);<br>— соответствие спектров (проверка физичности);<br>— наличие осадков и желательно метрик, отличных от MAE/RMSE;<br>— поддержка ансамблей.<br><br>Ключевым фактором развития DL-подходов стало усвоение данных. Современные техники ассимиляции позволили пересобрать архив наблюдений с 1940 года, получив ERA5 — самый полный и согласованный датасет состояния атмосферы на сетке 0,25°. Доступность большого числа качественных данных — благодатная почва для DL-подхода. Стандартный вход DL-моделей — около 72 карт (приземные переменные, переменные по уровням давления и статические поля).<br><br><strong>Обзор основных моделей</strong><br><br>За последние годы появились DL-модели глобального прогноза: Pangu Weather, GraphCast, Aurora, GenCast. Все они используют ERA5 и авторегрессионно транслируют состояние атмосферы в будущее.<br><br><a href="https://arxiv.org/abs/2211.02556" rel="nofollow noopener noreferrer"><strong>Pangu Weather</strong></a> показала, что «картиночная» модель может воспроизводить крупномасштабную динамику, но ансамбли через шум в начальных условиях оказались некачественными.<br><br><a href="https://arxiv.org/abs/2212.12794" rel="nofollow noopener noreferrer"><strong>GraphCast</strong></a> использует графовую архитектуру на икосаэдрической сетке и задаёт планку качества для детерминистских моделей. <a href="https://arxiv.org/abs/2312.15796v2" rel="nofollow noopener noreferrer"><strong>GenCast</strong></a> расширил этот подход, применив диффузию для получения ансамблей, что позволило уменьшить «мыло» и лучше моделировать экстремумы, но ценой более медленного инференса.<br><br>При этом выяснилось, что стандартных метрик (LW-RMSE и ACC) недостаточно: многие модели не проходят проверку на физичность по спектрам. Несоответствие спектров означает, что модель не улавливает вариации энергии на мелких масштабах, и неэффективно использует высокое разрешение.<br><br><a href="https://deepmind.google/science/weathernext/" rel="nofollow noopener noreferrer"><strong>WeatherNext-2 </strong></a><br><br>WeatherNext-2 — третья итерация модели Google. Это вероятностная модель, которая напрямую оптимизируется по CRPS и строит ансамбли без диффузии.<br><br>Ключевая идея — декомпозиция неопределённости:<br><br>— эпистемическая неопределённость моделируется deep-ансамблем (четыре модели с разными сидами);<br><br>— алеаторическая неопределённость моделируется через функциональные возмущения: для каждого члена ансамбля и шага сэмплируется один глобальный 32-мерный шумовой вектор, который через conditional layer norm подаётся во все слои модели.<br><br>Архитектура сохраняет подход GraphCast: переход grid→mesh, граф-трансформер на mesh и обратное отображение. Глобальный низкоразмерный шум, применяемый ко всем слоям и пространственным точкам, задаёт согласованную пространственную вариативность.<br><br>Модель работает с шагом шесть часов и делает полный 15-дневный прогноз ансамбля менее чем за минуту на одном TPU, что значительно быстрее GenCast. По метрикам CRPS и RMSE среднего ансамбля WeatherNext-2 превосходит GenCast и приближается к численным ансамблям. Про осадки в статье сообщается скупо, спектры лучше, чем у GenCast, но хуже, чем у FourCastNetV3.<br><br>В целом WeatherNext-2 показывает, что можно получить быстрый ансамбль без диффузии и существенно улучшить качество по сравнению с предыдущими нейромоделями.<br><br>При этом ключевые вопросы о соответствии спектров и корректной работе с осадками остаются. <br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Павел Анисимов <br></em><a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer">CV Time</a></div>
      <div class="actions">
        <span>3 756 просмотров · 23 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/226" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/226.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="225" data-search="sana-sprint: one-step diffusion with continuous-time consistency distillation сегодня разбираем статью от nvidia, в которой высокая скорость достигается в первую очередь за счёт генерации изображений в малое число шагов с приемлемым качеством. прошлые версии sana быстро генерировали благодаря vae с большим downsampling-фактором, а в sana sprint добились ещё большего ускорения с помощью дистилляции по шагам. основа работы — идея continuous-time consistency моделей, о которой ещё осенью прошлого года говорил yang song. по сути, она описывает движение от шума к сигналу через временную производную, превращая дискретный диффузионный процесс в непрерывный поток динамики. сontinuous-time consistency позволяет достигать качественных генераций в малое число шагов, но есть и нюанс. модель должна быть обучена со специальной trigflow-параметризацией, а имеющиеся диффузионные модели обычно используют стандартную flow-matching-постановку. поэтому следующая задача — правильно «перевести» предобученную модель в нужное представление. sana-sprint решает это с помощью серии преобразований: — переноса временной шкалы в тригонометрические координаты (cos / sin), — масштабирования латентов, чтобы шум совпадал по дисперсии с данными, — трансформации выходной head-функции, чтобы предсказания соответствовали формуле consistency-динамики. но перенести диффузионку в новую параметризацию — это только половина дела. вторая часть — заставить всё это стабильно учиться. и вот здесь начинаются инженерные приключения. стабильность «улетает в космос» из-за того, что временной эмбеддинг использует слишком большой масштаб шума — из-за этого производные становятся огромными. лечится это просто: нужно изменить масштаб частот эмбеддинга и немного дообучить модель, буквально несколько тысяч итераций. вторая проблема — большие нормы градиентов в механизме внимания. решение довольно стандартное: добавить rmsnorm на q/k (qk-normalization) в self- и cross-attention, после чего обучение стабилизируется. теперь самое главное — скорость. в разрешении 1024×1024 sana-sprint выдаёт картинку за ~0,1–0,18 секунды при одношаговой генерации. из них на сам трансформер уходит ≈0,03 секунды, остальное — vae-декодер, который становится основным бутылочным горлышком. по времени работы диффузионной модели sana-sprint быстрее flux-schnell примерно в 65 раз, а по end-to-end-задержке — примерно в 10 раз. то есть «быстро» тут — не просто эпитет. итоговое качество вполне пристойное: на 1–4 шагах она даёт fid и geneval на уровне или лучше, чем у других быстрых моделей. например, не уступает flux-schnell по метрикам (7,59 против 7,94 по fid и 0,74 против 0,71 по geneval), будучи заметно быстрее. разбор подготовил ❣ денис кузнеделев cv time sana-sprint: one-step diffusion with continuous-time consistency distillation сегодня разбираем статью от nvidia, в которой высокая скорость достигается в первую очередь за счёт генерации изображений в малое число шагов с приемлемым качеством. прошлые версии sana быстро генерировали благодаря vae с большим downsampling-фактором, а в sana sprint добились ещё большего ускорения с помощью дистилляции по шагам. основа работы — идея continuous-time consistency моделей, о которой ещё осенью прошлого года говорил yang song. по сути, она описывает движение от шума к сигналу через временную производную, превращая дискретный диффузионный процесс в непрерывный поток динамики. сontinuous-time consistency позволяет достигать качественных генераций в малое число шагов, но есть и нюанс. модель должна быть обучена со специальной trigflow-параметризацией, а имеющиеся диффузионные модели обычно используют стандартную flow-matching-постановку. поэтому следующая задача — правильно «перевести» предобученную модель в нужное представление. sana-sprint решает это с помощью серии преобразований: — переноса временной шкалы в тригонометрические координаты (cos / sin), — масштабирования латентов, чтобы шум совпадал по дисперсии с данными, — трансформации выходной head-функции, чтобы предсказания соответствовали формуле consistency-динамики. но перенести диффузионку в новую параметризацию — это только половина дела. вторая часть — заставить всё это стабильно учиться. и вот здесь начинаются инженерные приключения. стабильность «улетает в космос» из-за того, что временной эмбеддинг использует слишком большой масштаб шума — из-за этого производные становятся огромными. лечится это просто: нужно изменить масштаб частот эмбеддинга и немного дообучить модель, буквально несколько тысяч итераций. вторая проблема — большие нормы градиентов в механизме внимания. решение довольно стандартное: добавить rmsnorm на q/k (qk-normalization) в self- и cross-attention, после чего обучение стабилизируется. теперь самое главное — скорость. в разрешении 1024×1024 sana-sprint выдаёт картинку за ~0,1–0,18 секунды при одношаговой генерации. из них на сам трансформер уходит ≈0,03 секунды, остальное — vae-декодер, который становится основным бутылочным горлышком. по времени работы диффузионной модели sana-sprint быстрее flux-schnell примерно в 65 раз, а по end-to-end-задержке — примерно в 10 раз. то есть «быстро» тут — не просто эпитет. итоговое качество вполне пристойное: на 1–4 шагах она даёт fid и geneval на уровне или лучше, чем у других быстрых моделей. например, не уступает flux-schnell по метрикам (7,59 против 7,94 по fid и 0,74 против 0,71 по geneval), будучи заметно быстрее. разбор подготовил ❣ денис кузнеделев cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-12-15T11:58:01+00:00" href="./posts/225.html">2025-12-15 11:58 UTC</a></div>
      </div>
      <div class="post-body"><strong>SANA-Sprint: One-Step Diffusion with Continuous-Time Consistency Distillation</strong><br><br>Сегодня разбираем <a href="https://arxiv.org/abs/2503.09641v3" rel="nofollow noopener noreferrer">статью</a> от NVIDIA, в которой высокая скорость достигается в первую очередь за счёт генерации изображений в малое число шагов с приемлемым качеством. Прошлые версии SANA быстро генерировали благодаря VAE с большим downsampling-фактором, а в SANA Sprint добились ещё большего ускорения с помощью дистилляции по шагам.<br><br>Основа работы — идея continuous-time consistency моделей, о которой ещё осенью прошлого года <a href="https://arxiv.org/abs/2410.11081" rel="nofollow noopener noreferrer">говорил</a> Yang Song. По сути, она описывает движение от шума к сигналу через временную производную, превращая дискретный диффузионный процесс в непрерывный поток динамики.<br><br>Сontinuous-time consistency позволяет достигать качественных генераций в малое число шагов, но есть и нюанс. Модель должна быть обучена со специальной TrigFlow-параметризацией, а имеющиеся диффузионные модели обычно используют стандартную flow-matching-постановку. Поэтому следующая задача — правильно «перевести» предобученную модель в нужное представление.<br><br>SANA-Sprint решает это с помощью серии преобразований:<br>— переноса временной шкалы в тригонометрические координаты (cos / sin),<br>— масштабирования латентов, чтобы шум совпадал по дисперсии с данными,<br>— трансформации выходной head-функции, чтобы предсказания соответствовали формуле consistency-динамики.<br><br>Но перенести диффузионку в новую параметризацию — это только половина дела. Вторая часть — заставить всё это стабильно учиться. И вот здесь начинаются инженерные приключения. Стабильность «улетает в космос» из-за того, что временной эмбеддинг использует слишком большой масштаб шума — из-за этого производные становятся огромными. Лечится это просто: нужно изменить масштаб частот эмбеддинга и немного дообучить модель, буквально несколько тысяч итераций.<br><br>Вторая проблема — большие нормы градиентов в механизме внимания. Решение довольно стандартное: добавить RMSNorm на Q/K (QK-Normalization) в self- и cross-attention, после чего обучение стабилизируется.<br><br>Теперь самое главное — скорость. В разрешении 1024×1024 SANA-Sprint выдаёт картинку за ~0,1–0,18 секунды при одношаговой генерации. Из них на сам трансформер уходит ≈0,03 секунды, остальное — VAE-декодер, который становится основным бутылочным горлышком. По времени работы диффузионной модели SANA-Sprint быстрее FLUX-schnell примерно в 65 раз, а по end-to-end-задержке — примерно в 10 раз. То есть «быстро» тут — не просто эпитет.<br><br>Итоговое качество вполне пристойное: на 1–4 шагах она даёт FID и GenEval на уровне или лучше, чем у других быстрых моделей. Например, не уступает FLUX-schnell по метрикам (7,59 против 7,94 по FID и 0,74 против 0,71 по GenEval), будучи заметно быстрее.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Денис Кузнеделев<br></em><a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/225_480.webp" srcset="../assets/media/thumbs/225_480.webp 480w, ../assets/media/225.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="225" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 127 просмотров · 21 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/225" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/225.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="221" data-search="байки из склепа прода alice ai vlm сегодня делимся двумя скримерами историями из первых рук о том, с какими сложностями столкнулись разработчики новой алисы ai в продакшне. популярный сценарий использования нейросети — когда пользователь отправляет в чат картинку и просит помочь с тем, что на ней изображено. за этот навык отвечают alice ai vlm и команда компьютерного зрения яндекса, которая её развивает. слово руководителю подгруппы распознавания текста в vlm антону клочкову @blog_toxa. проблема первая: пережатие картинок те, кто имел дело с сервисами, где есть работа с картинками, не дадут соврать: найти баланс между качеством и скоростью загрузки изображений — сложная задача. иногда баланс перевешивает в одну из сторон, и в нашем случае была проблема качества. как-то во время тестирования алисы ai прилетает баг-репорт: фотография из учебника и комментарий: «формулы выписываются неверно!» (см. картинку 1). проверяем в тестинге — есть ошибка. прогоняем офлайн через модель — ошибки нет. странно? очень! оказалось, что в продакшене сильно пережимаются изображения (см картинку 2). из-за этого путаются мелкие обозначения, вроде знаков неравенства, и иногда теряется весь смысл. фикс был простой: мы ослабили правила на пережатие картинок. проблема вторая: парсинг latex наши первые шаги к тому, чтобы сделать алису ai действительно умной, проходили в поиске по картинкам — там уже была готовая инфраструктура, а в чате ещё требовалась донастройка. однажды пришла пора тестировать решение в сервисе. и в целом, всё было хорошо, кроме одной детали. оказалось, что на разных поверхностях (в нашем случае — поиска и алисы ai) по-разному работают правила парсинга latex-вставок в markdown. например, в поиске по картинкам формулы отображались одним образом (см. картинку 3), а в алиса ai — другим (см. картинку 4). и это было не единственное различие в парсинге. решили мы это в одних случаях дообучением vlm на форматы, в других — правками во фронтенде. алиса ai — это не только alice ai vlm, о которой мы пишем в этом посте, но и alice ai llm, alice ai llm search, alice ai art, а ещё много крутых инженерных решений. если хотите больше технических деталей, советуем почитать свежий техрепорт. а ознакомиться с главными фичами можно на лендинге. cv time байки из склепа прода alice ai vlm сегодня делимся двумя скримерами историями из первых рук о том, с какими сложностями столкнулись разработчики новой алисы ai в продакшне. популярный сценарий использования нейросети — когда пользователь отправляет в чат картинку и просит помочь с тем, что на ней изображено. за этот навык отвечают alice ai vlm и команда компьютерного зрения яндекса, которая её развивает. слово руководителю подгруппы распознавания текста в vlm антону клочкову @blog_toxa. проблема первая: пережатие картинок те, кто имел дело с сервисами, где есть работа с картинками, не дадут соврать: найти баланс между качеством и скоростью загрузки изображений — сложная задача. иногда баланс перевешивает в одну из сторон, и в нашем случае была проблема качества. как-то во время тестирования алисы ai прилетает баг-репорт: фотография из учебника и комментарий: «формулы выписываются неверно!» (см. картинку 1) . проверяем в тестинге — есть ошибка. прогоняем офлайн через модель — ошибки нет. странно? очень! оказалось, что в продакшене сильно пережимаются изображения (см картинку 2) . из-за этого путаются мелкие обозначения, вроде знаков неравенства, и иногда теряется весь смысл. фикс был простой: мы ослабили правила на пережатие картинок. проблема вторая: парсинг latex наши первые шаги к тому, чтобы сделать алису ai действительно умной, проходили в поиске по картинкам — там уже была готовая инфраструктура, а в чате ещё требовалась донастройка. однажды пришла пора тестировать решение в сервисе. и в целом, всё было хорошо, кроме одной детали. оказалось, что на разных поверхностях (в нашем случае — поиска и алисы ai) по-разному работают правила парсинга latex-вставок в markdown. например, в поиске по картинкам формулы отображались одним образом (см. картинку 3) , а в алиса ai — другим (см. картинку 4) . и это было не единственное различие в парсинге. решили мы это в одних случаях дообучением vlm на форматы, в других — правками во фронтенде. алиса ai — это не только alice ai vlm, о которой мы пишем в этом посте, но и alice ai llm, alice ai llm search, alice ai art, а ещё много крутых инженерных решений. если хотите больше технических деталей, советуем почитать свежий техрепорт . а ознакомиться с главными фичами можно на лендинге . cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-12-11T10:16:40+00:00" href="./posts/221.html">2025-12-11 10:16 UTC</a></div>
      </div>
      <div class="post-body"><strong>Байки из <del>склепа</del> прода Alice AI VLM<br></strong><br>Сегодня делимся двумя <del>скримерами</del> историями из первых рук о том, с какими сложностями столкнулись разработчики новой <a href="https://alice.yandex.ru/" rel="nofollow noopener noreferrer">Алисы AI</a> в продакшне.  <br><br>Популярный сценарий использования нейросети — когда пользователь отправляет в чат картинку и просит помочь с тем, что на ней изображено. За этот навык отвечают Alice AI VLM и команда компьютерного зрения Яндекса, которая её развивает. Слово руководителю подгруппы распознавания текста в VLM Антону Клочкову @blog_toxa.<br><br><blockquote><strong>Проблема первая: пережатие картинок</strong><br><strong><br></strong>Те, кто имел дело с сервисами, где есть работа с картинками, не дадут соврать: найти баланс между качеством и скоростью загрузки изображений — сложная задача. Иногда баланс перевешивает в одну из сторон, и в нашем случае была проблема качества.<br><br>Как-то во время тестирования Алисы AI прилетает баг-репорт: фотография из учебника и комментарий: «Формулы выписываются неверно!» <em>(см. картинку 1)</em>. <br><br>Проверяем в тестинге — есть ошибка. Прогоняем офлайн через модель — ошибки нет. Странно? Очень!<br><br>Оказалось, что в продакшене сильно пережимаются изображения <em>(см картинку 2)</em>. Из-за этого путаются мелкие обозначения, вроде знаков неравенства, и иногда теряется весь смысл. Фикс был простой: мы ослабили правила на пережатие картинок.<br><br><strong>Проблема вторая: парсинг LaTeX</strong><br><br>Наши первые шаги к тому, чтобы сделать Алису AI действительно умной, проходили в Поиске по картинкам — там уже была готовая инфраструктура, а в чате ещё требовалась донастройка.<br><br>Однажды пришла пора тестировать решение в сервисе. И в целом, всё было хорошо, кроме одной детали. Оказалось, что на разных поверхностях (в нашем случае — Поиска и Алисы AI) по-разному работают правила парсинга LaTeX-вставок в Markdown. Например, в Поиске по картинкам формулы отображались одним образом <em>(см. картинку 3)</em>, а в Алиса AI — другим <em>(см. картинку 4)</em>. И это было не единственное различие в парсинге. <br><br>Решили мы это в одних случаях дообучением VLM на форматы, в других — правками во фронтенде.</blockquote><br><br>Алиса AI — это не только Alice AI VLM, о которой мы пишем в этом посте, но и Alice AI LLM, Alice AI LLM Search, Alice AI ART, а ещё много крутых инженерных решений. Если хотите больше технических деталей, советуем почитать свежий <a href="https://habr.com/ru/companies/yandex/articles/974594/" rel="nofollow noopener noreferrer">техрепорт</a>. А ознакомиться с главными фичами можно на <a href="https://alice.yandex.ru/about" rel="nofollow noopener noreferrer">лендинге</a>. <br><br><a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/221_480.webp" srcset="../assets/media/thumbs/221_480.webp 480w, ../assets/media/221.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="221" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/222_480.webp" srcset="../assets/media/thumbs/222_480.webp 480w, ../assets/media/222.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="221" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/223_480.webp" srcset="../assets/media/thumbs/223_480.webp 480w, ../assets/media/223.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="221" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/224_480.webp" srcset="../assets/media/thumbs/224_480.webp 480w, ../assets/media/224.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="221" data-image-index="3" /></div></div>
      <div class="actions">
        <span>4 035 просмотров · 37 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/221" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/221.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="220" data-search="internvl3.5: advancing open-source multimodal models in versatility, reasoning and efficiency авторы опенсорс-семейства internvl постоянно выпускают всё новые и новые улучшения своих мультимодальных моделей, которые опережают sota-результаты в первую очередь по бенчмаркам. сегодня разберём статью о свежей версии internvl3.5. в основе улучшений — три основных нововведения. cascade reinforcement learning раньше модели internvl использовали mpo в качестве offline rl. в новой версии 3.5 авторы добавили ещё и online rl: принято считать, что на llm/vlm он гораздо лучше, чем offline. но offline rl значительно легче по вычислениям (в основном из-за того, что во время обучения не нужно генерировать ответы на инструкции). авторы показали, что offline rl не так уж сильно отстаёт от online rl, но при этом обучается в 20 раз быстрее. а лучшее качество модели достигается при совместном каскадном обучении: результаты лучше, чем у online rl, даже на двух эпохах. так offline rl превратился в warmup для online rl. в качестве online rl используется gspo — модификация grpo, которая решает проблему нестабильности обучения и «коллапса модели», особенно при тренировке mixture-of-experts-моделей. grpo работает на уровне отдельных токенов, создавая шумные градиенты, а gspo применяет оптимизацию на уровне всей последовательности целиком, что важно для длинных цепочек рассуждений. visual resolution router (vir) основная цель этого нововведения — снизить вычислительную нагрузку на модель во время инференса. этого удалось добиться за счёт уменьшения количества визуальных токенов в представлении каждого кропа картинки. сколько токенов нужно выделить на кроп, решает роутер. среднее количество визуальных токенов, поступающих в llm, при таком подходе сокращается на 50%. стандартный процесс кодирования картинки выглядит так: — изображение делится на кропы, — каждый патч преобразуется в 1024 токена для vit, — после обработки vit количество токенов уменьшается адаптером до 256 и передаются в llm. роутер может направить токены в более агрессивный адаптер и сжать до 64 токенов. обучение происходит в два этапа. на первом этапе модель тренируется решать задачу с меньшим количеством токенов за счёт минимизации kl-дивергенции между распределениями выходных данных изначального сжатия и более агрессивного сжатия. цель второго этапа — научить сам роутер vir принимать правильные решения о степени сжатия для каждого кропа. vir обучается как стандартный бинарный классификатор, где label кропа определяется по значению loss из первого этапа. итог — flash-модель практически без потери качества с ускорением до 4 раз (точная цифра зависит от разрешения картинки и размера модели). dvd (decoupled vision-language deployment) в этой системе модель для обработки изображений (vit) и языковая модель (llm) разворачиваются на отдельных серверах или gpu. они работают не последовательно (сначала картинка, потом текст), а параллельно. пока языковая модель генерирует ответ на предыдущий запрос, визуальный энкодер уже обрабатывает следующее изображение. это даёт ускорение до 2 раз для базовых моделей, а в комбинации с vir — до 4 раз на высоких разрешениях. по словам авторов, новая internvl3.5 рассуждает на +16,0% эффективнее и в 4,05 раз быстрее, чем её предшественники. разбор подготовил ❣ антон астахов cv time internvl3.5: advancing open-source multimodal models in versatility, reasoning and efficiency авторы опенсорс-семейства internvl постоянно выпускают всё новые и новые улучшения своих мультимодальных моделей, которые опережают sota-результаты в первую очередь по бенчмаркам. сегодня разберём статью о свежей версии internvl3.5. в основе улучшений — три основных нововведения. cascade reinforcement learning раньше модели internvl использовали mpo в качестве offline rl. в новой версии 3.5 авторы добавили ещё и online rl: принято считать, что на llm/vlm он гораздо лучше, чем offline. но offline rl значительно легче по вычислениям (в основном из-за того, что во время обучения не нужно генерировать ответы на инструкции). авторы показали, что offline rl не так уж сильно отстаёт от online rl, но при этом обучается в 20 раз быстрее. а лучшее качество модели достигается при совместном каскадном обучении: результаты лучше, чем у online rl, даже на двух эпохах. так offline rl превратился в warmup для online rl. в качестве online rl используется gspo — модификация grpo, которая решает проблему нестабильности обучения и «коллапса модели», особенно при тренировке mixture-of-experts-моделей. grpo работает на уровне отдельных токенов, создавая шумные градиенты, а gspo применяет оптимизацию на уровне всей последовательности целиком, что важно для длинных цепочек рассуждений. visual resolution router (vir) основная цель этого нововведения — снизить вычислительную нагрузку на модель во время инференса. этого удалось добиться за счёт уменьшения количества визуальных токенов в представлении каждого кропа картинки. сколько токенов нужно выделить на кроп, решает роутер. среднее количество визуальных токенов, поступающих в llm, при таком подходе сокращается на 50%. стандартный процесс кодирования картинки выглядит так: — изображение делится на кропы, — каждый патч преобразуется в 1024 токена для vit, — после обработки vit количество токенов уменьшается адаптером до 256 и передаются в llm. роутер может направить токены в более агрессивный адаптер и сжать до 64 токенов. обучение происходит в два этапа. на первом этапе модель тренируется решать задачу с меньшим количеством токенов за счёт минимизации kl-дивергенции между распределениями выходных данных изначального сжатия и более агрессивного сжатия. цель второго этапа — научить сам роутер vir принимать правильные решения о степени сжатия для каждого кропа. vir обучается как стандартный бинарный классификатор, где label кропа определяется по значению loss из первого этапа. итог — flash-модель практически без потери качества с ускорением до 4 раз (точная цифра зависит от разрешения картинки и размера модели). dvd (decoupled vision-language deployment) в этой системе модель для обработки изображений (vit) и языковая модель (llm) разворачиваются на отдельных серверах или gpu. они работают не последовательно (сначала картинка, потом текст), а параллельно. пока языковая модель генерирует ответ на предыдущий запрос, визуальный энкодер уже обрабатывает следующее изображение. это даёт ускорение до 2 раз для базовых моделей, а в комбинации с vir — до 4 раз на высоких разрешениях. по словам авторов, новая internvl3.5 рассуждает на +16,0% эффективнее и в 4,05 раз быстрее, чем её предшественники. разбор подготовил ❣ антон астахов cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-12-09T09:53:01+00:00" href="./posts/220.html">2025-12-09 09:53 UTC</a></div>
      </div>
      <div class="post-body"><strong>InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning and Efficiency<br></strong><br>Авторы опенсорс-семейства InternVL постоянно выпускают всё новые и новые улучшения своих мультимодальных моделей, которые опережают SoTA-результаты в первую очередь по бенчмаркам. Сегодня разберём <a href="https://arxiv.org/abs/2508.18265" rel="nofollow noopener noreferrer">статью</a> о свежей версии InternVL3.5.<br><br>В основе улучшений — три основных нововведения.<br><br><strong>Cascade Reinforcement Learning<br></strong><br>Раньше модели InternVL использовали MPO в качестве offline RL. В новой версии 3.5 авторы добавили ещё и online RL: принято считать, что на LLM/VLM он гораздо лучше, чем offline. Но offline RL значительно легче по вычислениям (в основном из-за того, что во время обучения не нужно генерировать ответы на инструкции).<br><br>Авторы показали, что offline RL не так уж сильно отстаёт от online RL, но при этом обучается в 20 раз быстрее. А лучшее качество модели достигается при совместном каскадном обучении: результаты лучше, чем у online RL, даже на двух эпохах. Так offline RL превратился в warmup для online RL.<br><br>В качестве online RL используется GSPO — модификация GRPO, которая решает проблему нестабильности обучения и «коллапса модели», особенно при тренировке Mixture-of-Experts-моделей. GRPO работает на уровне отдельных токенов, создавая шумные градиенты, а GSPO применяет оптимизацию на уровне всей последовательности целиком, что важно для длинных цепочек рассуждений.<br><br><strong>Visual Resolution Router (ViR)<br></strong><br>Основная цель этого нововведения — снизить вычислительную нагрузку на модель во время инференса. Этого удалось добиться за счёт уменьшения количества визуальных токенов в представлении каждого кропа картинки. Сколько токенов нужно выделить на кроп, решает роутер. Среднее количество визуальных токенов, поступающих в LLM, при таком подходе сокращается на 50%. <br><br>Стандартный процесс кодирования картинки выглядит так: <br><br>— изображение делится на кропы, <br>— каждый патч преобразуется в 1024 токена для ViT, <br>— после обработки ViT количество токенов уменьшается адаптером до 256 и передаются в LLM. <br><br>Роутер может направить токены в более агрессивный адаптер и сжать до 64 токенов. Обучение происходит в два этапа. На первом этапе модель тренируется решать задачу с меньшим количеством токенов за счёт минимизации KL-дивергенции между распределениями выходных данных изначального сжатия и более агрессивного сжатия.<br><br>Цель второго этапа — научить сам роутер ViR принимать правильные решения о степени сжатия для каждого кропа. ViR обучается как стандартный бинарный классификатор, где label кропа определяется по значению loss из первого этапа.<br><br>Итог — flash-модель практически без потери качества с ускорением до 4 раз (точная цифра зависит от разрешения картинки и размера модели).<br><br><strong>DvD (Decoupled Vision-Language Deployment)</strong><br><br>В этой системе модель для обработки изображений (ViT) и языковая модель (LLM) разворачиваются на отдельных серверах или GPU.<br><br>Они работают не последовательно (сначала картинка, потом текст), а параллельно. Пока языковая модель генерирует ответ на предыдущий запрос, визуальный энкодер уже обрабатывает следующее изображение. Это даёт ускорение до 2 раз для базовых моделей, а в комбинации с ViR — до 4 раз на высоких разрешениях. <br><br>По словам авторов, новая InternVL3.5 рассуждает на +16,0% эффективнее и в 4,05 раз быстрее, чем её предшественники. <br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Антон Астахов<br></em><a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer">CV Time</a></div>
      <div class="actions">
        <span>1 495 просмотров · 27 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/220" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/220.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="213" data-search="neurips в мехико: продолжаем делиться интересным червёртый день конференции в мексике получился насыщенным. было выступление ричарда саттона о его видении superintelligence, две сессии со статьями и две — с постерами. самая интересная статья дня, по мнению владислава фахретдинова, — perception encoder: the best visual embeddings are not at the output of the network от meta*. мы уже разбирали работу в канале, а теперь делимся тем, что о ней говорят сами авторы. исследователи рассказывают, что поставили перед собой цель создать лучший визуальный энкодер для многих downstream-задач. для этого двухстадийно обучались контрастив-лоссом на парах «изображение-текст» и потом — на парах «видео–текст», используя свою модель как кадровый энкодер. начав с clip-бейзлайна, добавили ряд улучшений и сравнили их по качеству и устойчивости. уже на этом этапе модель достигла sota в zero-shot retrieval и классификации; назвали её pe_core. затем авторы протестировали модель как энкодер на разных downstream-задачах: детекции, трекинге, предсказании глубин. увидели, что перфоманс оказался ниже ожидаемого. в ходе исследования с помощью аттеншен-карт заметили появление глобальных токенов на определённом слое. чтобы проверить гипотезу, стали брать эмбеддинги не с последнего слоя, а с предыдущих. построив график качества по слоям для разных downstream-задач и моделей, увидели, что качество растёт к эмбеддингам средних слоёв, а к последним слоям — резко падает. для решения этой проблемы использовали два метода после обучения: 1. чтобы сохранить глобальную информацию, провели файнтьюн на 41-м слое (который показывает близкие к лучшим значениям по всем задачам) с минимизацией косинусного расстояния между ним и последним слоем. 2. чтобы сохранить локальную информацию, добавили файнтьюн на mse попарного косинусного расстояния между эмбеддингами последнего слоя (h×w×1024 -&gt; hw×hw) и попарного косинусного расстояния между логитами sam для 1024 точек из равномерной сетки исходного изображения. эту модель авторы назвали pe_spatial и показали, что она достигает sota по многим downstream-задачам. хотя вышедший позже dinov3 достиг более высоких результатов, подход остаётся интересным. #yaneurips25 cv time ___ meta признана экстремистской организацией, а facebook и instagram запрещены на территории рф neurips в мехико: продолжаем делиться интересным червёртый день конференции в мексике получился насыщенным. было выступление ричарда саттона о его видении superintelligence, две сессии со статьями и две — с постерами. самая интересная статья дня, по мнению владислава фахретдинова, — perception encoder: the best visual embeddings are not at the output of the network от meta*. мы уже разбирали работу в канале, а теперь делимся тем, что о ней говорят сами авторы. исследователи рассказывают, что поставили перед собой цель создать лучший визуальный энкодер для многих downstream-задач. для этого двухстадийно обучались контрастив-лоссом на парах «изображение-текст» и потом — на парах «видео–текст», используя свою модель как кадровый энкодер. начав с clip-бейзлайна, добавили ряд улучшений и сравнили их по качеству и устойчивости. уже на этом этапе модель достигла sota в zero-shot retrieval и классификации; назвали её pe_core. затем авторы протестировали модель как энкодер на разных downstream-задачах: детекции, трекинге, предсказании глубин. увидели, что перфоманс оказался ниже ожидаемого. в ходе исследования с помощью аттеншен-карт заметили появление глобальных токенов на определённом слое. чтобы проверить гипотезу, стали брать эмбеддинги не с последнего слоя, а с предыдущих. построив график качества по слоям для разных downstream-задач и моделей, увидели, что качество растёт к эмбеддингам средних слоёв, а к последним слоям — резко падает. для решения этой проблемы использовали два метода после обучения: 1. чтобы сохранить глобальную информацию, провели файнтьюн на 41-м слое (который показывает близкие к лучшим значениям по всем задачам) с минимизацией косинусного расстояния между ним и последним слоем. 2. чтобы сохранить локальную информацию, добавили файнтьюн на mse попарного косинусного расстояния между эмбеддингами последнего слоя (h×w×1024 -&amp;gt; hw×hw) и попарного косинусного расстояния между логитами sam для 1024 точек из равномерной сетки исходного изображения. эту модель авторы назвали pe_spatial и показали, что она достигает sota по многим downstream-задачам. хотя вышедший позже dinov3 достиг более высоких результатов, подход остаётся интересным. #yaneurips25 cv time ___ meta признана экстремистской организацией, а facebook и instagram запрещены на территории рф">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-12-05T13:31:42+00:00" href="./posts/213.html">2025-12-05 13:31 UTC</a></div>
      </div>
      <div class="post-body"><strong>NeurIPS в Мехико: продолжаем делиться интересным<br><br></strong>Червёртый день конференции в Мексике получился насыщенным. Было выступление Ричарда Саттона о его видении SuperIntelligence, две сессии со статьями и две — с постерами.<br><br>Самая интересная статья дня, по мнению Владислава Фахретдинова, — <a href="https://arxiv.org/abs/2504.13181" rel="nofollow noopener noreferrer">Perception Encoder: The best visual embeddings are not at the output of the network</a> от Meta*. Мы уже <a href="https://t.me/c/2494702394/249" rel="nofollow noopener noreferrer">разбирали</a> работу в канале, а теперь делимся тем, что о ней говорят сами авторы.<br><br><blockquote>Исследователи рассказывают, что поставили перед собой цель создать лучший визуальный энкодер для многих downstream-задач. Для этого двухстадийно обучались контрастив-лоссом на парах «изображение-текст» и потом — на парах «видео–текст», используя свою модель как кадровый энкодер.<br><br>Начав с CLIP-бейзлайна, добавили ряд улучшений и сравнили их по качеству и устойчивости. Уже на этом этапе модель достигла SOTA в zero-shot retrieval и классификации; назвали её PE_core.<br><br>Затем авторы протестировали модель как энкодер на разных downstream-задачах: детекции, трекинге, предсказании глубин. Увидели, что перфоманс оказался ниже ожидаемого.<br><br>В ходе исследования с помощью аттеншен-карт заметили появление глобальных токенов на определённом слое. Чтобы проверить гипотезу, стали брать эмбеддинги не с последнего слоя, а с предыдущих. Построив график качества по слоям для разных downstream-задач и моделей, увидели, что качество растёт к эмбеддингам средних слоёв, а к последним слоям — резко падает.<br><br>Для решения этой проблемы использовали два метода после обучения:<br><br>1. Чтобы сохранить глобальную информацию, провели файнтьюн на 41-м слое (который показывает близкие к лучшим значениям по всем задачам) с минимизацией косинусного расстояния между ним и последним слоем.<br><br>2. Чтобы сохранить локальную информацию, добавили файнтьюн на MSE попарного косинусного расстояния между эмбеддингами последнего слоя (H×W×1024 -&gt; HW×HW) и попарного косинусного расстояния между логитами SAM для 1024 точек из равномерной сетки исходного изображения.<br><br>Эту модель авторы назвали PE_spatial и показали, что она достигает SOTA по многим downstream-задачам. Хотя вышедший позже DinoV3 достиг более высоких результатов, подход остаётся интересным.</blockquote><br><br>#YaNeurIPS25<br><br><a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer">CV Time</a><br>___<br><em>Meta признана экстремистской организацией, а Facebook и Instagram запрещены на территории РФ</em><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/213_480.webp" srcset="../assets/media/thumbs/213_480.webp 480w, ../assets/media/213.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="213" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/214_480.webp" srcset="../assets/media/thumbs/214_480.webp 480w, ../assets/media/214.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="213" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/215_480.webp" srcset="../assets/media/thumbs/215_480.webp 480w, ../assets/media/215.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="213" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/216_480.webp" srcset="../assets/media/thumbs/216_480.webp 480w, ../assets/media/216.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="213" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/217_480.webp" srcset="../assets/media/thumbs/217_480.webp 480w, ../assets/media/217.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="213" data-image-index="4" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/218_480.webp" srcset="../assets/media/thumbs/218_480.webp 480w, ../assets/media/218.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="213" data-image-index="5" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/219_480.webp" srcset="../assets/media/thumbs/219_480.webp 480w, ../assets/media/219.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="213" data-image-index="6" /></div></div>
      <div class="actions">
        <span>1 312 просмотров · 27 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/213" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/213.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="203" data-search="neurips в мехико: туториал о геопространственных foundation-моделях в третий день конференции прошло большое количество туториалов. один из них — geospatial foundation models: overview, application and benchmarking — посетил владислав фахретдинов из команды восприятия робота доставки. делимся его заметками! выступали докладчики из бразильского подразделения ibm research. начали с рассказа о задаче remote sensing — дистанционного зондирования по спутниковым данным. основное отличие от классических задач компьютерного зрения в том, что кроме rgb-сигналов необходимо использовать и другие спектральные каналы, у каждого из которых есть своё физическое назначение. на основе этих данных можно решать множество задач, таких как сегментация земного покрова, пожарных шрамов и наводнений, предсказание глубины для водного покрова и процента покрытия деревьями. затем был базовый экскурс в развитие компьютерного зрения: от свёрточных моделей и трансформеров до автоэнкодеров, а после — рассказ о foundation-моделях в этой сфере. докладчики представили множество работ, в которых главный архитектурный вопрос состоит в том, как правильно объединять данные из разных каналов (модальностей). отчасти это связано с тем, что нельзя просто склеить все каналы из-за отличий в разрешении, поэтому используются разные подходы: — отдельные энкодер и декодер для каждой модальности, но общий аттеншн; — динамический подбор размеров патчей для каждой модальности на основе длины волны и общий энкодер; — либо разные энкодеры, но совместный семплинг патчей со всех модальностей на этапе претрейна. после этого исследователи рассказали о своём фреймворке для обучения геопространственных моделей terratorch. на практике — собрали ноутбук с обучением двум разным задачам: land segmentation и burn scars. также авторы представили свой новый бенчмарк geobenchv2, который сгруппировали из 19 существующих датасетов. взяли множество популярных в cv моделей для сравнения и дофайнтюнили их на разные задачи только на основе rgb. в итоге оказалось, что общие модели, такие как dinov3, дают гораздо лучшие предсказания на основе rgb-изображений, но на задачах с мультиспектральными данными более маленькие, но узкоспециализированные модели всё ещё побеждают. #yaneurips25 cv time neurips в мехико: туториал о геопространственных foundation-моделях в третий день конференции прошло большое количество туториалов. один из них — geospatial foundation models: overview, application and benchmarking — посетил владислав фахретдинов из команды восприятия робота доставки. делимся его заметками! выступали докладчики из бразильского подразделения ibm research. начали с рассказа о задаче remote sensing — дистанционного зондирования по спутниковым данным. основное отличие от классических задач компьютерного зрения в том, что кроме rgb-сигналов необходимо использовать и другие спектральные каналы, у каждого из которых есть своё физическое назначение. на основе этих данных можно решать множество задач, таких как сегментация земного покрова, пожарных шрамов и наводнений, предсказание глубины для водного покрова и процента покрытия деревьями. затем был базовый экскурс в развитие компьютерного зрения: от свёрточных моделей и трансформеров до автоэнкодеров, а после — рассказ о foundation-моделях в этой сфере. докладчики представили множество работ, в которых главный архитектурный вопрос состоит в том, как правильно объединять данные из разных каналов (модальностей). отчасти это связано с тем, что нельзя просто склеить все каналы из-за отличий в разрешении, поэтому используются разные подходы: — отдельные энкодер и декодер для каждой модальности, но общий аттеншн; — динамический подбор размеров патчей для каждой модальности на основе длины волны и общий энкодер; — либо разные энкодеры, но совместный семплинг патчей со всех модальностей на этапе претрейна. после этого исследователи рассказали о своём фреймворке для обучения геопространственных моделей terratorch. на практике — собрали ноутбук с обучением двум разным задачам: land segmentation и burn scars. также авторы представили свой новый бенчмарк geobenchv2, который сгруппировали из 19 существующих датасетов. взяли множество популярных в cv моделей для сравнения и дофайнтюнили их на разные задачи только на основе rgb. в итоге оказалось, что общие модели, такие как dinov3, дают гораздо лучшие предсказания на основе rgb-изображений, но на задачах с мультиспектральными данными более маленькие, но узкоспециализированные модели всё ещё побеждают. #yaneurips25 cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-12-03T13:09:43+00:00" href="./posts/203.html">2025-12-03 13:09 UTC</a></div>
      </div>
      <div class="post-body"><strong>NeurIPS в Мехико: туториал о геопространственных foundation-моделях<br></strong><br>В третий день конференции прошло большое количество туториалов. Один из них — <a href="https://neurips.cc/virtual/2025/loc/mexico-city/128793" rel="nofollow noopener noreferrer">Geospatial Foundation Models: Overview, Application and Benchmarking</a> — посетил Владислав Фахретдинов из команды восприятия робота доставки. Делимся его заметками!<br><br><blockquote>Выступали докладчики из бразильского подразделения IBM Research. Начали с рассказа о задаче remote sensing — дистанционного зондирования по спутниковым данным. Основное отличие от классических задач компьютерного зрения в том, что кроме RGB-сигналов необходимо использовать и другие спектральные каналы, у каждого из которых есть своё физическое назначение.<br><br>На основе этих данных можно решать множество задач, таких как сегментация земного покрова, пожарных шрамов и наводнений, предсказание глубины для водного покрова и процента покрытия деревьями.<br><br>Затем был базовый экскурс в развитие компьютерного зрения: от свёрточных моделей и трансформеров до автоэнкодеров, а после — рассказ о foundation-моделях в этой сфере. <br><br>Докладчики представили множество работ, в которых главный архитектурный вопрос состоит в том, как правильно объединять данные из разных каналов (модальностей). Отчасти это связано с тем, что нельзя просто склеить все каналы из-за отличий в разрешении, поэтому используются разные подходы:<br><br>— отдельные энкодер и декодер для каждой модальности, но общий аттеншн; <br>— динамический подбор размеров патчей для каждой модальности на основе длины волны и общий энкодер;<br>— либо разные энкодеры, но совместный семплинг патчей со всех модальностей на этапе претрейна.<br><br>После этого исследователи рассказали о своём фреймворке для обучения геопространственных моделей TerraTorch. На практике — собрали ноутбук с обучением двум разным задачам: land segmentation и burn scars.<br><br>Также авторы представили свой новый бенчмарк GeoBenchV2, который сгруппировали из 19 существующих датасетов. Взяли множество популярных в CV моделей для сравнения и дофайнтюнили их на разные задачи только на основе RGB.<br><br>В итоге оказалось, что общие модели, такие как DinoV3, дают гораздо лучшие предсказания на основе RGB-изображений, но на задачах с мультиспектральными данными более маленькие, но узкоспециализированные модели всё ещё побеждают.</blockquote><br><br>#YaNeurIPS25<br><br><a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/203_480.webp" srcset="../assets/media/thumbs/203_480.webp 480w, ../assets/media/203.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="203" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/204_480.webp" srcset="../assets/media/thumbs/204_480.webp 480w, ../assets/media/204.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="203" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/205_480.webp" srcset="../assets/media/thumbs/205_480.webp 480w, ../assets/media/205.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="203" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/206_480.webp" srcset="../assets/media/thumbs/206_480.webp 480w, ../assets/media/206.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="203" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/207_480.webp" srcset="../assets/media/thumbs/207_480.webp 480w, ../assets/media/207.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="203" data-image-index="4" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/208_480.webp" srcset="../assets/media/thumbs/208_480.webp 480w, ../assets/media/208.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="203" data-image-index="5" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/209_480.webp" srcset="../assets/media/thumbs/209_480.webp 480w, ../assets/media/209.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="203" data-image-index="6" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/210_480.webp" srcset="../assets/media/thumbs/210_480.webp 480w, ../assets/media/210.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="203" data-image-index="7" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/211_480.webp" srcset="../assets/media/thumbs/211_480.webp 480w, ../assets/media/211.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="203" data-image-index="8" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/212_480.webp" srcset="../assets/media/thumbs/212_480.webp 480w, ../assets/media/212.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="203" data-image-index="9" /></div></div>
      <div class="actions">
        <span>1 252 просмотров · 27 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/203" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/203.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="195" data-search="neurips 2025 в мехико идёт полным ходом конференция продолжается, а наш коллега владислав фахретдинов делится заметками о воркшопе второго дня — 7th international workshop on large scale holistic video understanding: toward video foundation models. было немного спикеров, но почти каждый привёз по две-три статьи или исследования, поэтому день получился насыщенным. основной мотив воркшопа — большинство моделей для работы с видео недостаточно хорошо ориентируются «во времени». участники разбирались, что с этим можно сделать. первым выступил профессор университета амстердама. он заметил, что многие videollm не справляются даже с простым синтетическим бенчмарком: какой из двух объектов в видео появляется раньше. это показывает, что мы до конца не понимаем, как правильно оценивать такие способности модели. затем последовал рассказ о работе bench of time с более подробными исследованиями — оказалось, что большинство примеров в популярном бенчмарке (mvbench) решается либо знанием всего об одном кадре, либо вообще исключительно по тексту. чтобы исправить эту ситуацию, авторы сделали свой бенчмарк tvbench. в нём все вопросы были сформулированы так, что без понимания объектов и процессов в кадре нельзя дать правильный ответ. сравнение моделей на новом бенчмарке показало, что большинство языковых, картиночных и даже видеомоделей выдают результаты немногим лучше случайного предсказания. при этом все же нашлись несколько моделей, которые были достаточно хороши на обоих бенчмарках, например gemini-1.5. следом было выступление о генерации 3d-представления из изображения. по сути, это продолжение работы dust3r, в которой научились по любым входным изображениям без параметров камер и поз делать матчинг и генерировать плотное облако точек 3d-представления сцены. авторы сделали уточнение, что матчинг изображений по случайному видео с движением — вычислительно сложная задача. поэтому они собрали датасет 360-1m, где происходит движение и вращение вокруг оси, из-за чего матчить изображения стало гораздо проще. на основе своего датасета они обучили генеративную модель odin, которая по изображению и смещению позиции камеры генерирует новое изображение. подробностей было мало, никаких сравнений с dust3r или nerf не показали, но зато рассказали, что модель хорошо обобщается вне домена — например, на картины. самый интересный доклад за день — о том, что визуальные модели знают о нашем мире. авторы выделили и проверили три свойства: базовое представление о физическом устройстве мира, визуальное предсказание, а также обобщение — понимание аналогий. для первого свойства взяли часовые видео с прогулками по городам и с помощью сервиса визуальной локализации, а также небольшого объёма человеческой проверки, разметили эти видео. в частности, для каждого видео сгенерировали маршрут на карте. далее видео нарезали и собрали бенчмарк, в котором модели задавали вопросы по содержанию ролика, например: о евклидовом расстоянии от начальной до конечной точки на полученном маршруте; направлении; зацикленность маршрута; выборе правильного трека на карте среди нескольких вариантов (с текстом на карте и без текста); распознавании окружающей архитектуры. по всем этим вопросам модели уступают человеку — за исключением проверки на зацикленность маршрута. авторы также показали, что на самом деле модели не понимали, был цикл в маршруте или нет. вместо этого они просто смотрели на разметку на карте и сопоставляли её с текстовыми названиями улиц, которые видны в видео. напоследок был доклад из трёх частей, из которых я бы выделил как самую интересную — ssl-обучение мультимодальной модели видео+аудио cav-mae sync. из того, что мне кажется важным: авторы совместно используют аудио- и видеопатчи и добавляют регистровый токен, чтобы переносить накопленную информацию в следующие слои. больше всего мне понравилось, что новая модель позволяет локализовать на видео источники звука. #yaneurips25 cv time neurips 2025 в мехико идёт полным ходом конференция продолжается, а наш коллега владислав фахретдинов делится заметками о воркшопе второго дня — 7th international workshop on large scale holistic video understanding: toward video foundation models . было немного спикеров, но почти каждый привёз по две-три статьи или исследования, поэтому день получился насыщенным. основной мотив воркшопа — большинство моделей для работы с видео недостаточно хорошо ориентируются «во времени». участники разбирались, что с этим можно сделать. первым выступил профессор университета амстердама. он заметил, что многие videollm не справляются даже с простым синтетическим бенчмарком: какой из двух объектов в видео появляется раньше. это показывает, что мы до конца не понимаем, как правильно оценивать такие способности модели. затем последовал рассказ о работе bench of time с более подробными исследованиями — оказалось, что большинство примеров в популярном бенчмарке (mvbench) решается либо знанием всего об одном кадре, либо вообще исключительно по тексту. чтобы исправить эту ситуацию, авторы сделали свой бенчмарк tvbench. в нём все вопросы были сформулированы так, что без понимания объектов и процессов в кадре нельзя дать правильный ответ. сравнение моделей на новом бенчмарке показало, что большинство языковых, картиночных и даже видеомоделей выдают результаты немногим лучше случайного предсказания. при этом все же нашлись несколько моделей, которые были достаточно хороши на обоих бенчмарках, например gemini-1.5. следом было выступление о генерации 3d-представления из изображения. по сути, это продолжение работы dust3r , в которой научились по любым входным изображениям без параметров камер и поз делать матчинг и генерировать плотное облако точек 3d-представления сцены. авторы сделали уточнение, что матчинг изображений по случайному видео с движением — вычислительно сложная задача. поэтому они собрали датасет 360-1m, где происходит движение и вращение вокруг оси, из-за чего матчить изображения стало гораздо проще. на основе своего датасета они обучили генеративную модель odin, которая по изображению и смещению позиции камеры генерирует новое изображение. подробностей было мало, никаких сравнений с dust3r или nerf не показали, но зато рассказали, что модель хорошо обобщается вне домена — например, на картины. самый интересный доклад за день — о том, что визуальные модели знают о нашем мире. авторы выделили и проверили три свойства: базовое представление о физическом устройстве мира, визуальное предсказание, а также обобщение — понимание аналогий. для первого свойства взяли часовые видео с прогулками по городам и с помощью сервиса визуальной локализации, а также небольшого объёма человеческой проверки, разметили эти видео. в частности, для каждого видео сгенерировали маршрут на карте. далее видео нарезали и собрали бенчмарк, в котором модели задавали вопросы по содержанию ролика, например: о евклидовом расстоянии от начальной до конечной точки на полученном маршруте; направлении; зацикленность маршрута; выборе правильного трека на карте среди нескольких вариантов (с текстом на карте и без текста); распознавании окружающей архитектуры. по всем этим вопросам модели уступают человеку — за исключением проверки на зацикленность маршрута. авторы также показали, что на самом деле модели не понимали, был цикл в маршруте или нет. вместо этого они просто смотрели на разметку на карте и сопоставляли её с текстовыми названиями улиц, которые видны в видео. напоследок был доклад из трёх частей, из которых я бы выделил как самую интересную — ssl-обучение мультимодальной модели видео+аудио cav-mae sync. из того, что мне кажется важным: авторы совместно используют аудио- и видеопатчи и добавляют регистровый токен, чтобы переносить накопленную информацию в следующие слои. больше всего мне понравилось, что новая модель позволяет локализовать на видео источники звука. #yaneurips25 cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-12-02T13:00:12+00:00" href="./posts/195.html">2025-12-02 13:00 UTC</a></div>
      </div>
      <div class="post-body"><strong>NeurIPS 2025 в Мехико идёт полным ходом </strong><br><br>Конференция продолжается, а наш коллега Владислав Фахретдинов делится заметками о воркшопе второго дня — <a href="https://neurips.cc/virtual/2025/loc/mexico-city/workshop/127828" rel="nofollow noopener noreferrer">7th International Workshop on Large Scale Holistic Video Understanding: Toward Video Foundation Models</a>. <br><br><blockquote>Было немного спикеров, но почти каждый привёз по две-три статьи или исследования, поэтому день получился насыщенным. Основной мотив воркшопа — большинство моделей для работы с видео недостаточно хорошо ориентируются «во времени». Участники разбирались, что с этим можно сделать.<br><br>Первым выступил профессор университета Амстердама. Он заметил, что многие VideoLLM не справляются даже с простым синтетическим бенчмарком: какой из двух объектов в видео появляется раньше. Это показывает, что мы до конца не понимаем, как правильно оценивать такие способности модели.<br><br>Затем последовал рассказ о работе Bench of Time с более подробными исследованиями — оказалось, что большинство примеров в популярном бенчмарке (MVBench) решается либо знанием всего об одном кадре, либо вообще исключительно по тексту. Чтобы исправить эту ситуацию, авторы сделали свой бенчмарк TVBench. В нём все вопросы были сформулированы так, что без понимания объектов и процессов в кадре нельзя дать правильный ответ. <br><br>Сравнение моделей на новом бенчмарке показало, что большинство языковых, картиночных и даже видеомоделей выдают результаты немногим лучше случайного предсказания. При этом все же нашлись несколько моделей, которые были достаточно хороши на обоих бенчмарках, например Gemini-1.5.<br><br>Следом было выступление о генерации 3D-представления из изображения. По сути, это продолжение работы <a href="https://arxiv.org/abs/2312.14132" rel="nofollow noopener noreferrer">DUSt3R</a>, в которой научились по любым входным изображениям без параметров камер и поз делать матчинг и генерировать плотное облако точек 3D-представления сцены. <br><br>Авторы сделали уточнение, что матчинг изображений по случайному видео с движением — вычислительно сложная задача. Поэтому они собрали датасет 360-1M, где происходит движение и вращение вокруг оси, из-за чего матчить изображения стало гораздо проще. На основе своего датасета они обучили генеративную модель ODIN, которая по изображению и смещению позиции камеры генерирует новое изображение. Подробностей было мало, никаких сравнений с DUSt3R или NeRF не показали, но зато рассказали, что модель хорошо обобщается вне домена — например, на картины.<br><br>Самый интересный доклад за день — о том, что визуальные модели знают о нашем мире. Авторы выделили и проверили три свойства: базовое представление о физическом устройстве мира, визуальное предсказание, а также обобщение — понимание аналогий.<br><br>Для первого свойства взяли часовые видео с прогулками по городам и с помощью сервиса визуальной локализации, а также небольшого объёма человеческой проверки, разметили эти видео. В частности, для каждого видео сгенерировали маршрут на карте.<br><br>Далее видео нарезали и собрали бенчмарк, в котором модели задавали вопросы по содержанию ролика, например: о евклидовом расстоянии от начальной до конечной точки на полученном маршруте; направлении; зацикленность маршрута; выборе правильного трека на карте среди нескольких вариантов (с текстом на карте и без текста); распознавании окружающей архитектуры. По всем этим вопросам модели уступают человеку — за исключением проверки на зацикленность маршрута.<br><br>Авторы также показали, что на самом деле модели не понимали, был цикл в маршруте или нет. Вместо этого они просто смотрели на разметку на карте и сопоставляли её с текстовыми названиями улиц, которые видны в видео.<br><br>Напоследок был доклад из трёх частей, из которых я бы выделил как самую интересную — SSL-обучение мультимодальной модели видео+аудио CAV-MAE Sync. Из того, что мне кажется важным: авторы совместно используют аудио- и видеопатчи и добавляют регистровый токен, чтобы переносить накопленную информацию в следующие слои. Больше всего мне понравилось, что новая модель позволяет локализовать на видео источники звука.</blockquote><br><br>#YaNeurIPS25<br><br><a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/195_480.webp" srcset="../assets/media/thumbs/195_480.webp 480w, ../assets/media/195.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="195" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/196_480.webp" srcset="../assets/media/thumbs/196_480.webp 480w, ../assets/media/196.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="195" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/197_480.webp" srcset="../assets/media/thumbs/197_480.webp 480w, ../assets/media/197.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="195" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/198_480.webp" srcset="../assets/media/thumbs/198_480.webp 480w, ../assets/media/198.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="195" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/199_480.webp" srcset="../assets/media/thumbs/199_480.webp 480w, ../assets/media/199.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="195" data-image-index="4" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/200_480.webp" srcset="../assets/media/thumbs/200_480.webp 480w, ../assets/media/200.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="195" data-image-index="5" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/201_480.webp" srcset="../assets/media/thumbs/201_480.webp 480w, ../assets/media/201.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="195" data-image-index="6" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/202_480.webp" srcset="../assets/media/thumbs/202_480.webp 480w, ../assets/media/202.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="195" data-image-index="7" /></div></div>
      <div class="actions">
        <span>1 140 просмотров · 27 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/195" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/195.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="194" data-search="deepseek-ocr: contexts optical compression [2/2] в первой части разбора мы рассказали об особенностях архитектуры deepseek-ocr и ключевых задачах, которые решали авторы. а теперь посмотрим на нюансы обучения и на озвученные результаты. обучение модели процесс упрощён и включает только две стадии: тренировку энкодера и обучение модели целиком. важный момент: во время тренировки энкодера deepencoder учится работать и в режиме native-resolution, и в режиме tile-based-resolution. то есть модель видит как большие картинки, так и маленькие в разных представлениях. энкодер тренируется на парах картинок и текстовых описаний по схеме, описанной в статье vary: к нему приделывается маленький текстовый декодер, и они вместе обучаются авторегрессионно. второй этап с обучением всей vlm повторяет обычный претрейн/sft во множестве других vlm. результаты авторы представляют небольшую мультиязычную модель, которая может обрабатывать изображения в разном размере и даже в разных режимах динамического разрешения (tile-based, native-resolution). замеры точности распознавания в зависимости от размера изображения (и числа токенов) на ocr-бенчмарке fox показывают, что для надёжного чтения текста можно использовать примерно в 10 раз меньше картиночных токенов, чем необходимо текстовых токенов для представления текста на изображении. при уменьшении этого соотношения качество чтения быстро падает. deepseek-ocr показывает отличное качество на omnidocbench, опережая в зависимости от разрешения не только сильные опенсорсные бэйзлайны, вроде qwen-2.5vl, но и gemini2.5-pro. при этом скорость обработки на gpu сопоставима с пайплайновыми ocr-пакетами, такими как miner, обрабатывая около двух изображений в секунду на а100. в заключение можно заметить, что хотя результаты вышли довольно впечатляющими, в работе использованы только бенчмарки с фокусом на pdf-подобных картинках, а другие, более разнообразные ocr-бенчи для vlm (ocrbench_v2, cc-ocr), не замеряны. также в статье нет аблейтов влияния на результаты ни выбранной архитектуры, ни этапов обучения, поэтому авторы сами называют свои результаты proof-of-concept. разбор подготовил ❣ борис зимка cv time deepseek-ocr: contexts optical compression [2/2] в первой части разбора мы рассказали об особенностях архитектуры deepseek-ocr и ключевых задачах, которые решали авторы. а теперь посмотрим на нюансы обучения и на озвученные результаты. обучение модели процесс упрощён и включает только две стадии: тренировку энкодера и обучение модели целиком. важный момент: во время тренировки энкодера deepencoder учится работать и в режиме native-resolution, и в режиме tile-based-resolution. то есть модель видит как большие картинки, так и маленькие в разных представлениях. энкодер тренируется на парах картинок и текстовых описаний по схеме, описанной в статье vary: к нему приделывается маленький текстовый декодер, и они вместе обучаются авторегрессионно. второй этап с обучением всей vlm повторяет обычный претрейн/sft во множестве других vlm. результаты авторы представляют небольшую мультиязычную модель, которая может обрабатывать изображения в разном размере и даже в разных режимах динамического разрешения (tile-based, native-resolution). замеры точности распознавания в зависимости от размера изображения (и числа токенов) на ocr-бенчмарке fox показывают, что для надёжного чтения текста можно использовать примерно в 10 раз меньше картиночных токенов, чем необходимо текстовых токенов для представления текста на изображении . при уменьшении этого соотношения качество чтения быстро падает. deepseek-ocr показывает отличное качество на omnidocbench, опережая в зависимости от разрешения не только сильные опенсорсные бэйзлайны, вроде qwen-2.5vl, но и gemini2.5-pro. при этом скорость обработки на gpu сопоставима с пайплайновыми ocr-пакетами, такими как miner, обрабатывая около двух изображений в секунду на а100. в заключение можно заметить, что хотя результаты вышли довольно впечатляющими, в работе использованы только бенчмарки с фокусом на pdf-подобных картинках, а другие, более разнообразные ocr-бенчи для vlm (ocrbench_v2, cc-ocr), не замеряны. также в статье нет аблейтов влияния на результаты ни выбранной архитектуры, ни этапов обучения, поэтому авторы сами называют свои результаты proof-of-concept. разбор подготовил ❣ борис зимка cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-11-28T08:33:01+00:00" href="./posts/194.html">2025-11-28 08:33 UTC</a></div>
      </div>
      <div class="post-body"><strong>DeepSeek-OCR: Contexts Optical Compression [2/2]</strong><br><br>В <a href="https://t.me/timeforcv/193" rel="nofollow noopener noreferrer">первой части</a> разбора мы рассказали об особенностях архитектуры <a href="https://arxiv.org/abs/2510.18234" rel="nofollow noopener noreferrer">DeepSeek-OCR</a> и ключевых задачах, которые решали авторы. А теперь посмотрим на нюансы обучения и на озвученные результаты.<br><br><strong>Обучение модели</strong><br><br>Процесс упрощён и включает только две стадии: тренировку энкодера и обучение модели целиком.<br><br>Важный момент: во время тренировки энкодера DeepEncoder учится работать и в режиме native-resolution, и в режиме tile-based-resolution. То есть модель видит как большие картинки, так и маленькие в разных представлениях. <br><br>Энкодер тренируется на парах картинок и текстовых описаний по схеме, описанной в статье Vary: к нему приделывается маленький текстовый декодер, и они вместе обучаются авторегрессионно.<br><br>Второй этап с обучением всей VLM повторяет обычный претрейн/SFT во множестве других VLM. <br><br><strong>Результаты</strong><br> <br>Авторы представляют небольшую мультиязычную модель, которая может обрабатывать изображения в разном размере и даже в разных режимах динамического разрешения (tile-based, native-resolution). <br><br>Замеры точности распознавания в зависимости от размера изображения (и числа токенов) на OCR-бенчмарке Fox показывают, что для надёжного чтения текста можно использовать примерно <strong>в 10 раз меньше картиночных токенов, чем необходимо текстовых токенов для представления текста на изображении</strong>. При уменьшении этого соотношения качество чтения быстро падает.<br><br>DeepSeek-OCR показывает отличное качество на OmniDocBench, опережая в зависимости от разрешения не только сильные опенсорсные бэйзлайны, вроде Qwen-2.5VL, но и Gemini2.5-Pro. При этом скорость обработки на GPU сопоставима с пайплайновыми OCR-пакетами, такими как Miner, обрабатывая около двух изображений в секунду на А100.<br><br>В заключение можно заметить, что хотя результаты вышли довольно впечатляющими, в работе использованы только бенчмарки с фокусом на PDF-подобных картинках, а другие, более разнообразные OCR-бенчи для VLM (OCRBench_v2, CC-OCR), не замеряны. Также в статье нет аблейтов влияния на результаты ни выбранной архитектуры, ни этапов обучения, поэтому авторы сами называют свои результаты proof-of-concept.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Борис Зимка<br></em><a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/194_480.webp" srcset="../assets/media/thumbs/194_480.webp 480w, ../assets/media/194.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="194" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 524 просмотров · 18 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/194" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/194.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="193" data-search="deepseek-ocr: contexts optical compression [1/2] сегодня начинаем разбирать недавнюю статью deepseek-ocr. авторы работы сфокусировались на двух аспектах: 1. обучении эффективной vlm-модели, заточенной именно под ocr-задачи; 2. изучении влияния размера входного изображения на качество работы vlm (и компрессии визуальной информации в целом). сначала небольшое интро по каждому из этих аспектов. ocr-специфичные vlm-модели задачи, связанные с чтением текста, встречаются довольно часто и у простых пользователей, и в бизнес-процессах компаний. такие задачи не требуют знания фактов, агентности, рассуждений, и тратить много gpu на них жалко. за последний год вышло несколько статей по ocr-специализированным легковесным vlm (got, dolphin, uminer, dots.ocr). динамическое разрешение в vlm первые vlm, вроде llava, использовали статический размер изображения: любая картинка для обработки ресайзилась к фиксированному квадрату, прогонялась через энкодер (например clip), готовя картиночные токены на вход llm. так как изображение на входе может быть и пиксельной строкой текста 128 х 16, и большим фото со смартфона 1500 х 4500 пикселей — статический размер работает не оптимально. сегодня для vlm есть два основных способа сделать разрешение динамическим: 1. tile-based-resolution (intern-vl2) — изображение разрезается на квадраты, например 512х512 пикселей, и каждое прогоняется через картиночный энкодер. все выходные токены (чем больше размер — тем больше тайлов и токенов) подаются на вход llm. 2. native-resolution (qwen-vl2) — картиночный энкодер обучается принимать на вход изображение любого размера, используя подходящие для этого позицинные эмбеддинги типа rope. модель и данные deepseek-ocr архитектурно повторяет стандартную для vlm схему: картиночный энкодер, присоединенный к предобученной llm (в этом случае deepseek-3b). однако вместо стандартного clip/siglip в качестве энкодера используется пайплайн из segmentanything (sam-vit-det), свёрточного адаптера и clip (clip-vit), который в статье называют deepencoder. авторы хотели, чтоб энкодер был эффективным и быстрым, и чтобы в уже обученном энкодере можно было легко «на лету» менять количество картиночных токенов. sam-vit-det может принимать на вход изображение любого размера; токенизированные патчи обрабатываются независимо друг от друга благодаря window attention — поэтому количество вычислений уменьшается. затем адаптер снижает количество токенов в 16 раз, а после глобальный аттеншн в clip-vit агрегирует их вместе. для обучении используется типичная смесь пар (картинка-описание) и только текстовых данных с упором на ocr: печатный текст, графики и таблицы, формулы. в отличие от других ocr-специализированных vlm (обычно обучаемых только на английском и китайском), датасеты содержат более 100 языков. во второй части подробнее разберём, как обучали deepseek-ocr и к каким результатам пришли авторы. разбор подготовил ❣ борис зимка cv time deepseek-ocr: contexts optical compression [1/2] сегодня начинаем разбирать недавнюю статью deepseek-ocr . авторы работы сфокусировались на двух аспектах: 1. обучении эффективной vlm-модели, заточенной именно под ocr-задачи; 2. изучении влияния размера входного изображения на качество работы vlm (и компрессии визуальной информации в целом). сначала небольшое интро по каждому из этих аспектов. ocr-специфичные vlm-модели задачи, связанные с чтением текста, встречаются довольно часто и у простых пользователей, и в бизнес-процессах компаний. такие задачи не требуют знания фактов, агентности, рассуждений, и тратить много gpu на них жалко. за последний год вышло несколько статей по ocr-специализированным легковесным vlm (got, dolphin, uminer, dots.ocr). динамическое разрешение в vlm первые vlm, вроде llava, использовали статический размер изображения: любая картинка для обработки ресайзилась к фиксированному квадрату, прогонялась через энкодер (например clip), готовя картиночные токены на вход llm. так как изображение на входе может быть и пиксельной строкой текста 128 х 16, и большим фото со смартфона 1500 х 4500 пикселей — статический размер работает не оптимально. сегодня для vlm есть два основных способа сделать разрешение динамическим: 1. tile-based-resolution (intern-vl2) — изображение разрезается на квадраты, например 512х512 пикселей, и каждое прогоняется через картиночный энкодер. все выходные токены (чем больше размер — тем больше тайлов и токенов) подаются на вход llm. 2. native-resolution (qwen-vl2) — картиночный энкодер обучается принимать на вход изображение любого размера, используя подходящие для этого позицинные эмбеддинги типа rope. модель и данные deepseek-ocr архитектурно повторяет стандартную для vlm схему: картиночный энкодер, присоединенный к предобученной llm (в этом случае deepseek-3b). однако вместо стандартного clip/siglip в качестве энкодера используется пайплайн из segmentanything (sam-vit-det), свёрточного адаптера и clip (clip-vit), который в статье называют deepencoder. авторы хотели, чтоб энкодер был эффективным и быстрым, и чтобы в уже обученном энкодере можно было легко «на лету» менять количество картиночных токенов. sam-vit-det может принимать на вход изображение любого размера; токенизированные патчи обрабатываются независимо друг от друга благодаря window attention — поэтому количество вычислений уменьшается. затем адаптер снижает количество токенов в 16 раз, а после глобальный аттеншн в clip-vit агрегирует их вместе. для обучении используется типичная смесь пар (картинка-описание) и только текстовых данных с упором на ocr: печатный текст, графики и таблицы, формулы. в отличие от других ocr-специализированных vlm (обычно обучаемых только на английском и китайском), датасеты содержат более 100 языков. во второй части подробнее разберём, как обучали deepseek-ocr и к каким результатам пришли авторы. разбор подготовил ❣ борис зимка cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-11-25T10:06:12+00:00" href="./posts/193.html">2025-11-25 10:06 UTC</a></div>
      </div>
      <div class="post-body"><strong>DeepSeek-OCR: Contexts Optical Compression [1/2]</strong><br><br>Сегодня начинаем разбирать недавнюю <a href="https://arxiv.org/abs/2510.18234" rel="nofollow noopener noreferrer">статью DeepSeek-OCR</a>. Авторы работы сфокусировались на двух аспектах:<br><br>1. обучении эффективной VLM-модели, заточенной именно под OCR-задачи;<br>2. изучении влияния размера входного изображения на качество работы VLM (и компрессии визуальной информации в целом).<br><br>Сначала небольшое интро по каждому из этих аспектов.<br><br><strong>OCR-специфичные VLM-модели<br></strong><br>Задачи, связанные с чтением текста, встречаются довольно часто и у простых пользователей, и в бизнес-процессах компаний. Такие задачи не требуют знания фактов, агентности, рассуждений, и тратить много GPU на них жалко. За последний год вышло несколько статей по OCR-специализированным легковесным VLM (GOT, Dolphin, UMiner, dots.ocr).<br><br><strong>Динамическое разрешение в VLM<br></strong><br>Первые VLM, вроде LLaVA, использовали статический размер изображения: любая картинка для обработки ресайзилась к фиксированному квадрату, прогонялась через энкодер (например CLIP), готовя картиночные токены на вход LLM. Так как изображение на входе может быть и пиксельной строкой текста 128 х 16, и большим фото со смартфона 1500 х 4500 пикселей — статический размер работает не оптимально. Сегодня для VLM есть два основных способа сделать разрешение динамическим:<br><br>1. Tile-based-resolution (Intern-VL2) — изображение разрезается на квадраты, например 512х512 пикселей, и каждое прогоняется через картиночный энкодер. Все выходные токены (чем больше размер — тем больше тайлов и токенов) подаются на вход LLM. <br><br>2. Native-resolution (Qwen-VL2) — картиночный энкодер обучается принимать на вход изображение любого размера, используя подходящие для этого позицинные эмбеддинги типа RoPE.<br><br><strong>Модель и данные</strong><br><br>DeepSeek-OCR архитектурно повторяет стандартную для VLM схему: картиночный энкодер, присоединенный к предобученной LLM (в этом случае DeepSeek-3B). <br><br>Однако вместо стандартного CLIP/SigLIP в качестве энкодера используется пайплайн из SegmentAnything (SAM-ViT-Det), свёрточного адаптера и CLIP (CLIP-ViT), который в статье называют DeepEncoder. Авторы хотели, чтоб энкодер был эффективным и быстрым, и чтобы в уже обученном энкодере можно было легко «на лету» менять количество картиночных токенов.<br><br>SAM-ViT-Det может принимать на вход изображение любого размера; токенизированные патчи обрабатываются независимо друг от друга благодаря window attention — поэтому количество вычислений уменьшается. Затем адаптер снижает количество токенов в 16 раз, а после глобальный аттеншн в CLIP-ViT агрегирует их вместе. <br><br>Для обучении используется типичная смесь пар (картинка-описание) и только текстовых данных с упором на OCR: печатный текст, графики и таблицы, формулы. В отличие от других OCR-специализированных VLM (обычно обучаемых только на английском и китайском), датасеты содержат более 100 языков.<br><br>Во второй части подробнее разберём, как обучали DeepSeek-OCR и к каким результатам пришли авторы.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Борис Зимка<br></em><a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/193_480.webp" srcset="../assets/media/thumbs/193_480.webp 480w, ../assets/media/193.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="193" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 655 просмотров · 30 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/193" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/193.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="192" data-search="x-fusion: introducing new modality to frozen large language models сейчас индустрия унифицирует подходы к обработке различных видов данных. существенную часть задач компьютерного зрения решают vlm: генерируют текст на основе изображений и запросов, которые получают на вход. следующий шаг — наделить модели возможностью генерировать изображения. изображения, в отличие от текстов, недискретные, поэтому для них лучше применять вариации диффузионных лоссов, а не next-token prediction. сегодня рассмотрим статью, где предлагается объединить в одной системе два лосса. суперверхнеуровневая схема нового фреймворка x-fusion — на иллюстрации к посту. авторы предлагают использовать две одинаковых предобученных llm: первую заморозить, чтобы она стабильно хорошо справлялась с текстовыми задачами. а её копию — назвать визуальной башней и дообучить для работы с изображениями. если нужно обработать изображение, то закодируем его vae от sd-1,5 и подадим на вход визуальной башне. таким образом, генерация текста происходит через предсказание следующего токена. а для создания изображений выберем токены, расшумим их диффузией и декодируем vae. авторы сравнили четыре базовые архитектуры: — единообразно обрабатывать текстовые и картиночные входы одним трансформером. — дублировать каждый слой llm-gated-слоем. обучать только визуальные слои, результаты складывать, а визуальный выход домножать на обучаемый скаляр. — схема с двойной проекцией: копировать и добучать qkv-матрицы и mlp для визуальной модальности. — финальный вариант: две башни, одна из которых применяется для текстовой модальности, а вторая — для визуальной. а потом либо использовать (в целях экономии вычислений) выходы из соответствующих башен, либо суммировать их с некоторыми весами. x-fusion обучали на синтетике: caption сгенерировали internvl-2.0 26b. а для text-to-image взяли свой inhouse-датасет. хотя по словам авторов, подход с двумя башнями превосходит другие базовые решения в задачах создания изображений, в обратную сторону это не работает: задача генерации текста не помогает получать хорошие caption для изображений. авторы также изучают, стоит ли зашумлять входные латенты для задач распознавания изображений. их вывод — нет, это приводит к деградации качества. разбор подготовил ❣ сергей овчаренко cv time x-fusion: introducing new modality to frozen large language models сейчас индустрия унифицирует подходы к обработке различных видов данных. существенную часть задач компьютерного зрения решают vlm: генерируют текст на основе изображений и запросов, которые получают на вход. следующий шаг — наделить модели возможностью генерировать изображения. изображения, в отличие от текстов, недискретные, поэтому для них лучше применять вариации диффузионных лоссов, а не next-token prediction. сегодня рассмотрим статью , где предлагается объединить в одной системе два лосса. суперверхнеуровневая схема нового фреймворка x-fusion — на иллюстрации к посту. авторы предлагают использовать две одинаковых предобученных llm: первую заморозить, чтобы она стабильно хорошо справлялась с текстовыми задачами. а её копию — назвать визуальной башней и дообучить для работы с изображениями. если нужно обработать изображение, то закодируем его vae от sd-1,5 и подадим на вход визуальной башне. таким образом, генерация текста происходит через предсказание следующего токена. а для создания изображений выберем токены, расшумим их диффузией и декодируем vae. авторы сравнили четыре базовые архитектуры: — единообразно обрабатывать текстовые и картиночные входы одним трансформером. — дублировать каждый слой llm-gated-слоем. обучать только визуальные слои, результаты складывать, а визуальный выход домножать на обучаемый скаляр. — схема с двойной проекцией: копировать и добучать qkv-матрицы и mlp для визуальной модальности. — финальный вариант: две башни, одна из которых применяется для текстовой модальности, а вторая — для визуальной. а потом либо использовать (в целях экономии вычислений) выходы из соответствующих башен, либо суммировать их с некоторыми весами. x-fusion обучали на синтетике: caption сгенерировали internvl-2.0 26b. а для text-to-image взяли свой inhouse-датасет. хотя по словам авторов, подход с двумя башнями превосходит другие базовые решения в задачах создания изображений, в обратную сторону это не работает: задача генерации текста не помогает получать хорошие caption для изображений. авторы также изучают, стоит ли зашумлять входные латенты для задач распознавания изображений. их вывод — нет, это приводит к деградации качества. разбор подготовил ❣ сергей овчаренко cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-11-18T11:16:18+00:00" href="./posts/192.html">2025-11-18 11:16 UTC</a></div>
      </div>
      <div class="post-body"><strong>X-Fusion: Introducing New Modality to Frozen Large Language Models</strong><br><br>Сейчас индустрия унифицирует подходы к обработке различных видов данных. Существенную часть задач компьютерного зрения решают VLM: генерируют текст на основе изображений и запросов, которые получают на вход. Следующий шаг — наделить модели возможностью генерировать изображения. <br><br>Изображения, в отличие от текстов, недискретные, поэтому для них лучше применять вариации диффузионных лоссов, а не next-token prediction. Сегодня рассмотрим <a href="https://arxiv.org/abs/2504.20996" rel="nofollow noopener noreferrer">статью</a>, где предлагается объединить в одной системе два лосса.<br><br>Суперверхнеуровневая схема нового фреймворка X-Fusion — на иллюстрации к посту. Авторы предлагают использовать две одинаковых предобученных LLM: первую заморозить, чтобы она стабильно хорошо справлялась с текстовыми задачами. А её копию — назвать визуальной башней и дообучить для работы с изображениями. <br><br>Если нужно обработать изображение, то закодируем его VAE от SD-1,5 и подадим на вход визуальной башне. Таким образом, генерация текста происходит через предсказание следующего токена. А для создания изображений выберем токены, расшумим их диффузией и декодируем VAE.<br><br>Авторы сравнили четыре базовые архитектуры: <br>— Единообразно обрабатывать текстовые и картиночные входы одним трансформером.<br>— Дублировать каждый слой LLM-gated-слоем. Обучать только визуальные слои, результаты складывать, а визуальный выход домножать на обучаемый скаляр.<br>— Схема с двойной проекцией: копировать и добучать QKV-матрицы и MLP для визуальной модальности. <br>— Финальный вариант: две башни, одна из которых применяется для текстовой модальности, а вторая — для визуальной. А потом либо использовать (в целях экономии вычислений) выходы из соответствующих башен, либо суммировать их с некоторыми весами.<br><br>X-Fusion обучали на синтетике: caption сгенерировали InternVL-2.0 26B. А для text-to-image взяли свой inhouse-датасет. Хотя по словам авторов, подход с двумя башнями превосходит другие базовые решения в задачах создания изображений, в обратную сторону это не работает: задача генерации текста не помогает получать хорошие caption для изображений. Авторы также изучают, стоит ли зашумлять входные латенты для задач распознавания изображений. Их вывод — нет, это приводит к деградации качества. <br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Сергей Овчаренко </em><br><a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/192_480.webp" srcset="../assets/media/thumbs/192_480.webp 480w, ../assets/media/192.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="192" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 806 просмотров · 20 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/192" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/192.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="191" data-search="ernie 4.5 technical report [2/2] продолжаем разбирать технический репорт от baidu. в работе фактически выделены два независимых пайплайна алаймента: llm и vlm. после посттрейна в мультимодальном семействе модели получаются гибридными: они работают и в режиме с ризонингом, и без него. при этом авторы не объясняют, как эти два направления соотносятся между собой и как vlm-компонента влияет на метрики llm (и наоборот). llm-линия: sft и rl с множеством ревордов на этапе sft всё довольно просто — собрали и сбалансировали нужные срезы под задачи. дальше идёт многостадийный rl с разными сигналами. есть rule-based-реворды для ризонинг-задач, есть «верифицируемые» — когда можно проверить ответ прямо в среде, например, запустить код. также используется llm-as-a-judge, где отдельная модель оценивает ответы, и стандартный bradley-terry-реворд, в котором на вход подаётся ещё и ground truth, что не очень типично для таких моделей. вместо классического grpo в работе используют upo (unified preference optimization) — смесь онлайн-rl и офлайн-обучения на парах (dpo-подобный лосс). мотивация — не переобучаться на потенциально шумных сигналах reward-моделей и держать устойчивый сигнал на аккуратно подобранных офлайн-парах. инструкции отбирают так, чтобы дисперсия ревордов по ним была высокой — это даёт полезный сигнал в rl. vlm-линия: три sft-этапа и свой rl в сложных мультимодальных задачах часто «провисает» не сам ризонинг, а перцепция: модель плохо считывает сложные структуры и объекты на картинке. проблема — дефицит плотных, подробных пар «картинка-кэпшен». синтетика тут помогает ограниченно. поэтому авторы делают детальные кэпшены на реальных картинках в срезе stem так, чтобы текст-only-модель могла отвечать на вопрос по исходной картинке, имея только кэпшен. если это работает для множества моделей — кэпшен считается годным и идёт в обучение. sft включает три шага: 1. text-only reasoning cold start. сначала учат чисто текстовый ризонинг (визуальные эксперты и энкодер тут не участвуют). интересно, что vlm-способности при этом не разрушаются и даже появляется генерализация reasoning-паттернов на мультимодальные задачи в срезе stem. 2. reject sampling for multimodal enhancement. берут мультимодальные сэмплы, генерят много гипотез, ранжируют мультимодальными reward-моделями, отбирают лучшие — получается датасет для мультимодального reasoning-sft. 3. thinking / non-thinking fusion. обучение на смеси thinking- и non-thinking-данных; дополнительно описывают идею мёрджа экспертов между ризонинг- и неризонинг-моделью, чтобы перенести полезных мультимодальных экспертов. rl для vlm авторы используют как model-based-сигналы вознаграждения, так и верифицируемые задачи, где можно проверить правильность ответа. к таким задачам относятся stem-примеры (переписывание коротких тестовых вопросов в развёрнутые ответы), визуальные пазлы и генерация html по скриншоту интерфейса с автоматической проверкой через сравнение изображений (рендер против эталона). результаты текстовые модели ernie 4.5 чаще выигрывают у deepseek v3 на основных бенчмарках. после пост-трейна они держатся на уровне проприетарных моделей, вроде gpt-4, особенно хорошо справляясь с instruction-following и длинным контекстом. в мультимодальных задачах ernie 4.5 показывает результаты примерно на уровне qwen 2.5-vl — где-то чуть выше, где-то сопоставимо, особенно в reasoning-режиме. разбор подготовил ❣ алексей григорьев cv time ernie 4.5 technical report [2/2] продолжаем разбирать технический репорт от baidu. в работе фактически выделены два независимых пайплайна алаймента: llm и vlm. после посттрейна в мультимодальном семействе модели получаются гибридными: они работают и в режиме с ризонингом, и без него. при этом авторы не объясняют, как эти два направления соотносятся между собой и как vlm-компонента влияет на метрики llm (и наоборот). llm-линия: sft и rl с множеством ревордов на этапе sft всё довольно просто — собрали и сбалансировали нужные срезы под задачи. дальше идёт многостадийный rl с разными сигналами. есть rule-based-реворды для ризонинг-задач, есть «верифицируемые» — когда можно проверить ответ прямо в среде, например, запустить код. также используется llm-as-a-judge, где отдельная модель оценивает ответы, и стандартный bradley-terry-реворд, в котором на вход подаётся ещё и ground truth, что не очень типично для таких моделей. вместо классического grpo в работе используют upo (unified preference optimization) — смесь онлайн-rl и офлайн-обучения на парах (dpo-подобный лосс). мотивация — не переобучаться на потенциально шумных сигналах reward-моделей и держать устойчивый сигнал на аккуратно подобранных офлайн-парах. инструкции отбирают так, чтобы дисперсия ревордов по ним была высокой — это даёт полезный сигнал в rl. vlm-линия: три sft-этапа и свой rl в сложных мультимодальных задачах часто «провисает» не сам ризонинг, а перцепция: модель плохо считывает сложные структуры и объекты на картинке. проблема — дефицит плотных, подробных пар «картинка-кэпшен». синтетика тут помогает ограниченно. поэтому авторы делают детальные кэпшены на реальных картинках в срезе stem так, чтобы текст-only-модель могла отвечать на вопрос по исходной картинке, имея только кэпшен. если это работает для множества моделей — кэпшен считается годным и идёт в обучение. sft включает три шага: 1. text-only reasoning cold start. сначала учат чисто текстовый ризонинг (визуальные эксперты и энкодер тут не участвуют). интересно, что vlm-способности при этом не разрушаются и даже появляется генерализация reasoning-паттернов на мультимодальные задачи в срезе stem. 2. reject sampling for multimodal enhancement. берут мультимодальные сэмплы, генерят много гипотез, ранжируют мультимодальными reward-моделями, отбирают лучшие — получается датасет для мультимодального reasoning-sft. 3. thinking / non-thinking fusion. обучение на смеси thinking- и non-thinking-данных; дополнительно описывают идею мёрджа экспертов между ризонинг- и неризонинг-моделью, чтобы перенести полезных мультимодальных экспертов. rl для vlm авторы используют как model-based-сигналы вознаграждения, так и верифицируемые задачи, где можно проверить правильность ответа. к таким задачам относятся stem-примеры (переписывание коротких тестовых вопросов в развёрнутые ответы), визуальные пазлы и генерация html по скриншоту интерфейса с автоматической проверкой через сравнение изображений (рендер против эталона). результаты текстовые модели ernie 4.5 чаще выигрывают у deepseek v3 на основных бенчмарках. после пост-трейна они держатся на уровне проприетарных моделей, вроде gpt-4, особенно хорошо справляясь с instruction-following и длинным контекстом. в мультимодальных задачах ernie 4.5 показывает результаты примерно на уровне qwen 2.5-vl — где-то чуть выше, где-то сопоставимо, особенно в reasoning-режиме. разбор подготовил ❣ алексей григорьев cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-11-14T10:09:46+00:00" href="./posts/191.html">2025-11-14 10:09 UTC</a></div>
      </div>
      <div class="post-body"><strong>ERNIE 4.5 Technical Report [2/2]<br></strong><br>Продолжаем разбирать <a href="https://yiyan.baidu.com/blog/publication/" rel="nofollow noopener noreferrer">технический репорт</a> от Baidu. В работе фактически выделены два независимых пайплайна алаймента: LLM и VLM. После посттрейна в мультимодальном семействе модели получаются гибридными: они работают и в режиме с ризонингом, и без него. При этом авторы не объясняют, как эти два направления соотносятся между собой и как VLM-компонента влияет на метрики LLM (и наоборот).<br><br><strong>LLM-линия: SFT и RL с множеством ревордов</strong><br><br>На этапе SFT всё довольно просто — собрали и сбалансировали нужные срезы под задачи. Дальше идёт многостадийный RL с разными сигналами. Есть rule-based-реворды для ризонинг-задач, есть «верифицируемые» — когда можно проверить ответ прямо в среде, например, запустить код. Также используется LLM-as-a-judge, где отдельная модель оценивает ответы, и стандартный Bradley-Terry-реворд, в котором на вход подаётся ещё и ground truth, что не очень типично для таких моделей.<br><br>Вместо классического GRPO в работе используют UPO (Unified Preference Optimization) — смесь онлайн-RL и офлайн-обучения на парах (DPO-подобный лосс). Мотивация — не переобучаться на потенциально шумных сигналах reward-моделей и держать устойчивый сигнал на аккуратно подобранных офлайн-парах. Инструкции отбирают так, чтобы дисперсия ревордов по ним была высокой — это даёт полезный сигнал в RL.<br><br><strong>VLM-линия: три SFT-этапа и свой RL<br></strong><br>В сложных мультимодальных задачах часто «провисает» не сам ризонинг, а перцепция: модель плохо считывает сложные структуры и объекты на картинке. Проблема — дефицит плотных, подробных пар «картинка-кэпшен». Синтетика тут помогает ограниченно. Поэтому авторы делают детальные кэпшены на реальных картинках в срезе STEM так, чтобы текст-only-модель могла отвечать на вопрос по исходной картинке, имея только кэпшен. Если это работает для множества моделей — кэпшен считается годным и идёт в обучение.<br><br><strong>SFT включает три шага:</strong><br><br><strong>1. Text-only Reasoning Cold Start.</strong> Сначала учат чисто текстовый ризонинг (визуальные эксперты и энкодер тут не участвуют). Интересно, что VLM-способности при этом не разрушаются и даже появляется генерализация reasoning-паттернов на мультимодальные задачи в срезе STEM.<br><br><strong>2. Reject Sampling for Multimodal Enhancement.</strong> Берут мультимодальные сэмплы, генерят много гипотез, ранжируют мультимодальными reward-моделями, отбирают лучшие — получается датасет для мультимодального reasoning-SFT.<br><br><strong>3. Thinking / Non-Thinking Fusion.</strong> Обучение на смеси thinking- и non-thinking-данных; дополнительно описывают идею мёрджа экспертов между ризонинг- и неризонинг-моделью, чтобы перенести полезных мультимодальных экспертов.<br><br><strong>RL для VLM</strong><br>Авторы используют как model-based-сигналы вознаграждения, так и верифицируемые задачи, где можно проверить правильность ответа. К таким задачам относятся STEM-примеры (переписывание коротких тестовых вопросов в развёрнутые ответы), визуальные пазлы и генерация HTML по скриншоту интерфейса с автоматической проверкой через сравнение изображений (рендер против эталона). <br><br><strong>Результаты<br></strong><br>Текстовые модели ERNIE 4.5 чаще выигрывают у DeepSeek V3 на основных бенчмарках. После пост-трейна они держатся на уровне проприетарных моделей, вроде GPT-4, особенно хорошо справляясь с instruction-following и длинным контекстом.<br><br>В мультимодальных задачах ERNIE 4.5 показывает результаты примерно на уровне Qwen 2.5-VL — где-то чуть выше, где-то сопоставимо, особенно в reasoning-режиме.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Алексей Григорьев</em> <br><a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer">CV Time</a></div>
      <div class="actions">
        <span>1 763 просмотров · 23 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/191" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/191.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="190" data-search="ernie 4.5 technical report [1/2] сегодня начинаем разбирать большой технический репорт (около 40 страниц без аппендикса) от baidu о том, как они обучали мультимодальные mixture-of-experts (moe)-модели. авторы предлагают целую линейку моделей: moe- и dense-версии, с ризонингом и без, варианты под llm- и vlm-задачи. в этой части разбираем интересные решения в архитектуре и претрейне. архитектура авторы предлагают мультимодальную гетерогенную moe-архитектуру. поддерживаются текст, изображения и видео, на выходе — текст. внутри блока трансформера два роутера: один маршрутизирует текстовые токены, второй — визуальные. кроме специализированных экспертов, есть shared-эксперты, которые всегда активны для обеих модальностей. это нужно, чтобы не было сдвигов при совместном обучении и модальности не разбегались в эмбеддинговом пространстве (авторы ссылаются на работу mixture-of-transformers). для роутинга используется привычный top-k-подход, знакомый нам по deepseek. визуальный энкодер реализован аналогично qwen. с помощью адаптивного 2d rope картинку приводят к подходящему разрешению по соотношению сторон, разбивают на патчи и кодируют. для видео применяют тот же принцип, только с 3d rope и таймстемпами. если ролик не влезает в контекст, выбираются кадры с нужным шагом (adaptive frame-resolution sampling strategy) и при необходимости уменьшают разрешение. потом идёт pixel shuffle и темпоральная компрессия — пространственные размеры урезаются, а временная часть остаётся. в итоге визуальные токены из картинок и видео отправляются в мультимодальный self-attention. претрейн в работе описан стандартный пайплайн с дедупликацией, удалением мусора и quality-фильтрами. но есть и особенности: — data map: с её помощью данные организуют по языку, домену знаний, сценарию, качеству. — human-model-in-the-loop data refinement: асессоры помогают улучшать качество и разметку, результаты возвращаются в обучение классификаторов. — text-only-данные: делятся на пять типов по dikw-фреймворку; отдельный акцент делается на фактические знания и программирование. — interleaved-данные (текст + картинка из веба): аккуратная каталогизация источников, аугментации, чистка, генерация и фильтрация кэпшенов, дедупликация по хешам изображений и текстов; категоризация типов картинок (натуральные сцены, таблицы, скриншоты, чаты, документы и др.). — видео: авторы парсили ролики с богатым контекстом, прогоняли через asr и использовали транскрипты. — domain-specific data: здесь используют прогрессивный рефильтринг данных — примерно так же, как это делалось в deepseekmath. собирают пул url по нужному домену, фильтруют, отправляют на оценку асессором, парсят содержимое, обучают новый классификатор и повторяют цикл. интересная находка: авторы собирали сетки из нескольких картинок в один кадр — так модель лучше учится работать с несколькими изображениями сразу и точнее понимает, о каком объекте речь. также исследователи пишут о применении reeao (record everything everywhere all at once) — способе упаковывать сэмплы так, чтобы максимально заполнять контекст, не теряя остатки, и при этом быть робастными к смене data-parallel-группы. в следующей части разберём интересное из посттрейна. разбор подготовил ❣ данил кашин cv time ernie 4.5 technical report [1/2] сегодня начинаем разбирать большой технический репорт (около 40 страниц без аппендикса) от baidu о том, как они обучали мультимодальные mixture-of-experts (moe)-модели. авторы предлагают целую линейку моделей: moe- и dense-версии, с ризонингом и без, варианты под llm- и vlm-задачи. в этой части разбираем интересные решения в архитектуре и претрейне. архитектура авторы предлагают мультимодальную гетерогенную moe-архитектуру. поддерживаются текст, изображения и видео, на выходе — текст. внутри блока трансформера два роутера: один маршрутизирует текстовые токены, второй — визуальные. кроме специализированных экспертов, есть shared-эксперты, которые всегда активны для обеих модальностей. это нужно, чтобы не было сдвигов при совместном обучении и модальности не разбегались в эмбеддинговом пространстве (авторы ссылаются на работу mixture-of-transformers ). для роутинга используется привычный top-k-подход, знакомый нам по deepseek. визуальный энкодер реализован аналогично qwen. с помощью адаптивного 2d rope картинку приводят к подходящему разрешению по соотношению сторон, разбивают на патчи и кодируют. для видео применяют тот же принцип, только с 3d rope и таймстемпами. если ролик не влезает в контекст, выбираются кадры с нужным шагом (adaptive frame-resolution sampling strategy) и при необходимости уменьшают разрешение. потом идёт pixel shuffle и темпоральная компрессия — пространственные размеры урезаются, а временная часть остаётся. в итоге визуальные токены из картинок и видео отправляются в мультимодальный self-attention. претрейн в работе описан стандартный пайплайн с дедупликацией, удалением мусора и quality-фильтрами. но есть и особенности: — data map : с её помощью данные организуют по языку, домену знаний, сценарию, качеству. — human-model-in-the-loop data refinement : асессоры помогают улучшать качество и разметку, результаты возвращаются в обучение классификаторов. — text-only-данные : делятся на пять типов по dikw-фреймворку; отдельный акцент делается на фактические знания и программирование. — interleaved-данные (текст + картинка из веба): аккуратная каталогизация источников, аугментации, чистка, генерация и фильтрация кэпшенов, дедупликация по хешам изображений и текстов; категоризация типов картинок (натуральные сцены, таблицы, скриншоты, чаты, документы и др.). — видео : авторы парсили ролики с богатым контекстом, прогоняли через asr и использовали транскрипты. — domain-specific data : здесь используют прогрессивный рефильтринг данных — примерно так же, как это делалось в deepseekmath. собирают пул url по нужному домену, фильтруют, отправляют на оценку асессором, парсят содержимое, обучают новый классификатор и повторяют цикл. интересная находка: авторы собирали сетки из нескольких картинок в один кадр — так модель лучше учится работать с несколькими изображениями сразу и точнее понимает, о каком объекте речь. также исследователи пишут о применении reeao (record everything everywhere all at once) — способе упаковывать сэмплы так, чтобы максимально заполнять контекст, не теряя остатки, и при этом быть робастными к смене data-parallel-группы. в следующей части разберём интересное из посттрейна. разбор подготовил ❣ данил кашин cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-11-07T08:26:08+00:00" href="./posts/190.html">2025-11-07 08:26 UTC</a></div>
      </div>
      <div class="post-body"><strong>ERNIE 4.5 Technical Report [1/2]</strong><br><strong><br></strong>Сегодня начинаем разбирать большой <a href="https://yiyan.baidu.com/blog/publication/" rel="nofollow noopener noreferrer">технический репорт</a> (около 40 страниц без аппендикса) от Baidu о том, как они обучали мультимодальные Mixture-of-Experts (MoE)-модели. Авторы предлагают целую линейку моделей: MoE- и dense-версии, с ризонингом и без, варианты под LLM- и VLM-задачи. <br><br>В этой части разбираем интересные решения в архитектуре и претрейне.<br><br><strong>Архитектура </strong><br><br>Авторы предлагают мультимодальную гетерогенную MoE-архитектуру. Поддерживаются текст, изображения и видео, на выходе — текст. Внутри блока трансформера два роутера: один маршрутизирует текстовые токены, второй — визуальные. <br><br>Кроме специализированных экспертов, есть shared-эксперты, которые всегда активны для обеих модальностей. Это нужно, чтобы не было сдвигов при совместном обучении и модальности не разбегались в эмбеддинговом пространстве (авторы ссылаются на работу <a href="https://arxiv.org/abs/2411.04996" rel="nofollow noopener noreferrer">Mixture-of-Transformers</a>). Для роутинга используется привычный top-k-подход, знакомый нам по DeepSeek.<br><br>Визуальный энкодер реализован аналогично Qwen. С помощью адаптивного 2D RoPE  картинку приводят к подходящему разрешению по соотношению сторон, разбивают на патчи и кодируют. Для видео применяют тот же принцип, только с 3D RoPE и таймстемпами.<br><br>Если ролик не влезает в контекст, выбираются кадры с нужным шагом (adaptive frame-resolution sampling strategy) и при необходимости уменьшают разрешение. Потом идёт pixel shuffle и темпоральная компрессия — пространственные размеры урезаются, а временная часть остаётся.<br>В итоге визуальные токены из картинок и видео отправляются в мультимодальный self-attention.<br><br><strong>Претрейн</strong><br><br>В работе описан стандартный пайплайн с дедупликацией, удалением мусора и quality-фильтрами. Но есть и особенности:<br><br>— <strong>Data Map</strong>: с её помощью данные организуют по языку, домену знаний, сценарию, качеству.<br>— <strong>Human-Model-in-the-Loop Data Refinement</strong>: асессоры помогают улучшать качество и разметку, результаты возвращаются в обучение классификаторов.<br>— <strong>Text-only-данные</strong>: делятся на пять типов по DIKW-фреймворку; отдельный акцент делается на фактические знания и программирование.<br>— <strong>Interleaved-данные</strong> (текст + картинка из веба): аккуратная каталогизация источников, аугментации, чистка, генерация и фильтрация кэпшенов, дедупликация по хешам изображений и текстов; категоризация типов картинок (натуральные сцены, таблицы, скриншоты, чаты, документы и др.).<br>— <strong>Видео</strong>: авторы парсили ролики с богатым контекстом, прогоняли через ASR и использовали транскрипты.<br>— <strong>Domain-specific data</strong>: здесь используют прогрессивный рефильтринг данных — примерно так же, как это делалось в DeepSeekMath. Собирают пул URL по нужному домену, фильтруют, отправляют на оценку асессором, парсят содержимое, обучают новый классификатор и повторяют цикл. <br><br>Интересная находка: авторы собирали сетки из нескольких картинок в один кадр — так модель лучше учится работать с несколькими изображениями сразу и точнее понимает, о каком объекте речь.<br><br>Также исследователи пишут о применении REEAO (Record Everything Everywhere All at Once) — способе упаковывать сэмплы так, чтобы максимально заполнять контекст, не теряя остатки, и при этом быть робастными к смене data-parallel-группы. <br><br>В следующей части разберём интересное из посттрейна.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Данил Кашин</em> <br><a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/190_480.webp" srcset="../assets/media/thumbs/190_480.webp 480w, ../assets/media/190.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="190" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 838 просмотров · 23 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/190" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/190.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="189" data-search="loong: generating minute-level long videos with autoregressive language models сегодня разберём статью о loong — авторегрессионной модели для генерации видео на основе llm. архитектура у неё типичная: 1. видео токенизируют. в качестве энкодера использует magvit2. это 3d cnn свёрточная модель, которая обрабатывает темпоральную часть кадров видео, токенизированную с помощью clustering vector quantization. размер токенайзера — 246m параметров. 2. вектора видео подают на вход llm. авторы учат с нуля llama от 700m до 7b параметров: 32 000 токенов для текста, 8 192 — для видео и 10 специальных — скорее всего, для разделителей между кадрами. 3. llm возвращает другие вектора, на основе которых модель-декодер vqgan предсказывает изображения — кадры видео. лосс в конце длинной последовательности кадров оказывается меньше, так как видеотокены в одном видео похожи между собой, а модели проще предсказывать похожие токены последовательно. текстовые токены сильно отличаются от видео: для того чтобы качественно генерировать первые кадры, авторы предлагают перевзвешивать их лосс. обучение делят на три стадии: 1-я стадия. модель предсказывает только одно изображение. 2-я стадия. генерируется 1 секунда видео и 17 фреймов. 3-я стадия. самое длинное видео — 10 секунд. модель обучают на десятисекундных видео. этого мало, если на выходе должно получиться качественное длинное видео. чтобы повысить качество генерации, авторы предлагают так называемый реинкодинг. то есть, генерировать первые кадры по исходному промпту пользователя. а потом брать в качестве следующего промпта несколько последних кадров получившегося видео и генерировать новое. такой подход замедляет инференс, но снижает требования к обучающему датасету. loong тренировали на 100m пар «текст + изображение». для первой стадии использовали датасеты laion-2b и cc12m. обучающие видео — 5,5m клипов, отфильтрованных из hdvg. пример loong подтверждает: генерировать качественные длинные видео можно, даже если обучать модель только на коротких примерах. посмотреть результаты генераций можно на github. разбор подготовил ❣ андрей чернов cv time loong: generating minute-level long videos with autoregressive language models сегодня разберём статью о loong — авторегрессионной модели для генерации видео на основе llm. архитектура у неё типичная: 1. видео токенизируют. в качестве энкодера использует magvit2. это 3d cnn свёрточная модель, которая обрабатывает темпоральную часть кадров видео, токенизированную с помощью clustering vector quantization. размер токенайзера — 246m параметров. 2. вектора видео подают на вход llm. авторы учат с нуля llama от 700m до 7b параметров: 32 000 токенов для текста, 8 192 — для видео и 10 специальных — скорее всего, для разделителей между кадрами. 3. llm возвращает другие вектора, на основе которых модель-декодер vqgan предсказывает изображения — кадры видео. лосс в конце длинной последовательности кадров оказывается меньше, так как видеотокены в одном видео похожи между собой, а модели проще предсказывать похожие токены последовательно. текстовые токены сильно отличаются от видео: для того чтобы качественно генерировать первые кадры, авторы предлагают перевзвешивать их лосс. обучение делят на три стадии: 1-я стадия. модель предсказывает только одно изображение. 2-я стадия. генерируется 1 секунда видео и 17 фреймов. 3-я стадия. самое длинное видео — 10 секунд. модель обучают на десятисекундных видео. этого мало, если на выходе должно получиться качественное длинное видео. чтобы повысить качество генерации, авторы предлагают так называемый реинкодинг. то есть, генерировать первые кадры по исходному промпту пользователя. а потом брать в качестве следующего промпта несколько последних кадров получившегося видео и генерировать новое. такой подход замедляет инференс, но снижает требования к обучающему датасету. loong тренировали на 100m пар «текст + изображение». для первой стадии использовали датасеты laion-2b и cc12m. обучающие видео — 5,5m клипов, отфильтрованных из hdvg. пример loong подтверждает: генерировать качественные длинные видео можно, даже если обучать модель только на коротких примерах. посмотреть результаты генераций можно на github . разбор подготовил ❣ андрей чернов cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-10-28T12:18:01+00:00" href="./posts/189.html">2025-10-28 12:18 UTC</a></div>
      </div>
      <div class="post-body"><strong>Loong: Generating Minute-level Long Videos with Autoregressive Language Models</strong><br><br>Сегодня разберём <a href="https://arxiv.org/abs/2410.02757" rel="nofollow noopener noreferrer">статью</a> о Loong — авторегрессионной модели для генерации видео на основе LLM. Архитектура у неё типичная:<br><br>1. Видео токенизируют. В качестве энкодера использует MAGViT2. Это 3D CNN свёрточная модель, которая обрабатывает темпоральную часть кадров видео, токенизированную с помощью Clustering Vector Quantization. Размер токенайзера — 246M параметров. <br><br>2. Вектора видео подают на вход LLM. Авторы учат с нуля LLaMa от 700M до 7B параметров: 32 000 токенов для текста, 8 192 — для видео и 10 специальных — скорее всего, для разделителей между кадрами. <br><br>3. LLM возвращает другие вектора, на основе которых модель-декодер VQGAN предсказывает изображения — кадры видео. <br><br>Лосс в конце длинной последовательности кадров оказывается меньше, так как видеотокены в одном видео похожи между собой, а модели проще предсказывать похожие токены последовательно. Текстовые токены сильно отличаются от видео: для того чтобы качественно генерировать первые кадры, авторы предлагают перевзвешивать их лосс.<br><br>Обучение делят на три стадии:<br><br><strong>1-я стадия.</strong> Модель предсказывает только одно изображение.<br><strong>2-я стадия.</strong> Генерируется 1 секунда видео и 17 фреймов.<br><strong>3-я стадия.</strong> Самое длинное видео — 10 секунд.<br><br>Модель обучают на десятисекундных видео. Этого мало, если на выходе должно получиться качественное длинное видео. Чтобы повысить качество генерации, авторы предлагают так называемый реинкодинг. То есть, генерировать первые кадры по исходному промпту пользователя. А потом брать в качестве следующего промпта несколько последних кадров получившегося видео и генерировать новое. <br><br>Такой подход замедляет инференс, но снижает требования к обучающему датасету. Loong тренировали на 100M пар «текст + изображение». Для первой стадии использовали датасеты LAION-2B и CC12M. Обучающие видео — 5,5M клипов, отфильтрованных из HDVG.<br><br>Пример Loong подтверждает: генерировать качественные длинные видео можно, даже если обучать модель только на коротких примерах.<br><br>Посмотреть результаты генераций можно на <a href="https://yuqingwang1029.github.io/Loong-video" rel="nofollow noopener noreferrer">GitHub</a>.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Андрей Чернов</em><br><a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><video controls preload="metadata" src="../assets/media/189_fish.mp4"></video></div></div>
      <div class="actions">
        <span>1 774 просмотров · 24 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/189" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/189.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="188" data-search="что читает команда стримингового зрения: подборка актуальных статей заглянули к инженерам команды стримингового зрения в яндексе — узнали, что они читают и обсуждают в последнее время. в сегодняшней подборке: новый мультивидовый датасет для устойчивого отслеживания объектов, трекинг мяча под окклюзией в спортивных видео и рекурсивное рассуждение маленьких нейросетей, которые обгоняют крупные llm на логических задачах. mitracker: multi-view integration for visual object tracking авторы собрали и разметили крупный мультивидовый датасет (260 видео, около 234 тысяч кадров) с калибровкой камер, bev-аннотациями и девятью атрибутами (occlusion, motion blur, low-res и др.). с одной стороны, этот датасет отличается разнообразием классов, с другой — ограничен только сценами в помещениях, что снижает переносимость в уличные условия. как устроен mitracker: — view-specific feature extraction: для каждой камеры используется отдельный vision transformer, который извлекает представления целевого объекта в поточном кадре; объект задаётся эталонным изображением. — multi-view integration: 2d-признаки всех ракурсов проецируются и объединяются в 3d-feature volume с использованием bev-информации; этот объём применяется в spatial-enhanced attention, который корректирует представления и улучшает локализацию и ассоциацию. totnet: occlusion-aware temporal tracking for robust ball detection in sports videos totnet вводит архитектуру для трекинга мяча в спортивных видео, специально сфокусированную на работе в условиях частичной и полной окклюзии. модель сохраняет временную структуру данных за счёт применения 3d-свёрток. это позволяет извлекать динамические признаки движения, а не статические из пачки кадров. ключевые компоненты totnet: — occlusion augmentation: специальная аугментация, которая имитирует скрытие мяча, чтобы модель училась восстанавливать позицию по контексту. — visibility-weighted bce loss: взвешенная функция потерь, которая усиливает вклад случаев с окклюзией при обучении. — интеграция оптического потока (raft): используется для более точного захвата движения мяча в быстрых сценах. в результате модель устойчиво отслеживает мяч, даже когда он временно исчезает из кадра, и превосходит предыдущие методы на всех спортивных датасетах, включая новый датасет tta (table tennis under occlusion). less is more: recursive reasoning with tiny networks в статье авторыпредставляют tiny recursive model (trm) — простой и эффективный подход к решению сложных логических задач. суть метода в использовании маленькой нейросети (всего 7 млн параметров), которая рекурсивно, шаг за шагом «размышляет» над решением и улучшает свои ответы с помощью механизма deep supervision. по результатам экспериментов trm превосходит современные llm на бенчмарках sudoku и arc-agi, используя при этом в тысячи раз меньше вычислительных ресурсов. авторы отмечают, что для некоторых типов задач, особенно при ограниченном количестве обучающих данных, глубокая рекурсия компактной сети помогает избежать переобучения и оказывается намного эффективнее простого увеличения размера модели. cv time что читает команда стримингового зрения: подборка актуальных статей заглянули к инженерам команды стримингового зрения в яндексе — узнали, что они читают и обсуждают в последнее время. в сегодняшней подборке: новый мультивидовый датасет для устойчивого отслеживания объектов, трекинг мяча под окклюзией в спортивных видео и рекурсивное рассуждение маленьких нейросетей, которые обгоняют крупные llm на логических задачах. mitracker: multi-view integration for visual object tracking авторы собрали и разметили крупный мультивидовый датасет (260 видео, около 234 тысяч кадров) с калибровкой камер, bev-аннотациями и девятью атрибутами (occlusion, motion blur, low-res и др.). с одной стороны, этот датасет отличается разнообразием классов, с другой — ограничен только сценами в помещениях, что снижает переносимость в уличные условия. как устроен mitracker: — view-specific feature extraction: для каждой камеры используется отдельный vision transformer, который извлекает представления целевого объекта в поточном кадре; объект задаётся эталонным изображением. — multi-view integration: 2d-признаки всех ракурсов проецируются и объединяются в 3d-feature volume с использованием bev-информации; этот объём применяется в spatial-enhanced attention, который корректирует представления и улучшает локализацию и ассоциацию. totnet: occlusion-aware temporal tracking for robust ball detection in sports videos totnet вводит архитектуру для трекинга мяча в спортивных видео, специально сфокусированную на работе в условиях частичной и полной окклюзии. модель сохраняет временную структуру данных за счёт применения 3d-свёрток. это позволяет извлекать динамические признаки движения, а не статические из пачки кадров. ключевые компоненты totnet: — occlusion augmentation: специальная аугментация, которая имитирует скрытие мяча, чтобы модель училась восстанавливать позицию по контексту. — visibility-weighted bce loss: взвешенная функция потерь, которая усиливает вклад случаев с окклюзией при обучении. — интеграция оптического потока (raft): используется для более точного захвата движения мяча в быстрых сценах. в результате модель устойчиво отслеживает мяч, даже когда он временно исчезает из кадра, и превосходит предыдущие методы на всех спортивных датасетах, включая новый датасет tta (table tennis under occlusion). less is more: recursive reasoning with tiny networks в статье авторыпредставляют tiny recursive model (trm) — простой и эффективный подход к решению сложных логических задач. суть метода в использовании маленькой нейросети (всего 7 млн параметров), которая рекурсивно, шаг за шагом «размышляет» над решением и улучшает свои ответы с помощью механизма deep supervision. по результатам экспериментов trm превосходит современные llm на бенчмарках sudoku и arc-agi, используя при этом в тысячи раз меньше вычислительных ресурсов. авторы отмечают, что для некоторых типов задач, особенно при ограниченном количестве обучающих данных, глубокая рекурсия компактной сети помогает избежать переобучения и оказывается намного эффективнее простого увеличения размера модели. cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-10-24T09:04:51+00:00" href="./posts/188.html">2025-10-24 09:04 UTC</a></div>
      </div>
      <div class="post-body"><strong>Что читает команда стримингового зрения: подборка актуальных статей <br></strong><br>Заглянули к инженерам команды стримингового зрения в Яндексе — узнали, что они читают и обсуждают в последнее время. В сегодняшней подборке: новый мультивидовый датасет для устойчивого отслеживания объектов, трекинг мяча под окклюзией в спортивных видео и рекурсивное рассуждение маленьких нейросетей, которые обгоняют крупные LLM на логических задачах.<br><br><a href="https://arxiv.org/html/2502.20111v1" rel="nofollow noopener noreferrer"><strong>MITracker: Multi-View Integration for Visual Object Tracking<br></strong></a><br>Авторы собрали и разметили крупный мультивидовый датасет (260 видео, около 234 тысяч кадров) с калибровкой камер, BEV-аннотациями и девятью атрибутами (occlusion, motion blur, low-res и др.). С одной стороны, этот датасет отличается разнообразием классов, с другой — ограничен только сценами в помещениях, что снижает переносимость в уличные условия.<br><br>Как устроен MITracker: <br><br>— View-specific feature extraction: для каждой камеры используется отдельный Vision Transformer, который извлекает представления целевого объекта в поточном кадре; объект задаётся эталонным изображением.<br><br>— Multi-view integration: 2D-признаки всех ракурсов проецируются и объединяются в 3D-feature volume с использованием BEV-информации; этот объём применяется в spatial-enhanced attention, который корректирует представления и улучшает локализацию и ассоциацию.<br><br><a href="https://arxiv.org/html/2508.09650v1" rel="nofollow noopener noreferrer"><strong>TOTNet: Occlusion-Aware Temporal Tracking for Robust Ball Detection in Sports Videos</strong></a><a href="https://arxiv.org/pdf/2508.09650" rel="nofollow noopener noreferrer"><strong><br></strong></a><br>TOTNet вводит архитектуру для трекинга мяча в спортивных видео, специально сфокусированную на работе в условиях частичной и полной окклюзии. Модель сохраняет временную структуру данных за счёт применения 3D-свёрток. Это позволяет извлекать динамические признаки движения, а не статические из пачки кадров.<br><br>Ключевые компоненты TOTNet:<br><br>— Occlusion Augmentation: специальная аугментация, которая имитирует скрытие мяча, чтобы модель училась восстанавливать позицию по контексту.<br><br>— Visibility-weighted BCE loss: взвешенная функция потерь, которая усиливает вклад случаев с окклюзией при обучении.<br><br>— Интеграция оптического потока (RAFT): используется для более точного захвата движения мяча в быстрых сценах.<br><br>В результате модель устойчиво отслеживает мяч, даже когда он временно исчезает из кадра, и превосходит предыдущие методы на всех спортивных датасетах, включая новый датасет TTA (Table Tennis under Occlusion).<br><br><a href="https://arxiv.org/abs/2510.04871" rel="nofollow noopener noreferrer"><strong>Less is More: Recursive Reasoning with Tiny Networks<br></strong></a><br>В статье авторыпредставляют Tiny Recursive Model (TRM) — простой и эффективный подход к решению сложных логических задач. Суть метода в использовании маленькой нейросети (всего 7 млн параметров), которая рекурсивно, шаг за шагом «размышляет» над решением и улучшает свои ответы с помощью механизма deep supervision.<br><br>По результатам экспериментов TRM превосходит современные LLM на бенчмарках Sudoku и ARC-AGI, используя при этом в тысячи раз меньше вычислительных ресурсов. Авторы отмечают, что для некоторых типов задач, особенно при ограниченном количестве обучающих данных, глубокая рекурсия компактной сети помогает избежать переобучения и оказывается намного эффективнее простого увеличения размера модели.<br><em><br></em><a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer">CV Time</a></div>
      <div class="actions">
        <span>2 145 просмотров · 27 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/188" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/188.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="187" data-search="look again, think slowly: enhancing visual reflection in vision-language models сегодня разбираем статью с любопытным методом разметки данных, который возвращает внимание модели к картинке, а не только к тексту. при обучении на синтетике визуально-языковые модели быстро перестают смотреть на изображение и уходят в чисто текстовый ризонинг. пример из статьи: нужно вычислить площадь под графиком. текстовая модель пересчитывает шаги правильно, но не учитывает, что площадь под осью идёт с минусом. а модель с «визуальным рефлекшеном» может повторно взглянуть на картинку и заметить этот нюанс. чтобы показать проблему, в статье приводят несколько метрик. первая — attention score между токенами рассуждения и визуальными токенами. чем длиннее ризонинг, тем меньше внимания остаётся на картинку. вторая метрика — расстояние хеллингера. сначала запускают генерацию с картинкой, а затем убирают визуальные токены и продолжают без них. график показывает, что расстояние со временем уменьшается. это значит, что итоговые генерации с убранной картинкой (после нескольких токенов, сгенерированных с изображением) почти не отличаются от генераций, где картинка присутствует. иначе говоря, начиная с какого-то шага модель просто перестаёт использовать изображение и игнорирует его. авторы предлагают модель reflection-v, которая умеет делать рефлекшн именно по изображению. решением становится новая разметка. сначала составляется максимально подробный кэпшн, затем сильная текстовая модель (например, deepseek) выполняет задачу только по описанию. но ключевая идея статьи — агентский пайплайн. llm-агент получает задачу: «на что похожа фигура — на телевизор, телефон, компьютер или часы?». он вызывает vlm и уточняет: «похоже ли это на часы?». vlm отвечает: «есть треугольники и квадраты, ничего круглого — не часы». агент делает вывод: «значит, может быть телефон — у него кнопки сеткой, как клавиатура», и снова уточняет. так формируется диалог, который суммаризатор превращает в связный reasoning trace. в итоге рассуждение действительно опирается на картинку, а не на текст. дополнительно используются фильтрации: если агент ответил без обращения к vlm, пример удаляется. на собранных данных модель обучается с grpo. к обычной награде за правильный ответ добавляется ещё одна — по attention. она измеряет, насколько во второй половине ризонинга модель продолжает опираться на изображение. идея в том, чтобы не дать ей «забыть» картинку в середине рассуждения. тесты проводили на mathvision, mathvista, mmmu, immu-pro, m3cot и hallbench. обучали две версии — reflection-v-3b и reflection-v-7b на базе qwen2.5-vl. они уверенно обгоняют опенсорсные ризонёры на синтетике и даже внутренние модели qwen. в агентской системе «мозгом» выступает qwq-32b (llm-reasoner), визуальным экспертом — qwen-2.5-vl-72b. обучение идёт в два этапа: сначала sft (три эпохи на двух h100), затем grpo (двенадцать эпох на восьми h100 через vllm). всего — около 16 тысяч ризонинг-семплов. сетап скромный, особенно по объёму данных. аблейшны показывают, что полная модель (3b и 7b) даёт лучшие результаты. убираем reward по attention — метрики падают. без sft — ещё хуже. убираем и то, и другое — совсем провал. вывод авторов очевиден: все элементы нужны и каждый вносит свой вклад. разбор подготовил ❣ илья димов cv time look again, think slowly: enhancing visual reflection in vision-language models сегодня разбираем статью с любопытным методом разметки данных, который возвращает внимание модели к картинке, а не только к тексту. при обучении на синтетике визуально-языковые модели быстро перестают смотреть на изображение и уходят в чисто текстовый ризонинг. пример из статьи: нужно вычислить площадь под графиком. текстовая модель пересчитывает шаги правильно, но не учитывает, что площадь под осью идёт с минусом. а модель с «визуальным рефлекшеном» может повторно взглянуть на картинку и заметить этот нюанс. чтобы показать проблему, в статье приводят несколько метрик. первая — attention score между токенами рассуждения и визуальными токенами. чем длиннее ризонинг, тем меньше внимания остаётся на картинку. вторая метрика — расстояние хеллингера. сначала запускают генерацию с картинкой, а затем убирают визуальные токены и продолжают без них. график показывает, что расстояние со временем уменьшается. это значит, что итоговые генерации с убранной картинкой (после нескольких токенов, сгенерированных с изображением) почти не отличаются от генераций, где картинка присутствует. иначе говоря, начиная с какого-то шага модель просто перестаёт использовать изображение и игнорирует его. авторы предлагают модель reflection-v, которая умеет делать рефлекшн именно по изображению. решением становится новая разметка. сначала составляется максимально подробный кэпшн, затем сильная текстовая модель (например, deepseek) выполняет задачу только по описанию. но ключевая идея статьи — агентский пайплайн. llm-агент получает задачу: «на что похожа фигура — на телевизор, телефон, компьютер или часы?» . он вызывает vlm и уточняет: «похоже ли это на часы?» . vlm отвечает: « есть треугольники и квадраты, ничего круглого — не часы» . агент делает вывод: «значит, может быть телефон — у него кнопки сеткой, как клавиатура» , и снова уточняет. так формируется диалог, который суммаризатор превращает в связный reasoning trace. в итоге рассуждение действительно опирается на картинку, а не на текст. дополнительно используются фильтрации: если агент ответил без обращения к vlm, пример удаляется. на собранных данных модель обучается с grpo. к обычной награде за правильный ответ добавляется ещё одна — по attention. она измеряет, насколько во второй половине ризонинга модель продолжает опираться на изображение. идея в том, чтобы не дать ей «забыть» картинку в середине рассуждения. тесты проводили на mathvision, mathvista, mmmu, immu-pro, m3cot и hallbench. обучали две версии — reflection-v-3b и reflection-v-7b на базе qwen2.5-vl. они уверенно обгоняют опенсорсные ризонёры на синтетике и даже внутренние модели qwen. в агентской системе «мозгом» выступает qwq-32b (llm-reasoner), визуальным экспертом — qwen-2.5-vl-72b. обучение идёт в два этапа: сначала sft (три эпохи на двух h100), затем grpo (двенадцать эпох на восьми h100 через vllm). всего — около 16 тысяч ризонинг-семплов. сетап скромный, особенно по объёму данных. аблейшны показывают, что полная модель (3b и 7b) даёт лучшие результаты. убираем reward по attention — метрики падают. без sft — ещё хуже. убираем и то, и другое — совсем провал. вывод авторов очевиден: все элементы нужны и каждый вносит свой вклад. разбор подготовил ❣ илья димов cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-10-17T12:03:08+00:00" href="./posts/187.html">2025-10-17 12:03 UTC</a></div>
      </div>
      <div class="post-body"><strong>Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language Models</strong><br><br>Сегодня разбираем <a href="https://arxiv.org/abs/2509.12132" rel="nofollow noopener noreferrer">статью</a> с любопытным методом разметки данных, который возвращает внимание модели к картинке, а не только к тексту.<br><br>При обучении на синтетике визуально-языковые модели быстро перестают смотреть на изображение и уходят в чисто текстовый ризонинг. Пример из статьи: нужно вычислить площадь под графиком. Текстовая модель пересчитывает шаги правильно, но не учитывает, что площадь под осью идёт с минусом. А модель с «визуальным рефлекшеном» может повторно взглянуть на картинку и заметить этот нюанс.<br><br>Чтобы показать проблему, в статье приводят несколько метрик. Первая — attention score между токенами рассуждения и визуальными токенами. Чем длиннее ризонинг, тем меньше внимания остаётся на картинку. <br><br>Вторая метрика — расстояние Хеллингера. Сначала запускают генерацию с картинкой, а затем убирают визуальные токены и продолжают без них. График показывает, что расстояние со временем уменьшается. Это значит, что итоговые генерации с убранной картинкой (после нескольких токенов, сгенерированных с изображением) почти не отличаются от генераций, где картинка присутствует. Иначе говоря, начиная с какого-то шага модель просто перестаёт использовать изображение и игнорирует его.<br><br>Авторы предлагают модель Reflection-V, которая умеет делать рефлекшн именно по изображению. <br><br>Решением становится новая разметка. Сначала составляется максимально подробный кэпшн, затем сильная текстовая модель (например, DeepSeek) выполняет задачу только по описанию. <br><br>Но ключевая идея статьи — агентский пайплайн. LLM-агент получает задачу: <em>«На что похожа фигура — на телевизор, телефон, компьютер или часы?»</em>. Он вызывает VLM и уточняет: <em>«Похоже ли это на часы?»</em>. VLM отвечает: «<em>Есть треугольники и квадраты, ничего круглого — не часы»</em>. Агент делает вывод: <em>«Значит, может быть телефон — у него кнопки сеткой, как клавиатура»</em>, и снова уточняет. Так формируется диалог, который суммаризатор превращает в связный reasoning trace. В итоге рассуждение действительно опирается на картинку, а не на текст.<br><br>Дополнительно используются фильтрации: если агент ответил без обращения к VLM, пример удаляется. На собранных данных модель обучается с GRPO. К обычной награде за правильный ответ добавляется ещё одна — по attention. Она измеряет, насколько во второй половине ризонинга модель продолжает опираться на изображение. Идея в том, чтобы не дать ей «забыть» картинку в середине рассуждения.<br><br>Тесты проводили на MathVision, MathVista, MMMU, IMMU-Pro, M3CoT и HallBench. Обучали две версии — Reflection-V-3B и Reflection-V-7B на базе Qwen2.5-VL. Они уверенно обгоняют опенсорсные ризонёры на синтетике и даже внутренние модели Qwen. <br><br>В агентской системе «мозгом» выступает QWQ-32B (LLM-reasoner), визуальным экспертом — Qwen-2.5-VL-72B. Обучение идёт в два этапа: сначала SFT (три эпохи на двух H100), затем GRPO (двенадцать эпох на восьми H100 через vLLM). Всего — около 16 тысяч ризонинг-семплов. Сетап скромный, особенно по объёму данных.<br>Аблейшны показывают, что полная модель (3B и 7B) даёт лучшие результаты. <br><br>Убираем reward по attention — метрики падают. Без SFT — ещё хуже. Убираем и то, и другое — совсем провал. Вывод авторов очевиден: все элементы нужны и каждый вносит свой вклад.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Илья Димов<br></em><a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/187_480.webp" srcset="../assets/media/thumbs/187_480.webp 480w, ../assets/media/187.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="187" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 419 просмотров · 21 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/187" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/187.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="186" data-search="работы по сбору датасетов для задачи instruction-based editing вместе с ростом популярности t2i-генерации стала активно развиваться и задача редактирования изображений. несмотря на очевидные сходства, между ними есть как минимум одно ключевое отличие: редактирование — не одна задача, а целое семейство, и с точки зрения ml, и с точки зрения данных. сергей кастрюлин, исследователь yandex research, разобрал основные работы по сбору датасетов для задачи instruction-based editing. paint by inpaint: learning to add image objects by removing them first [датасет на hf, без лицензии] крупный (1,8m сэмплов) датасет, полностью посвящённый задаче добавления/удаления объектов. авторы стартуют с картинок из coco и openimages, для которых уже просчитаны маски (датасет lvis). по этим маскам делают remove через sd-inpainting. основная часть работы посвящена фильтрациям: — исходные пары картинка-маска фильтруют по размеру и положению маски (слишком мелкая, слишком близка к краю картинки). — после инпейнтинга проверяют, что объект действительно удалён, что удалён именно важный объект и что в целом картинка не испортилась, вычисляя набор эвристических метрик на основе локальных clip-эмбеддингов. в статье указано соотношение source- и target-картинок: из ~800к исходных получили 1,800к таргетов. это довольно сбалансированное распределение. seed-data-edit technical report: a hybrid dataset for instructional image editing [датасет на hf, некоммерческий] ещё один большой (1,5м сэмплов) датасет, состоящий из трёх частей. часть 1: синтетические данные 1) добавление и удаление объектов: — берут изображения из unsplash и openimages. — с помощью моделей llava-1.5, groundingdino и sam сегментируют объекты, подходящие для удаления. — делают удаление с помощью модели инпейнтинга lama. — для получения данных на задачу добавления объектов инвертируют триплеты. 2) изменение объектов: — берут реальную картинку, кепшенят её. — с помощью chatgpt изменяют часть исходного инстракта. — берут image-guided t2i-модель pnp, подают в неё исходную картинку и измененный инстракт, получают результат. части 2 и 3: реальные данные — парсят сайты, где пользователи просят отфотошопить картинки. получают 52к триплетов. — просят асессоров в фотошопе последовательно внести простые изменения и описать их кепшенами. получают 21к последовательностей разной длины (до пяти редактирований на картинку). на смеси данных учат lora для модели seed-x. минусы: — в отличие от qwen-image авторы не перераспределяют данные по стадиям (было бы логично начать с плохой синетики, а закончить обучение на чистых реальных данных). — информация о последовательных редактированиях никак не используется — её просто перегруппируют в триплеты. — о фильтрации не сказано ни слова, так что датасет почти наверняка шумный. anyedit: mastering unified high-quality image editing for any idea [датасет на hf, без лицензии] 2,5м сэмплов, разбитых на 5 категорий для увеличения разнообразия данных: — локальное редактирование: добавление, удаление или замена объектов, изменение цвета и действий; — глобальное редактирование: изменение тона, стиля или фона изображения; — редактирование, связанное с движением камеры: расширение кадра, поворот, изменение размера; — визуальное редактирование: перенос материалов, работа со скетчами и масками; — неявное редактирование (implicit editing). авторы стартуют с 680к из нескольких открытых датасетов. в данных отсутствуют «редкие концепты», поэтому генерят синтетические исходные картинки: — определяют редкие концепты. — просят lm сгенерить промпты для t2i-модели, чтоб они включали эти концепты. — генерят еще 700к картинок, доливают к исходным реальным. затем берутся промпты к исходным синтетическим картинкам и кепшены к реальным и — из них с помощью llama3-8b генерятся editing-инстракты. в статье описаны 9 пайплайнов генерации данных для покрытия пяти категорий задач указанных выше (figure 7, appendix). после генерации есть фильтрация на основе clip-based эвристик. продолжение читайте в авторском канале сергея кастрюлина @c_research. cv time работы по сбору датасетов для задачи instruction-based editing вместе с ростом популярности t2i-генерации стала активно развиваться и задача редактирования изображений. несмотря на очевидные сходства, между ними есть как минимум одно ключевое отличие: редактирование — не одна задача, а целое семейство, и с точки зрения ml, и с точки зрения данных. сергей кастрюлин, исследователь yandex research, разобрал основные работы по сбору датасетов для задачи instruction-based editing. paint by inpaint: learning to add image objects by removing them first [ датасет на hf , без лицензии] крупный (1,8m сэмплов) датасет, полностью посвящённый задаче добавления/удаления объектов. авторы стартуют с картинок из coco и openimages, для которых уже просчитаны маски (датасет lvis). по этим маскам делают remove через sd-inpainting. основная часть работы посвящена фильтрациям: — исходные пары картинка-маска фильтруют по размеру и положению маски (слишком мелкая, слишком близка к краю картинки). — после инпейнтинга проверяют, что объект действительно удалён, что удалён именно важный объект и что в целом картинка не испортилась, вычисляя набор эвристических метрик на основе локальных clip-эмбеддингов. в статье указано соотношение source- и target-картинок: из ~800к исходных получили 1,800к таргетов. это довольно сбалансированное распределение. seed-data-edit technical report: a hybrid dataset for instructional image editing [ датасет на hf , некоммерческий] ещё один большой (1,5м сэмплов) датасет, состоящий из трёх частей. часть 1: синтетические данные 1) добавление и удаление объектов: — берут изображения из unsplash и openimages. — с помощью моделей llava-1.5, groundingdino и sam сегментируют объекты, подходящие для удаления. — делают удаление с помощью модели инпейнтинга lama. — для получения данных на задачу добавления объектов инвертируют триплеты. 2) изменение объектов: — берут реальную картинку, кепшенят её. — с помощью chatgpt изменяют часть исходного инстракта. — берут image-guided t2i-модель pnp, подают в неё исходную картинку и измененный инстракт, получают результат. части 2 и 3: реальные данные — парсят сайты, где пользователи просят отфотошопить картинки. получают 52к триплетов. — просят асессоров в фотошопе последовательно внести простые изменения и описать их кепшенами. получают 21к последовательностей разной длины (до пяти редактирований на картинку). на смеси данных учат lora для модели seed-x. минусы: — в отличие от qwen-image авторы не перераспределяют данные по стадиям (было бы логично начать с плохой синетики, а закончить обучение на чистых реальных данных). — информация о последовательных редактированиях никак не используется — её просто перегруппируют в триплеты. — о фильтрации не сказано ни слова, так что датасет почти наверняка шумный. anyedit: mastering unified high-quality image editing for any idea [ датасет на hf , без лицензии] 2,5м сэмплов, разбитых на 5 категорий для увеличения разнообразия данных: — локальное редактирование: добавление, удаление или замена объектов, изменение цвета и действий; — глобальное редактирование: изменение тона, стиля или фона изображения; — редактирование, связанное с движением камеры: расширение кадра, поворот, изменение размера; — визуальное редактирование: перенос материалов, работа со скетчами и масками; — неявное редактирование (implicit editing). авторы стартуют с 680к из нескольких открытых датасетов. в данных отсутствуют «редкие концепты», поэтому генерят синтетические исходные картинки: — определяют редкие концепты. — просят lm сгенерить промпты для t2i-модели, чтоб они включали эти концепты. — генерят еще 700к картинок, доливают к исходным реальным. затем берутся промпты к исходным синтетическим картинкам и кепшены к реальным и — из них с помощью llama3-8b генерятся editing-инстракты. в статье описаны 9 пайплайнов генерации данных для покрытия пяти категорий задач указанных выше (figure 7, appendix). после генерации есть фильтрация на основе clip-based эвристик. продолжение читайте в авторском канале сергея кастрюлина @c_research . cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-10-13T08:52:06+00:00" href="./posts/186.html">2025-10-13 08:52 UTC</a></div>
      </div>
      <div class="post-body"><strong>Работы по сбору датасетов для задачи instruction-based editing<br></strong><br>Вместе с ростом популярности T2I-генерации стала активно развиваться и задача редактирования изображений. Несмотря на очевидные сходства, между ними есть как минимум одно ключевое отличие: редактирование — не одна задача, а целое семейство, и с точки зрения ML, и с точки зрения данных.<br><br>Сергей Кастрюлин, исследователь Yandex Research, разобрал основные работы по сбору датасетов для задачи instruction-based editing. <br><br><a href="https://arxiv.org/abs/2404.18212" rel="nofollow noopener noreferrer"><strong>Paint by Inpaint: Learning to Add Image Objects by Removing Them First</strong></a> [<a href="https://huggingface.co/datasets/paint-by-inpaint/PIPE" rel="nofollow noopener noreferrer">датасет на HF</a>, без лицензии]<br><br>Крупный (1,8M сэмплов) датасет, полностью посвящённый задаче добавления/удаления объектов. Авторы стартуют с картинок из COCO и OpenImages, для которых уже просчитаны маски (датасет LVIS). По этим маскам делают Remove через SD-Inpainting.<br><br>Основная часть работы посвящена фильтрациям:<br>— Исходные пары картинка-маска фильтруют по размеру и положению маски (слишком мелкая, слишком близка к краю картинки).<br>— После инпейнтинга проверяют, что объект действительно удалён, что удалён именно важный объект и что в целом картинка не испортилась, вычисляя набор эвристических метрик на основе локальных CLIP-эмбеддингов.<br><br>В статье указано соотношение source- и target-картинок: из ~800К исходных получили 1,800К таргетов. Это довольно сбалансированное распределение.<br><br><a href="https://arxiv.org/abs/2405.04007" rel="nofollow noopener noreferrer"><strong>SEED-Data-Edit Technical Report: A Hybrid Dataset for Instructional Image Editing</strong></a> [<a href="https://huggingface.co/datasets/AILab-CVC/SEED-Data-Edit" rel="nofollow noopener noreferrer">датасет на HF</a>, некоммерческий]<br><br>Ещё один большой (1,5М сэмплов) датасет, состоящий из трёх частей.<br><br><strong>Часть 1: синтетические данные<br></strong><br> 1) Добавление и удаление объектов:<br>— Берут изображения из Unsplash и OpenImages.<br>— С помощью моделей LLAVA-1.5, GroundingDINO и SAM сегментируют объекты, подходящие для удаления.<br>— Делают удаление с помощью модели инпейнтинга LaMa.<br>— Для получения данных на задачу добавления объектов инвертируют триплеты.<br><br>2) Изменение объектов:<br>— Берут реальную картинку, кепшенят её.<br>— С помощью ChatGPT изменяют часть исходного инстракта. <br>— Берут image-guided T2I-модель PnP, подают в неё исходную картинку и измененный инстракт, получают результат.<br><br><strong>Части 2 и 3: реальные данные<br></strong><br>— Парсят сайты, где пользователи просят отфотошопить картинки. Получают 52К триплетов.<br>— Просят асессоров в фотошопе последовательно внести простые изменения и описать их кепшенами. Получают 21К последовательностей разной длины (до пяти редактирований на картинку).<br><br>На смеси данных учат LoRA для модели SEED-X. Минусы:<br>— В отличие от Qwen-Image авторы не перераспределяют данные по стадиям (было бы логично начать с плохой синетики, а закончить обучение на чистых реальных данных).<br>— Информация о последовательных редактированиях никак не используется — её просто перегруппируют в триплеты.<br>— О фильтрации не сказано ни слова, так что датасет почти наверняка шумный.<br><br><a href="https://arxiv.org/abs/2411.15738" rel="nofollow noopener noreferrer"><strong>AnyEdit: Mastering Unified High-Quality Image Editing for Any Idea</strong></a> [<a href="https://huggingface.co/datasets/Bin1117/AnyEdit" rel="nofollow noopener noreferrer">датасет на HF</a>, без лицензии]<br><br>2,5М сэмплов, разбитых на 5 категорий для увеличения разнообразия данных:<br><br>— Локальное редактирование: добавление, удаление или замена объектов, изменение цвета и действий;<br>— Глобальное редактирование: изменение тона, стиля или фона изображения;<br>— Редактирование, связанное с движением камеры: расширение кадра, поворот, изменение размера;<br>— Визуальное редактирование: перенос материалов, работа со скетчами и масками;<br>— Неявное редактирование (Implicit Editing).<br><br>Авторы стартуют с 680К из нескольких открытых датасетов. В данных отсутствуют «редкие концепты», поэтому генерят синтетические исходные картинки:<br><br>— Определяют редкие концепты.<br>— Просят LM сгенерить промпты для T2I-модели, чтоб они включали эти концепты.<br>— Генерят еще 700К картинок, доливают к исходным реальным.<br><br>Затем берутся промпты к исходным синтетическим картинкам и кепшены к реальным и — из них с помощью Llama3-8b генерятся editing-инстракты. <br><br>В статье описаны 9 пайплайнов генерации данных для покрытия пяти категорий задач указанных выше (Figure 7, appendix). После генерации есть фильтрация на основе CLIP-based эвристик.<br><br><strong>Продолжение читайте в авторском канале Сергея Кастрюлина </strong><strong>@c_research</strong><strong>.<br></strong><br><a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer">CV Time</a></div>
      <div class="actions">
        <span>1 963 просмотров · 13 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/186" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/186.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="184" data-search="glm-4.5v and glm-4.1v-thinking: towards versatile multimodal reasoning with scalable reinforcement learning сегодня разберём статью о том, как с помощью reinforcement learning (rl) и curriculum sampling обучить сильную визуально-языковую модель (vlm), опережающую аналоги в опенсорс-бенчмарках. именно такой подход помог создать glm-4.5v. авторы позиционируют glm‑4.5v как одну из лучших открытых моделей для широкого круга задач: работа с длинными документами, агентный режим, видеоанализ, ocr и графика, генерация кода, stem и vqa. архитектурно glm‑4.5v близка к современным vlm и во многом напоминает qwen2‑vl. модель состоит из трёх ключевых компонентов: визуального энкодера, mlp‑адаптера и llm‑декодера (moe 12а109b). для кодирования визуальных токенов в vit применяются 2d‑rope и интерполяция абсолютных позиций для произвольных разрешений и экстремальных аспект‑ratios. а в llm используются 3d‑rope и временные индексы для видео, что улучшает моделирование темпоральных зависимостей. модель предобучали с нуля на академических текстовых корпусах и больших, разнообразных наборах изображений. для этого понадобилось свыше 10b пар «изображение + текст», отфильтрованных при помощи clip‑подобной модели. чтобы минимизировать смещения, все операции с данными сопровождались сбором статистик: нормировали частоты в корпусе, следили за распределениями и итеративно улучшали собственный captioning‑пайплайн. итоговый объём претренировочного датасета составил около 2t токенов. крупный претрейн и аккуратно собранный корпус для sft с чётко заданным форматом ответов создали прочную основу для rl‑стадии. качество модели оценивали через многократное сэмплирование предсказаний и подсчёт pass@k на разных бенчмарках — это позволило заранее понимать, как система проявит себя после rl. главное новшество — мультидоменный онлайн‑rl с продуманной reward‑системой на базе grpo. авторы валидировали отдельные критерии оценки для каждого домена, контролировали риск reward hacking и балансировали сложность примеров. такой подход позволил получить хорошее межпредметное обобщение: обучение в одном домене повышало качество в других, а совместное обучение сразу в нескольких — приводило к ещё большим улучшениям в каждом из них. второе важное нововведение — curriculum sampling: отбор наиболее полезных примеров для обучения. подготовка выборки (rlcs) и её динамическое расширение реализованы с помощью экспоненциальной скользящей средней (ema), что стабилизирует траекторию обучения и ускоряет сходимость модели. по итогам проверки на 42 публичных бенчмарках glm‑4.5v обеспечивает высокие результаты почти во всех задачах среди открытых моделей сопоставимого размера и демонстрирует конкурентоспособность по отношению к закрытым решениям. познакомиться с glm-4.5v можно на github. разбор подготовил ❣ данил кашин cv time glm-4.5v and glm-4.1v-thinking: towards versatile multimodal reasoning with scalable reinforcement learning сегодня разберём статью о том, как с помощью reinforcement learning (rl) и curriculum sampling обучить сильную визуально-языковую модель (vlm), опережающую аналоги в опенсорс-бенчмарках. именно такой подход помог создать glm-4.5v. авторы позиционируют glm‑4.5v как одну из лучших открытых моделей для широкого круга задач: работа с длинными документами, агентный режим, видеоанализ, ocr и графика, генерация кода, stem и vqa. архитектурно glm‑4.5v близка к современным vlm и во многом напоминает qwen2‑vl. модель состоит из трёх ключевых компонентов: визуального энкодера, mlp‑адаптера и llm‑декодера (moe 12а109b). для кодирования визуальных токенов в vit применяются 2d‑rope и интерполяция абсолютных позиций для произвольных разрешений и экстремальных аспект‑ratios. а в llm используются 3d‑rope и временные индексы для видео, что улучшает моделирование темпоральных зависимостей. модель предобучали с нуля на академических текстовых корпусах и больших, разнообразных наборах изображений. для этого понадобилось свыше 10b пар «изображение + текст», отфильтрованных при помощи clip‑подобной модели. чтобы минимизировать смещения, все операции с данными сопровождались сбором статистик: нормировали частоты в корпусе, следили за распределениями и итеративно улучшали собственный captioning‑пайплайн. итоговый объём претренировочного датасета составил около 2t токенов. крупный претрейн и аккуратно собранный корпус для sft с чётко заданным форматом ответов создали прочную основу для rl‑стадии. качество модели оценивали через многократное сэмплирование предсказаний и подсчёт pass@k на разных бенчмарках — это позволило заранее понимать, как система проявит себя после rl. главное новшество — мультидоменный онлайн‑rl с продуманной reward‑системой на базе grpo. авторы валидировали отдельные критерии оценки для каждого домена, контролировали риск reward hacking и балансировали сложность примеров. такой подход позволил получить хорошее межпредметное обобщение: обучение в одном домене повышало качество в других, а совместное обучение сразу в нескольких — приводило к ещё большим улучшениям в каждом из них. второе важное нововведение — curriculum sampling: отбор наиболее полезных примеров для обучения. подготовка выборки (rlcs) и её динамическое расширение реализованы с помощью экспоненциальной скользящей средней (ema), что стабилизирует траекторию обучения и ускоряет сходимость модели. по итогам проверки на 42 публичных бенчмарках glm‑4.5v обеспечивает высокие результаты почти во всех задачах среди открытых моделей сопоставимого размера и демонстрирует конкурентоспособность по отношению к закрытым решениям. познакомиться с glm-4.5v можно на github . разбор подготовил ❣ данил кашин cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-10-08T09:02:44+00:00" href="./posts/184.html">2025-10-08 09:02 UTC</a></div>
      </div>
      <div class="post-body"><strong>GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning</strong><br><br>Сегодня разберём <a href="https://arxiv.org/abs/2507.01006" rel="nofollow noopener noreferrer">статью</a> о том, как с помощью reinforcement learning (RL) и curriculum sampling обучить сильную визуально-языковую модель (VLM), опережающую аналоги в опенсорс-бенчмарках. Именно такой подход помог создать GLM-4.5V.<br><br>Авторы позиционируют GLM‑4.5V как одну из лучших открытых моделей для широкого круга задач: работа с длинными документами, агентный режим, видеоанализ, OCR и графика, генерация кода, STEM и VQA.<br><br>Архитектурно GLM‑4.5V близка к современным VLM и во многом напоминает Qwen2‑VL. Модель состоит из трёх ключевых компонентов: визуального энкодера, MLP‑адаптера и LLM‑декодера (MoE 12А109B). Для кодирования визуальных токенов в ViT применяются 2D‑RoPE и интерполяция абсолютных позиций для произвольных разрешений и экстремальных аспект‑ratios. А в LLM используются 3D‑RoPE и временные индексы для видео, что улучшает моделирование темпоральных зависимостей.<br><br>Модель предобучали с нуля на академических текстовых корпусах и больших, разнообразных наборах изображений. Для этого понадобилось свыше 10B пар «изображение + текст», отфильтрованных при помощи CLIP‑подобной модели. Чтобы минимизировать смещения, все операции с данными сопровождались сбором статистик: нормировали частоты в корпусе, следили за распределениями и итеративно улучшали собственный captioning‑пайплайн. Итоговый объём претренировочного датасета составил около 2T токенов.<br><br>Крупный претрейн и аккуратно собранный корпус для SFT с чётко заданным форматом ответов создали прочную основу для RL‑стадии. Качество модели оценивали через многократное сэмплирование предсказаний и подсчёт PASS@k на разных бенчмарках — это позволило заранее понимать, как система проявит себя после RL.<br><br>Главное новшество — мультидоменный онлайн‑RL с продуманной reward‑системой на базе GRPO. Авторы валидировали отдельные критерии оценки для каждого домена, контролировали риск reward hacking и балансировали сложность примеров. Такой подход позволил получить хорошее межпредметное обобщение: обучение в одном домене повышало качество в других, а совместное обучение сразу в нескольких — приводило к ещё большим улучшениям в каждом из них.<br><br>Второе важное нововведение — curriculum sampling: отбор наиболее полезных примеров для обучения. Подготовка выборки (RLCS) и её динамическое расширение реализованы с помощью экспоненциальной скользящей средней (EMA), что стабилизирует траекторию обучения и ускоряет сходимость модели.<br><br>По итогам проверки на 42 публичных бенчмарках GLM‑4.5V обеспечивает высокие результаты почти во всех задачах среди открытых моделей сопоставимого размера и демонстрирует конкурентоспособность по отношению к закрытым решениям.<br><br>Познакомиться с GLM-4.5V можно на <a href="https://github.com/zai-org/" rel="nofollow noopener noreferrer">github</a>.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Данил Кашин<br></em><a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/184_480.webp" srcset="../assets/media/thumbs/184_480.webp 480w, ../assets/media/184.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="184" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/185_480.webp" srcset="../assets/media/thumbs/185_480.webp 480w, ../assets/media/185.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="184" data-image-index="1" /></div></div>
      <div class="actions">
        <span>1 989 просмотров · 20 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/184" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/184.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="183" data-search="что читает команда алайнмента vlm: подборка актуальных статей узнали у инженеров яндекса из команды алайнмента визуально-языковых моделей, какие статьи они читали и обсуждали в последнее время. в сегодняшней подборке: новый способ обучения mae с прогрессивным замораживанием слоёв для видеолатентов без коллапса, как именно теряется сигнал в коннекторах vlm, объединение текста, картинки и звука в одной модели с сильным алайнментом и другое. layerlock: non-collapsing representation learning with progressive freezing в статье предлагается новый способ обучения mae (masked autoencoder) моделей для сжатия видео в латентные векторы на неразмеченных данных. авторы заметили, что слои vit на разной глубине сходятся с разной скоростью, и придумали прогрессивно замораживать по ходу обучения ранние слои, одновременно меняя таргет от восстановления пикселей к всё более глубоким латентным признакам. это решает проблемы с representation collapse, и модель учится хорошо извлекать высокоуровневые фичи из видео. lost in embeddings: information loss in vision-language models авторы исследуют потерю информации в коннекторе — модуле, связывающем модальности в архитектуре современных vlm. в статье предлагают довольно интересные методы выявления этой потери, вплоть до определения конкретных участков изображения. готовых решений нет, но работа помогает лучше понять, как сигнал передаётся от изображения к языковой модели внутри vlm, и подсвечивает информационный bottleneck современных архитектур. qwen3-omni technical report это инженерное чудо и второй подход к объединению всех модальностей (текста, картинки и звука) в семействе qwen. на этот раз модель не уступает эквивалентным по размеру моделям-экспертам в каждой из модальностей. в работе описан пайплайн обучения и процесс объединения модальностей на разных стадиях. примечательно, что стадия алайнмента включает дистилляцию более сильных тестовых моделей из семейства qwen, возможно, с использованием моделей-экспертов в других модальностях. а вот об rl доподлинно известно, что часть ревордов в нём относятся к картиночной модальности, причём в обучении фигурируют, как model-based-, так и verifiable-реворды. mini-o3: scaling up reasoning patterns and interaction turns for visual search в работе предлагают систему, способную решать сложные задачи визуального поиска с помощью многошаговых рассуждений на основе tool calling в виде зума изображения. в отличие от существующих подходов, ограниченных короткими цепочками действий, mini-o3 может выполнять десятки взаимодействий методом проб и ошибок. предложенная стратегия обучения на разнообразных траекториях рассуждений позволяет получить модель, генерирующую длинные цепочки рассуждений и повышающую свою точность с каждым шагом. интересно, что схожая особенность появилась в передовой модели qwen3-vl. basereward: a strong baseline for multimodal reward model в работе исследуется рецепт создания мультимодальных моделей вознаграждения (mrm). путём обширных экспериментов авторы определили оптимальную парадигму обучения, архитектуру, состав и баланс данных, обнаружив, что добавление текстовой информации значительно улучшает оценку мультимодальных задач. в результате исследователи получили модель вознаграждения, превосходящую прочие подходы по ключевым бенчмаркам. cv time что читает команда алайнмента vlm: подборка актуальных статей узнали у инженеров яндекса из команды алайнмента визуально-языковых моделей, какие статьи они читали и обсуждали в последнее время. в сегодняшней подборке: новый способ обучения mae с прогрессивным замораживанием слоёв для видеолатентов без коллапса, как именно теряется сигнал в коннекторах vlm, объединение текста, картинки и звука в одной модели с сильным алайнментом и другое. layerlock: non-collapsing representation learning with progressive freezing в статье предлагается новый способ обучения mae (masked autoencoder) моделей для сжатия видео в латентные векторы на неразмеченных данных. авторы заметили, что слои vit на разной глубине сходятся с разной скоростью, и придумали прогрессивно замораживать по ходу обучения ранние слои, одновременно меняя таргет от восстановления пикселей к всё более глубоким латентным признакам. это решает проблемы с representation collapse, и модель учится хорошо извлекать высокоуровневые фичи из видео. lost in embeddings: information loss in vision-language models авторы исследуют потерю информации в коннекторе — модуле, связывающем модальности в архитектуре современных vlm. в статье предлагают довольно интересные методы выявления этой потери, вплоть до определения конкретных участков изображения. готовых решений нет, но работа помогает лучше понять, как сигнал передаётся от изображения к языковой модели внутри vlm, и подсвечивает информационный bottleneck современных архитектур. qwen3-omni technical report это инженерное чудо и второй подход к объединению всех модальностей (текста, картинки и звука) в семействе qwen. на этот раз модель не уступает эквивалентным по размеру моделям-экспертам в каждой из модальностей. в работе описан пайплайн обучения и процесс объединения модальностей на разных стадиях. примечательно, что стадия алайнмента включает дистилляцию более сильных тестовых моделей из семейства qwen, возможно, с использованием моделей-экспертов в других модальностях. а вот об rl доподлинно известно, что часть ревордов в нём относятся к картиночной модальности, причём в обучении фигурируют, как model-based-, так и verifiable-реворды. mini-o3: scaling up reasoning patterns and interaction turns for visual search в работе предлагают систему, способную решать сложные задачи визуального поиска с помощью многошаговых рассуждений на основе tool calling в виде зума изображения. в отличие от существующих подходов, ограниченных короткими цепочками действий, mini-o3 может выполнять десятки взаимодействий методом проб и ошибок. предложенная стратегия обучения на разнообразных траекториях рассуждений позволяет получить модель, генерирующую длинные цепочки рассуждений и повышающую свою точность с каждым шагом. интересно, что схожая особенность появилась в передовой модели qwen3-vl. basereward: a strong baseline for multimodal reward model в работе исследуется рецепт создания мультимодальных моделей вознаграждения (mrm). путём обширных экспериментов авторы определили оптимальную парадигму обучения, архитектуру, состав и баланс данных, обнаружив, что добавление текстовой информации значительно улучшает оценку мультимодальных задач. в результате исследователи получили модель вознаграждения, превосходящую прочие подходы по ключевым бенчмаркам. cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-09-30T13:05:13+00:00" href="./posts/183.html">2025-09-30 13:05 UTC</a></div>
      </div>
      <div class="post-body"><strong>Что читает команда алайнмента VLM: подборка актуальных статей <br></strong><br>Узнали у инженеров Яндекса из команды алайнмента визуально-языковых моделей, какие статьи они читали и обсуждали в последнее время. В сегодняшней подборке: новый способ обучения MAE с прогрессивным замораживанием слоёв для видеолатентов без коллапса, как именно теряется сигнал в коннекторах VLM, объединение текста, картинки и звука в одной модели с сильным алайнментом и другое.<br><br><a href="http://arxiv.org/abs/2509.10156v1" rel="nofollow noopener noreferrer"><strong>LayerLock: Non-collapsing Representation Learning with Progressive Freezing</strong></a><br><br>В статье предлагается новый способ обучения MAE (Masked AutoEncoder) моделей для сжатия видео в латентные векторы на неразмеченных данных. Авторы заметили, что слои ViT на разной глубине сходятся с разной скоростью, и придумали прогрессивно замораживать по ходу обучения ранние слои, одновременно меняя таргет от восстановления пикселей к всё более глубоким латентным признакам. Это решает проблемы с representation collapse, и модель учится хорошо извлекать высокоуровневые фичи из видео.<br><br><a href="http://arxiv.org/abs/2509.11986v1" rel="nofollow noopener noreferrer"><strong>Lost in Embeddings: Information Loss in Vision-Language Models</strong></a><br><br>Авторы исследуют потерю информации в коннекторе — модуле, связывающем модальности в архитектуре современных VLM. В статье предлагают довольно интересные методы выявления этой потери, вплоть до определения конкретных участков изображения. Готовых решений нет, но работа помогает лучше понять, как сигнал передаётся от изображения к языковой модели внутри VLM, и подсвечивает информационный bottleneck современных архитектур.<br><br><a href="https://arxiv.org/pdf/2509.17765" rel="nofollow noopener noreferrer"><strong>Qwen3-Omni Technical Report<br></strong></a><br>Это инженерное чудо и второй подход к объединению всех модальностей (текста, картинки и звука) в семействе Qwen. На этот раз модель не уступает эквивалентным по размеру моделям-экспертам в каждой из модальностей. В работе описан пайплайн обучения и процесс объединения модальностей на разных стадиях. <br><br>Примечательно, что стадия алайнмента включает дистилляцию более сильных тестовых моделей из семейства Qwen, возможно, с использованием моделей-экспертов в других модальностях. А вот об RL доподлинно известно, что часть ревордов в нём относятся к картиночной модальности, причём в обучении фигурируют, как model-based-, так и verifiable-реворды.<br><br><a href="https://arxiv.org/pdf/2509.07969" rel="nofollow noopener noreferrer"><strong>Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search</strong></a><br><br>В работе предлагают систему, способную решать сложные задачи визуального поиска с помощью многошаговых рассуждений на основе tool calling в виде зума изображения. В отличие от существующих подходов, ограниченных короткими цепочками действий, Mini-o3 может выполнять десятки взаимодействий методом проб и ошибок. Предложенная стратегия обучения на разнообразных траекториях рассуждений позволяет получить модель, генерирующую длинные цепочки рассуждений и повышающую свою точность с каждым шагом. Интересно, что схожая особенность появилась в передовой модели Qwen3-VL.<br><br><a href="https://arxiv.org/pdf/2509.16127" rel="nofollow noopener noreferrer"><strong>BaseReward: A Strong Baseline for Multimodal Reward Model</strong></a><br><br>В работе исследуется рецепт создания мультимодальных моделей вознаграждения (MRM). Путём обширных экспериментов авторы определили оптимальную парадигму обучения, архитектуру, состав и баланс данных, обнаружив, что добавление текстовой информации значительно улучшает оценку мультимодальных задач. В результате исследователи получили модель вознаграждения, превосходящую прочие подходы по ключевым бенчмаркам.<br><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a></div>
      <div class="actions">
        <span>2 426 просмотров · 28 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/183" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/183.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="182" data-search="should vlms be pre-trained with image data? сегодня разбираем статью о том, как лучше организовать претрейн для vlm. архитектурных новшеств здесь нет: модель напоминает стандартные опенсорсные vlm вроде llava. картинка кодируется вижн-энкодером, эмбеддинги прогоняются через несколько mlp-слоёв и подаются вместе с текстовыми эмбеддингами в llm-декодер. главный вопрос статьи: на каком этапе и в каких пропорциях подключать мультимодальные данные, чтобы итоговая модель была сильной и в text-only, и в мультимодальном режимах. разберём три интересных аблейшна, представленных в работе. когда останавливать llm-претрейн обычно берут полностью обученную llm (например, на 3–4t токенов) и затем добавляют мультимодальный претрейн со своим lr-шедулером, который часто начинается с warmup. авторы считают это неэффективным: мы сначала «убиваем» learning rate, а потом снова разгоняем его на мультимодальных данных. исследователи пробуют прервать обучение llm не в самом конце, а на определённом проценте (например, ~80% от шага). дальше продолжают обучение уже на смеси текстовых и мультимодальных данных, сохраняя текущий learning rate. по представленным vlm метрикам и отдельно text-only-числам, такой вариант даёт лучше результаты, чем стратегия «сначала — до конца llm, потом — мультимодальность». соотношение текстовых и мультимодальных данных во многих открытых моделях текстовые и мультимодальные данные миксуют на претрейне vlm, однако аблейшенов не дают. в статье показано, что оптимально брать в претрейн 10–20% мультимодальных данных. это можно объяснить качеством датасета: картинки проще, но сами мультимодальные пары нередко «грязные», особенно в опенсорсе. исходя из практики, мы тоже видим необходимость подбирать соотношение, однако это сильно зависит от качества данных и представленных в них доменов. инструктивность и sft-эпохи в классическом vlm-pretrain нет инструктивности — модели просто описывают картинки. в последнее время часть инструктивных примеров добавляется уже на претрейне, и это работает. у авторов эффект почти незаметен, скорее всего, из-за слабого датасета (устаревшие llava-данные) и малого количества инструктивных данных. ещё одно наблюдение связано с количеством эпох на sft. авторы пишут, что в их случае оптимальны четыре эпохи. при данных среднего качества выводы ограниченные и вряд ли могут быть перенесены на любую модель, однако результат полезный. по нашему же опыту — если данные хорошие, дополнительные эпохи действительно помогают. в целом статья скорее систематизирует наблюдения, чем открывает новое, но её результаты подтверждают, как важно грамотно комбинировать текст и мультимодальность и где именно стоит искать улучшения. разбор подготовил ❣ владислав смирнов cv time should vlms be pre-trained with image data? сегодня разбираем статью о том, как лучше организовать претрейн для vlm. архитектурных новшеств здесь нет: модель напоминает стандартные опенсорсные vlm вроде llava. картинка кодируется вижн-энкодером, эмбеддинги прогоняются через несколько mlp-слоёв и подаются вместе с текстовыми эмбеддингами в llm-декодер. главный вопрос статьи: на каком этапе и в каких пропорциях подключать мультимодальные данные, чтобы итоговая модель была сильной и в text-only, и в мультимодальном режимах. разберём три интересных аблейшна, представленных в работе. когда останавливать llm-претрейн обычно берут полностью обученную llm (например, на 3–4t токенов) и затем добавляют мультимодальный претрейн со своим lr-шедулером, который часто начинается с warmup. авторы считают это неэффективным: мы сначала «убиваем» learning rate, а потом снова разгоняем его на мультимодальных данных. исследователи пробуют прервать обучение llm не в самом конце, а на определённом проценте (например, ~80% от шага). дальше продолжают обучение уже на смеси текстовых и мультимодальных данных, сохраняя текущий learning rate. по представленным vlm метрикам и отдельно text-only-числам, такой вариант даёт лучше результаты, чем стратегия «сначала — до конца llm, потом — мультимодальность». соотношение текстовых и мультимодальных данных во многих открытых моделях текстовые и мультимодальные данные миксуют на претрейне vlm, однако аблейшенов не дают. в статье показано, что оптимально брать в претрейн 10–20% мультимодальных данных. это можно объяснить качеством датасета: картинки проще, но сами мультимодальные пары нередко «грязные», особенно в опенсорсе. исходя из практики, мы тоже видим необходимость подбирать соотношение, однако это сильно зависит от качества данных и представленных в них доменов. инструктивность и sft-эпохи в классическом vlm-pretrain нет инструктивности — модели просто описывают картинки. в последнее время часть инструктивных примеров добавляется уже на претрейне, и это работает. у авторов эффект почти незаметен, скорее всего, из-за слабого датасета (устаревшие llava-данные) и малого количества инструктивных данных. ещё одно наблюдение связано с количеством эпох на sft. авторы пишут, что в их случае оптимальны четыре эпохи. при данных среднего качества выводы ограниченные и вряд ли могут быть перенесены на любую модель, однако результат полезный. по нашему же опыту — если данные хорошие, дополнительные эпохи действительно помогают. в целом статья скорее систематизирует наблюдения, чем открывает новое, но её результаты подтверждают, как важно грамотно комбинировать текст и мультимодальность и где именно стоит искать улучшения. разбор подготовил ❣ владислав смирнов cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-09-25T17:00:01+00:00" href="./posts/182.html">2025-09-25 17:00 UTC</a></div>
      </div>
      <div class="post-body"><strong>Should VLMs be Pre-trained with Image Data?<br></strong><br>Сегодня разбираем <a href="https://arxiv.org/abs/2503.07603" rel="nofollow noopener noreferrer">статью</a> о том, как лучше организовать претрейн для VLM. Архитектурных новшеств здесь нет: модель напоминает стандартные опенсорсные VLM вроде LLaVA. Картинка кодируется вижн-энкодером, эмбеддинги прогоняются через несколько MLP-слоёв и подаются вместе с текстовыми эмбеддингами в LLM-декодер.<br><br>Главный вопрос статьи: на каком этапе и в каких пропорциях подключать мультимодальные данные, чтобы итоговая модель была сильной и в text-only, и в мультимодальном режимах.<br><br>Разберём три интересных аблейшна, представленных в работе.<br><br><strong>Когда останавливать LLM-претрейн</strong><br><br>Обычно берут полностью обученную LLM (например, на 3–4T токенов) и затем добавляют мультимодальный претрейн со своим LR-шедулером, который часто начинается с warmup. Авторы считают это неэффективным: мы сначала «убиваем» learning rate, а потом снова разгоняем его на мультимодальных данных.<br><br>Исследователи пробуют прервать обучение LLM не в самом конце, а на определённом проценте (например, ~80% от шага). Дальше продолжают обучение уже на смеси текстовых и мультимодальных данных, сохраняя текущий learning rate. По представленным VLM метрикам и отдельно text-only-числам, такой вариант даёт лучше результаты, чем стратегия «сначала — до конца LLM, потом — мультимодальность».<br><br><strong>Соотношение текстовых и мультимодальных данных<br><br></strong>Во многих открытых моделях текстовые и мультимодальные данные миксуют на претрейне VLM, однако аблейшенов не дают. В статье показано, что оптимально брать в претрейн 10–20% мультимодальных данных.<br><br>Это можно объяснить качеством датасета: картинки проще, но сами мультимодальные пары нередко «грязные», особенно в опенсорсе. Исходя из практики, мы тоже видим необходимость подбирать соотношение, однако это сильно зависит от качества данных и представленных в них доменов.<br><br><strong>Инструктивность и SFT-эпохи<br></strong><br>В классическом VLM-pretrain нет инструктивности — модели просто описывают картинки. В последнее время часть инструктивных примеров добавляется уже на претрейне, и это работает. У авторов эффект почти незаметен, скорее всего, из-за слабого датасета (устаревшие LLaVA-данные) и малого количества инструктивных данных.<br><br>Ещё одно наблюдение связано с количеством эпох на SFT. Авторы пишут, что в их случае оптимальны четыре эпохи. При данных среднего качества выводы ограниченные и вряд ли могут быть перенесены на любую модель, однако результат полезный. По нашему же опыту — если данные хорошие, дополнительные эпохи действительно помогают.<br><br>В целом статья скорее систематизирует наблюдения, чем открывает новое, но её результаты подтверждают, как важно грамотно комбинировать текст и мультимодальность и где именно стоит искать улучшения.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Владислав Смирнов</em><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a></div>
      <div class="actions">
        <span>2 257 просмотров · 26 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/182" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/182.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="180" data-search="эволюция florence: от генеративных моделей к mllm сегодня разберём сразу две статьи о семействе моделей florence: что такое florence-2 и как авторы использовали её в vlm. florence-2: advancing a unified representation for a variety of vision tasks это cемейство vlm-моделей появилось в 2023 году. по сути, это и была vlm, хотя сам термин тогда ещё не вошёл в широкое употребление. показательно, что в florence-2 авторы сделали ставку не на архитектуру, а на огромный и качественно собранный датасет fld-5b. в основе архитектуры — обычная схема энкодер-декодер-трансформер. разве что схему vlm авторы нарисовали не так, как принято в 2025-м. вся суть статьи в пайплайне обработки данных. авторы сформулировали множество разных задач в формате «текст на входе — текст на выходе». так всю разметку можно условно поделить на три группы: — понимание картинки в целом (classification, captioning, vqa) — семантика; — умение локализовать объект (object detection, segmentation, referring expression comprehension) — геометрия; — поиск и детекция объектов по набору признаков (text grounding) — семантика + геометрия. пайплайн обработки данных, с помощью которого получили обучающий датасет — на первой иллюстрации к посту: 1. первичная аннотация с помощью специализированных моделей (детекторы, ocr, сегментаторы); 2. фильтрация данных той же нейросетью: исправляют ошибки, удаляют ненужные аннотации; 3. итеративный процесс уточнения данных всё той же нейросетью. fld-5b состоит из 5 млн аннотаций, 126 млн изображений, 500 млн текстовых аннотаций, 1,3 млн текстовых аннотаций для локализации объекта на изображении и 3,6 млн текстовых аннотаций для поиска и детекции объектов по набору признаков. как итог, florence-2 умеет делать 10+ задач (ocr, detection, segmentation, caption to phrase grounding и др.) и довольно редко галлюцинирует. однако, в отличие от современных vlm, она не справляется со сложными инстрактами, потому что не училась этому. да и инстракты может принимать небольшие. florence-vl: enhancing vision-language models with generative vision encoder and depth-breadth fusion во второй статье авторы предлагают простую идею — использовать в качестве энкодера в vlm florence-2. причина проста: эта модель явно училась на ocr, детекцию и сегментацию, в отличие от clip/siglip (хотя siglip2 уже училась с next token prediction). заменить image encoder на florence несложно. нужно трижды инферить image encoder — по одному разу для получения признаков с прицелом на ocr, детекцию и сегментацию. дальше фичи конкатенируются и пропускаются через projection (dbfusion), чтобы получить желаемое число каналов. так появилось семейство florence-vl. подробнее — на второй иллюстрации к посту. в результате florence-vl демонстрирует высокую согласованность визуального энкодера и llm, превосходя другие модели по 25 критериям. в том числе в задачах распознавания объектов, понимания семантики, распознавания текста и построения диаграмм. идея интересная, но, как показало время, не прижилась. видимо, из-за того, что при таком подходе растёт число операций для получения фичей. разбор подготовил ❣ егор шестопалов cv time эволюция florence: от генеративных моделей к mllm сегодня разберём сразу две статьи о семействе моделей florence: что такое florence-2 и как авторы использовали её в vlm. florence-2: advancing a unified representation for a variety of vision tasks это cемейство vlm-моделей появилось в 2023 году. по сути, это и была vlm, хотя сам термин тогда ещё не вошёл в широкое употребление. показательно, что в florence-2 авторы сделали ставку не на архитектуру, а на огромный и качественно собранный датасет fld-5b. в основе архитектуры — обычная схема энкодер-декодер-трансформер. разве что схему vlm авторы нарисовали не так, как принято в 2025-м. вся суть статьи в пайплайне обработки данных. авторы сформулировали множество разных задач в формате «текст на входе — текст на выходе». так всю разметку можно условно поделить на три группы: — понимание картинки в целом (classification, captioning, vqa) — семантика; — умение локализовать объект (object detection, segmentation, referring expression comprehension) — геометрия; — поиск и детекция объектов по набору признаков (text grounding) — семантика + геометрия. пайплайн обработки данных, с помощью которого получили обучающий датасет — на первой иллюстрации к посту: 1. первичная аннотация с помощью специализированных моделей (детекторы, ocr, сегментаторы); 2. фильтрация данных той же нейросетью: исправляют ошибки, удаляют ненужные аннотации; 3. итеративный процесс уточнения данных всё той же нейросетью. fld-5b состоит из 5 млн аннотаций, 126 млн изображений, 500 млн текстовых аннотаций, 1,3 млн текстовых аннотаций для локализации объекта на изображении и 3,6 млн текстовых аннотаций для поиска и детекции объектов по набору признаков. как итог, florence-2 умеет делать 10+ задач (ocr, detection, segmentation, caption to phrase grounding и др.) и довольно редко галлюцинирует. однако, в отличие от современных vlm, она не справляется со сложными инстрактами, потому что не училась этому. да и инстракты может принимать небольшие. florence-vl: enhancing vision-language models with generative vision encoder and depth-breadth fusion во второй статье авторы предлагают простую идею — использовать в качестве энкодера в vlm florence-2. причина проста: эта модель явно училась на ocr, детекцию и сегментацию, в отличие от clip/siglip (хотя siglip2 уже училась с next token prediction). заменить image encoder на florence несложно. нужно трижды инферить image encoder — по одному разу для получения признаков с прицелом на ocr, детекцию и сегментацию. дальше фичи конкатенируются и пропускаются через projection (dbfusion), чтобы получить желаемое число каналов. так появилось семейство florence-vl. подробнее — на второй иллюстрации к посту. в результате florence-vl демонстрирует высокую согласованность визуального энкодера и llm, превосходя другие модели по 25 критериям. в том числе в задачах распознавания объектов, понимания семантики, распознавания текста и построения диаграмм. идея интересная, но, как показало время, не прижилась. видимо, из-за того, что при таком подходе растёт число операций для получения фичей. разбор подготовил ❣ егор шестопалов cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-09-18T09:03:44+00:00" href="./posts/180.html">2025-09-18 09:03 UTC</a></div>
      </div>
      <div class="post-body"><strong>Эволюция Florence: от генеративных моделей к MLLM<br></strong><br>Сегодня разберём сразу две статьи о семействе моделей Florence: что такое Florence-2 и как авторы использовали её в VLM.<br><br><a href="https://arxiv.org/abs/2311.06242" rel="nofollow noopener noreferrer"><strong>Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks</strong></a> <br><br>Это cемейство VLM-моделей появилось в 2023 году. По сути, это и была VLM, хотя сам термин тогда ещё не вошёл в широкое употребление. Показательно, что в Florence-2 авторы сделали ставку не на архитектуру, а на огромный и качественно собранный датасет FLD-5B.<br><br>В основе архитектуры — обычная схема энкодер-декодер-трансформер. Разве что схему VLM авторы нарисовали не так, как принято в 2025-м.  <br><br>Вся суть статьи в пайплайне обработки данных. Авторы сформулировали множество разных задач в формате «текст на входе — текст на выходе». Так всю разметку можно условно поделить на три группы: <br><br>— понимание картинки в целом (classification, captioning, VQA) — семантика;<br>— умение локализовать объект (object detection, segmentation, referring expression comprehension) — геометрия; <br>— поиск и детекция объектов по набору признаков (text grounding) — семантика + геометрия.<br><br>Пайплайн обработки данных, с помощью которого получили обучающий датасет — на первой иллюстрации к посту: <br><br>1. первичная аннотация с помощью специализированных моделей (детекторы, OCR, сегментаторы);<br>2. фильтрация данных той же нейросетью: исправляют ошибки, удаляют ненужные аннотации;<br>3. итеративный процесс уточнения данных всё той же нейросетью.<br><br>FLD-5B состоит из 5 млн аннотаций, 126 млн изображений, 500 млн текстовых аннотаций, 1,3 млн текстовых аннотаций для локализации объекта на изображении и 3,6 млн текстовых аннотаций для поиска и детекции объектов по набору признаков. <br><br>Как итог, Florence-2 умеет делать 10+ задач (OCR, detection, segmentation, Caption to Phrase Grounding и др.) и довольно редко галлюцинирует. Однако, в отличие от современных VLM, она не справляется со сложными инстрактами, потому что не училась этому. Да и инстракты может принимать небольшие.<br><br><a href="https://arxiv.org/abs/2412.04424" rel="nofollow noopener noreferrer"><strong>Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion<br></strong></a><br>Во второй статье авторы предлагают простую идею — использовать в качестве энкодера в VLM Florence-2. Причина проста: эта модель явно училась на OCR, детекцию и сегментацию, в отличие от CLIP/SigLIP (хотя SigLIP2 уже училась с next token prediction).<br><br>Заменить Image Encoder на Florence несложно. Нужно трижды инферить Image Encoder — по одному разу для получения признаков с прицелом на OCR, детекцию и сегментацию. Дальше фичи конкатенируются и пропускаются через projection (DBFusion), чтобы получить желаемое число каналов. Так появилось семейство Florence-VL. Подробнее — на второй иллюстрации к посту. <br><br>В результате Florence-VL демонстрирует высокую согласованность визуального энкодера и LLM, превосходя другие модели по 25 критериям. В том числе в задачах распознавания объектов, понимания семантики, распознавания текста и построения диаграмм. <br><br>Идея интересная, но, как показало время, не прижилась. Видимо, из-за того, что при таком подходе растёт число операций для получения фичей.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> <em>Егор Шестопалов</em><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/180_480.webp" srcset="../assets/media/thumbs/180_480.webp 480w, ../assets/media/180.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="180" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/181_480.webp" srcset="../assets/media/thumbs/181_480.webp 480w, ../assets/media/181.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="180" data-image-index="1" /></div></div>
      <div class="actions">
        <span>11 462 просмотров · 25 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/180" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/180.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="179" data-search="scale-wise distillation of diffusion models сегодня разбираем статью от исследователей из yandex research, появившуюся на arxiv.org в марте 2025 года. авторы предложили метод дистилляции scale-wise distillation (swd), при котором диффузионная модель не сразу генерирует изображение в полном разрешении, а постепенно повышает его на каждом шаге. такой подход позволяет ускорить процесс генерации более чем в два раза по сравнению с обычной дистилляцией. диффузия на данный момент — ведущая парадигма в области генерации изображений. но, к сожалению, генерация даже одной картинки может быть довольно долгой. причина: нужно делать много шагов, каждый из которых считается в фиксированном конечном разрешении и вычислительно затратен. проблему попытались решить с помощью scale-wise-генерации: стартовать с одного пикселя и постепенно повышать разрешение, приходя к результату за несколько шагов. тогда первые шаги идут в низком разрешении и стоят очень дёшево — затраты растут по мере увеличения размера изображения. эта парадигма реализована в var (visual autoregressive transformer), но кроме scale-wise-генерации, там используется представление изображения в виде дискретных токенов и авторегрессия. однако дискретное представление изображений приводит к неустранимым ошибкам в представлении картинок и ограничивает максимально достижимое качество. отсюда возникла идея вытащить из var scale-wise-генерацию и поместить её во фреймворк, сочетающий лучшие стороны обеих парадигм (var и диффузии). метод обучения swd-подхода основан на известных процедурах дистилляции диффузионных моделей. но дистилляция в этом случае позволяет не только уменьшить число шагов генерации, но ещё и генерировать при меньших разрешениях. интуиция авторов исходит из анализа диффузионного процесса в фурье-пространстве. у естественных картинок амплитуды частот убывают с ростом частоты, а у гауссова шума спектр плоский. когда мы добавляем шум, высокочастотные компоненты изображения маскируются — сначала самые тонкие, потом всё больше. в итоге на ранних шагах модели остаются только низкие частоты, а детали всё равно «съедаются» шумом. это объясняет, почему диффузия хорошо подходит для генерации изображений: она восстанавливает сигнал от грубых низкочастотных структур к высоким частотам и деталям. однако становится очевидно, что на начальных этапах нет смысла использовать полное разрешение — всё, что модель посчитает, будет уничтожено шумом. есть важные нюансы: — если напрямую увеличивать разрешение шумных латентных представлений, возникает много артефактов, и качество изображения значительно ухудшается. поэтому лучше сначала увеличить разрешение чистой картинки в низком разрешении, а затем добавить шум; — важно подобрать такие шаги, чтобы уровень шума подавлял артефакты увеличения разрешения. расписание шумов имеет критическое значение: в отличие от базовой дистилляции с равномерным расписанием, здесь его следует сдвинуть в сторону более высокого уровня шума, чтобы «погасить» дефекты увеличения разрешения; — «перезашумить» — не так страшно, как «недозашумить». если шума будет меньше, чем требует текущий шаг, качество сильно упадёт, и на финальных картинках появятся артефакты. обучение строится на парах соседних разрешений. исходное изображение уменьшают до меньшего и до целевого размера. малоразмерное изображение увеличивают, добавляют шум в соответствии с шагом t и подают в генератор, который предсказывает изображение в целевом разрешении. функция потерь основана на сопоставлении распределения между предсказанием и целевым изображением (distribution matching). отдельно важно, что модель учится на синтетике учителя. предобученной диффузией генерируют много картинок на основе некоторой выборки пользовательских запросов. такой подход даёт заметный прирост качества по сравнению с обучением на реальных картинках. разбор подготовил ❣ денис кузнеделев cv time scale-wise distillation of diffusion models сегодня разбираем статью от исследователей из yandex research, появившуюся на arxiv.org в марте 2025 года. авторы предложили метод дистилляции scale-wise distillation (swd), при котором диффузионная модель не сразу генерирует изображение в полном разрешении, а постепенно повышает его на каждом шаге. такой подход позволяет ускорить процесс генерации более чем в два раза по сравнению с обычной дистилляцией. диффузия на данный момент — ведущая парадигма в области генерации изображений. но, к сожалению, генерация даже одной картинки может быть довольно долгой. причина: нужно делать много шагов, каждый из которых считается в фиксированном конечном разрешении и вычислительно затратен. проблему попытались решить с помощью scale-wise-генерации: стартовать с одного пикселя и постепенно повышать разрешение, приходя к результату за несколько шагов. тогда первые шаги идут в низком разрешении и стоят очень дёшево — затраты растут по мере увеличения размера изображения. эта парадигма реализована в var (visual autoregressive transformer), но кроме scale-wise-генерации, там используется представление изображения в виде дискретных токенов и авторегрессия. однако дискретное представление изображений приводит к неустранимым ошибкам в представлении картинок и ограничивает максимально достижимое качество. отсюда возникла идея вытащить из var scale-wise-генерацию и поместить её во фреймворк, сочетающий лучшие стороны обеих парадигм (var и диффузии). метод обучения swd-подхода основан на известных процедурах дистилляции диффузионных моделей. но дистилляция в этом случае позволяет не только уменьшить число шагов генерации, но ещё и генерировать при меньших разрешениях. интуиция авторов исходит из анализа диффузионного процесса в фурье-пространстве. у естественных картинок амплитуды частот убывают с ростом частоты, а у гауссова шума спектр плоский. когда мы добавляем шум, высокочастотные компоненты изображения маскируются — сначала самые тонкие, потом всё больше. в итоге на ранних шагах модели остаются только низкие частоты, а детали всё равно «съедаются» шумом. это объясняет, почему диффузия хорошо подходит для генерации изображений: она восстанавливает сигнал от грубых низкочастотных структур к высоким частотам и деталям. однако становится очевидно, что на начальных этапах нет смысла использовать полное разрешение — всё, что модель посчитает, будет уничтожено шумом. есть важные нюансы: — если напрямую увеличивать разрешение шумных латентных представлений, возникает много артефактов, и качество изображения значительно ухудшается. поэтому лучше сначала увеличить разрешение чистой картинки в низком разрешении, а затем добавить шум; — важно подобрать такие шаги, чтобы уровень шума подавлял артефакты увеличения разрешения. расписание шумов имеет критическое значение: в отличие от базовой дистилляции с равномерным расписанием, здесь его следует сдвинуть в сторону более высокого уровня шума, чтобы «погасить» дефекты увеличения разрешения; — «перезашумить» — не так страшно, как «недозашумить». если шума будет меньше, чем требует текущий шаг, качество сильно упадёт, и на финальных картинках появятся артефакты. обучение строится на парах соседних разрешений. исходное изображение уменьшают до меньшего и до целевого размера. малоразмерное изображение увеличивают, добавляют шум в соответствии с шагом t и подают в генератор, который предсказывает изображение в целевом разрешении. функция потерь основана на сопоставлении распределения между предсказанием и целевым изображением (distribution matching). отдельно важно, что модель учится на синтетике учителя. предобученной диффузией генерируют много картинок на основе некоторой выборки пользовательских запросов. такой подход даёт заметный прирост качества по сравнению с обучением на реальных картинках. разбор подготовил ❣ денис кузнеделев cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-09-11T09:04:32+00:00" href="./posts/179.html">2025-09-11 09:04 UTC</a></div>
      </div>
      <div class="post-body"><strong>Scale-wise Distillation of Diffusion Models<br></strong><br>Сегодня разбираем <a href="https://arxiv.org/html/2503.16397v1" rel="nofollow noopener noreferrer">статью</a> от исследователей из Yandex Research, появившуюся на <a href="http://arXiv.org/" rel="nofollow noopener noreferrer">arXiv.org</a> в марте 2025 года. Авторы предложили метод дистилляции Scale-wise Distillation (SwD), при котором диффузионная модель не сразу генерирует изображение в полном разрешении, а постепенно повышает его на каждом шаге. Такой подход позволяет ускорить процесс генерации более чем в два раза по сравнению с обычной дистилляцией. <br><br>Диффузия на данный момент — ведущая парадигма в области генерации изображений. Но, к сожалению, генерация даже одной картинки может быть довольно долгой. Причина: нужно делать много шагов, каждый из которых считается в фиксированном конечном разрешении и вычислительно затратен.<br><br>Проблему попытались решить с помощью scale-wise-генерации: стартовать с одного пикселя и постепенно повышать разрешение, приходя к результату за несколько шагов. Тогда первые шаги идут в низком разрешении и стоят очень дёшево — затраты растут по мере увеличения размера изображения.<br><br>Эта парадигма реализована в <a href="https://arxiv.org/html/2502.06167v1" rel="nofollow noopener noreferrer">VAR</a> (Visual Autoregressive Transformer), но кроме scale-wise-генерации, там используется представление изображения в виде дискретных токенов и авторегрессия. Однако дискретное представление изображений приводит к неустранимым ошибкам в представлении картинок и ограничивает максимально достижимое качество. <br><br>Отсюда возникла идея вытащить из VAR scale-wise-генерацию и поместить её во фреймворк, сочетающий лучшие стороны обеих парадигм (VAR и диффузии). Метод обучения SwD-подхода основан на известных процедурах дистилляции диффузионных моделей. Но дистилляция в этом случае позволяет не только уменьшить число шагов генерации, но ещё и генерировать при меньших разрешениях.<br><br>Интуиция авторов исходит из анализа диффузионного процесса в фурье-пространстве. У естественных картинок амплитуды частот убывают с ростом частоты, а у гауссова шума спектр плоский. Когда мы добавляем шум, высокочастотные компоненты изображения маскируются — сначала самые тонкие, потом всё больше. В итоге на ранних шагах модели остаются только низкие частоты, а детали всё равно «съедаются» шумом.<br><br>Это объясняет, почему диффузия хорошо подходит для генерации изображений: она восстанавливает сигнал от грубых низкочастотных структур к высоким частотам и деталям. Однако становится очевидно, что на начальных этапах нет смысла использовать полное разрешение — всё, что модель посчитает, будет уничтожено шумом.<br><br>Есть важные нюансы:<br><br>— если напрямую увеличивать разрешение шумных латентных представлений, возникает много артефактов, и качество изображения значительно ухудшается. Поэтому лучше сначала увеличить разрешение чистой картинки в низком разрешении, а затем добавить шум;<br><br>— важно подобрать такие шаги, чтобы уровень шума подавлял артефакты увеличения разрешения. Расписание шумов имеет критическое значение: в отличие от базовой дистилляции с равномерным расписанием, здесь его следует сдвинуть в сторону более высокого уровня шума, чтобы «погасить» дефекты увеличения разрешения;<br><br>— «перезашумить» — не так страшно, как «недозашумить». Если шума будет меньше, чем требует текущий шаг, качество сильно упадёт, и на финальных картинках появятся артефакты.<br><br>Обучение строится на парах соседних разрешений. Исходное изображение уменьшают до меньшего и до целевого размера. Малоразмерное изображение увеличивают, добавляют шум в соответствии с шагом t и подают в генератор, который предсказывает изображение в целевом разрешении. Функция потерь основана на сопоставлении распределения между предсказанием и целевым изображением (distribution matching).<br><br>Отдельно важно, что модель учится на синтетике учителя. Предобученной диффузией генерируют много картинок на основе некоторой выборки пользовательских запросов. Такой подход даёт заметный прирост качества по сравнению с обучением на реальных картинках.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> <em>Денис Кузнеделев</em><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/179_480.webp" srcset="../assets/media/thumbs/179_480.webp 480w, ../assets/media/179.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="179" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 301 просмотров · 30 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/179" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/179.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="176" data-search="подборка статей о peft в vlm сегодня у нас краткий обзор peft (parameter-efficient fine-tuning) в визуальных моделях. разберём три подхода и ключевые статьи в каждом из них. аддитивные методы ﻿﻿adaptformer базовый метод в этом классе, который фактически копирует адаптер-тюнинг из llm. подразумевает добавление адаптер-блока с понижением, нелинейным преобразованием и повышением размерности. обычно адаптер-блоки последовательно добавляют к feed-forward-слоям, а авторы подключают их параллельно — при этом адаптер складывается с результатом feed-forward-слоя с некоторым весом. этот вес задаётся как гиперпараметр. в llm его обычно берут больше единицы (например, 4), а для vit у авторов лучший результат получился при 0,1. в статье утверждают, что этот метод, применённый к vlm, даёт более высокие результаты по сравнению с prompt tuning, а иногда и с full tuning. ﻿﻿vit-adapter авторы исходят из того, что cnn лучше извлекают пространственные признаки, поэтому добавляют в vit адаптер, который объединяет cnn и vit. основные компоненты адаптера: — spatial prior module — cnn на основе stem из resnet (свёртки 3×3 со stride=2 и свёртка 1×1), которая проецирует карты признаков в размерность d. на выходе получается пирамида {f1, f2, f3} из d-мерных карт с разрешениями 1/8, 1/16 и 1/32 от исходного. эти карты разворачиваются и конкатенируются в один вектор. — spatial feature injector — компонент, состоящий из n блоков, где i-й блок добавляет пространственную информацию в i-й блок vit с помощью слоя cross-attention. — spatial feature extractor — компонент, состоящий из n блоков, где в i-й блок добавляют многоуровневые признаки из i-го блок vit с помощью: слоя cross-attention, ffn-слоя и skip connection с результатом i-го блока инъектора. side tuning lst: ladder side-tuning side-tuning впервые предложили в lst. идея в том, что адаптеры и prompt-tuning уменьшают число обучаемых параметров, но не решают проблему памяти, так как требуют полного распространения градиента. в side-tuning выходы адаптеров в исходную архитектуру не попадают напрямую, что экономит ресурсы. реализация: — добавляют несколько блоков-адаптеров, которые представляют собой маленькие трансформеры; — с каждого трансформерного блока основной модели выход подают на соответствующий адаптер через линейное сжатие размерности. при такой подаче выход трансформерного блока суммируется с результатом предыдущего блока адаптера; — суммирование происходит с помощью gate-механизма (обычный обучаемый гейт); — метод можно применять как к декодеру, так и к энкодер-декодер-архитектурам. в vilt-5 авторы использовали его только на уровне энкодеров-декодеров llm, но не в самом vit, так как там выход напрямую передаётся в адаптер для перевода визуальных токенов в языковые. эксперименты показали, что использование классических адаптеров вместо трансформерных блоков ухудшает качество, как и замена gate на cross-attention. для инициализации маленьких трансформеров применяли pruning с матрицей информации фишера. prompt-like-методы ﻿﻿visual prompt tuning метод — буквально обычный ptune, добавленный в сам vit. сравнивали, куда именно добавлять промпты: базовый вариант даёт результат не хуже остальных. аналогично проверяли, куда подключать «классификационную голову» на выходе vit, и снова базовый вариант оказался не хуже. есть несколько вариаций: добавление промптов только в первый слой или deep visual prompt tuning — обучаемые векторы для каждого блока. coop: context optimization метод, сделанный для clip в задачах классификации. вместо ручного промпта используют обучаемые векторы. в отличие от ptune, текстовый промпт тут убирается полностью. метод сам по себе тривиальный, но стал базой для других подходов (например, clip-adapter). разбор подготовил ❣ александр мандров cv time подборка статей о peft в vlm сегодня у нас краткий обзор peft (parameter-efficient fine-tuning) в визуальных моделях. разберём три подхода и ключевые статьи в каждом из них. аддитивные методы ﻿﻿ adaptformer базовый метод в этом классе, который фактически копирует адаптер-тюнинг из llm. подразумевает добавление адаптер-блока с понижением, нелинейным преобразованием и повышением размерности. обычно адаптер-блоки последовательно добавляют к feed-forward-слоям, а авторы подключают их параллельно — при этом адаптер складывается с результатом feed-forward-слоя с некоторым весом. этот вес задаётся как гиперпараметр. в llm его обычно берут больше единицы (например, 4), а для vit у авторов лучший результат получился при 0,1. в статье утверждают, что этот метод, применённый к vlm, даёт более высокие результаты по сравнению с prompt tuning, а иногда и с full tuning. ﻿﻿ vit-adapter авторы исходят из того, что cnn лучше извлекают пространственные признаки, поэтому добавляют в vit адаптер, который объединяет cnn и vit. основные компоненты адаптера: — spatial prior module — cnn на основе stem из resnet (свёртки 3×3 со stride=2 и свёртка 1×1), которая проецирует карты признаков в размерность d. на выходе получается пирамида {f1, f2, f3} из d-мерных карт с разрешениями 1/8, 1/16 и 1/32 от исходного. эти карты разворачиваются и конкатенируются в один вектор. — spatial feature injector — компонент, состоящий из n блоков, где i-й блок добавляет пространственную информацию в i-й блок vit с помощью слоя cross-attention. — spatial feature extractor — компонент, состоящий из n блоков, где в i-й блок добавляют многоуровневые признаки из i-го блок vit с помощью: слоя cross-attention, ffn-слоя и skip connection с результатом i-го блока инъектора. side tuning lst: ladder side-tuning side-tuning впервые предложили в lst. идея в том, что адаптеры и prompt-tuning уменьшают число обучаемых параметров, но не решают проблему памяти, так как требуют полного распространения градиента. в side-tuning выходы адаптеров в исходную архитектуру не попадают напрямую, что экономит ресурсы. реализация: — добавляют несколько блоков-адаптеров, которые представляют собой маленькие трансформеры; — с каждого трансформерного блока основной модели выход подают на соответствующий адаптер через линейное сжатие размерности. при такой подаче выход трансформерного блока суммируется с результатом предыдущего блока адаптера; — суммирование происходит с помощью gate-механизма (обычный обучаемый гейт); — метод можно применять как к декодеру, так и к энкодер-декодер-архитектурам. в vilt-5 авторы использовали его только на уровне энкодеров-декодеров llm, но не в самом vit, так как там выход напрямую передаётся в адаптер для перевода визуальных токенов в языковые. эксперименты показали, что использование классических адаптеров вместо трансформерных блоков ухудшает качество, как и замена gate на cross-attention. для инициализации маленьких трансформеров применяли pruning с матрицей информации фишера. prompt-like-методы ﻿﻿visual prompt tuning метод — буквально обычный ptune, добавленный в сам vit. сравнивали, куда именно добавлять промпты: базовый вариант даёт результат не хуже остальных. аналогично проверяли, куда подключать «классификационную голову» на выходе vit, и снова базовый вариант оказался не хуже. есть несколько вариаций: добавление промптов только в первый слой или deep visual prompt tuning — обучаемые векторы для каждого блока. coop: context optimization метод, сделанный для clip в задачах классификации. вместо ручного промпта используют обучаемые векторы. в отличие от ptune, текстовый промпт тут убирается полностью. метод сам по себе тривиальный, но стал базой для других подходов (например, clip-adapter). разбор подготовил ❣ александр мандров cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-09-03T09:06:07+00:00" href="./posts/176.html">2025-09-03 09:06 UTC</a></div>
      </div>
      <div class="post-body"><strong>Подборка статей о PEFT в VLM <br></strong><br>Сегодня у нас краткий обзор PEFT (Parameter-Efficient Fine-Tuning) в визуальных моделях. Разберём три подхода и ключевые статьи в каждом из них. <br><br><strong>Аддитивные методы</strong><br><strong><br></strong>﻿﻿<a href="https://arxiv.org/pdf/2205.13535" rel="nofollow noopener noreferrer">AdaptFormer</a><br><a href="https://arxiv.org/pdf/2205.13535" rel="nofollow noopener noreferrer"><br></a>Базовый метод в этом классе, который фактически копирует адаптер-тюнинг из LLM. Подразумевает добавление адаптер-блока с понижением, нелинейным преобразованием и повышением размерности. <br><br>Обычно адаптер-блоки последовательно добавляют к feed-forward-слоям, а авторы подключают их параллельно — при этом адаптер складывается с результатом feed-forward-слоя с некоторым весом. Этот вес задаётся как гиперпараметр. В LLM его обычно берут больше единицы (например, 4), а для ViT у авторов лучший результат получился при 0,1. <br><br>В статье утверждают, что этот метод, применённый к VLM, даёт более высокие результаты по сравнению с prompt tuning, а иногда и с full tuning.<br><br>﻿﻿<a href="https://arxiv.org/abs/2205.08534" rel="nofollow noopener noreferrer">ViT-Adapter</a><br><br>Авторы исходят из того, что CNN лучше извлекают пространственные признаки, поэтому добавляют в ViT адаптер, который объединяет CNN и ViT. Основные компоненты адаптера:<br><br>— Spatial prior module — CNN на основе Stem из ResNet (свёртки 3×3 со stride=2 и свёртка 1×1), которая проецирует карты признаков в размерность D. На выходе получается пирамида {F1, F2, F3} из D-мерных карт с разрешениями 1/8, 1/16 и 1/32 от исходного. Эти карты разворачиваются и конкатенируются в один вектор.<br><br>— Spatial Feature Injector — компонент, состоящий из n блоков, где i-й блок добавляет пространственную информацию в i-й блок ViT с помощью слоя cross-attention.<br><br>— Spatial Feature Extractor — компонент, состоящий из n блоков, где в i-й блок добавляют многоуровневые  признаки из i-го блок ViT с помощью: слоя cross-attention, FFN-слоя и skip connection с результатом i-го блока инъектора.<br><br><strong>Side Tuning</strong><br><br><a href="https://arxiv.org/abs/2206.06522" rel="nofollow noopener noreferrer">LST: Ladder Side-Tuning </a><br><br>Side-tuning впервые предложили в LST. Идея в том, что адаптеры и prompt-tuning уменьшают число обучаемых параметров, но не решают проблему памяти, так как требуют полного распространения градиента. В side-tuning выходы адаптеров в исходную архитектуру не попадают напрямую, что экономит ресурсы.<br><br>Реализация:<br>— добавляют несколько блоков-адаптеров, которые представляют собой маленькие трансформеры;<br>— с каждого трансформерного блока основной модели выход подают на соответствующий адаптер через линейное сжатие размерности. При такой подаче выход трансформерного блока суммируется с результатом предыдущего блока адаптера;<br>— суммирование происходит с помощью gate-механизма (обычный обучаемый гейт);<br>— метод можно применять как к декодеру, так и к энкодер-декодер-архитектурам. В ViLT-5 авторы использовали его только на уровне энкодеров-декодеров LLM, но не в самом ViT, так как там выход напрямую передаётся в адаптер для перевода визуальных токенов в языковые.<br><br>Эксперименты показали, что использование классических адаптеров вместо трансформерных блоков ухудшает качество, как и замена gate на cross-attention. Для инициализации маленьких трансформеров применяли pruning с матрицей информации Фишера.<br><br><strong>Prompt-like-методы<br></strong><br><a href="https://arxiv.org/pdf/2203.12119" rel="nofollow noopener noreferrer">﻿﻿Visual prompt tuning<br></a><br>Метод — буквально обычный Ptune, добавленный в сам ViT. Сравнивали, куда именно добавлять промпты: базовый вариант даёт результат не хуже остальных. Аналогично проверяли, куда подключать «классификационную голову» на выходе ViT, и снова базовый вариант оказался не хуже. Есть несколько вариаций: добавление промптов только в первый слой или deep visual prompt tuning — обучаемые векторы для каждого блока. <br><br><a href="https://arxiv.org/abs/2109.01134" rel="nofollow noopener noreferrer">CoOp: Context Optimization</a><br><a href="https://arxiv.org/abs/2109.01134" rel="nofollow noopener noreferrer"><br></a>Метод, сделанный для CLIP в задачах классификации. Вместо ручного промпта используют обучаемые векторы. В отличие от Ptune, текстовый промпт тут убирается полностью. Метод сам по себе тривиальный, но стал базой для других подходов (например, CLIP-Adapter). <br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em>  Александр Мандров<br></em><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/176_480.webp" srcset="../assets/media/thumbs/176_480.webp 480w, ../assets/media/176.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="176" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/177_480.webp" srcset="../assets/media/thumbs/177_480.webp 480w, ../assets/media/177.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="176" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/178_480.webp" srcset="../assets/media/thumbs/178_480.webp 480w, ../assets/media/178.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="176" data-image-index="2" /></div></div>
      <div class="actions">
        <span>2 076 просмотров · 23 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/176" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/176.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    </div>
    
    <div class="pager static-pager" style="justify-content:center">
      <div class="page-links">
        <a class="nav-link disabled" href="#">←</a>
        <a class="page-link current" href="index.html">1</a> <a class="page-link" href="page-2.html">2</a> <a class="page-link" href="page-3.html">3</a> <a class="page-link" href="page-4.html">4</a>
        <a class="nav-link" href="page-2.html">→</a>
      </div>
    </div>
    
  </main>

  <footer class="footer">
    <div class="container">
      <div class="footer-inner">
        <span>based on <a href="https://github.com/ml-brand/tg-to-gh-pages" target="_blank" rel="noopener">tg-to-gh-pages</a> (created by <a href="https://github.com/ml-brand" target="_blank" rel="noopener">ML Brand</a>)</span>
        <a id="repoLink" href="https://github.com/ml-brand/tg-to-gh-pages" target="_blank" rel="noopener">Do the same with your channel.</a>
        <span class="footer-links">
          static copy ·
          <a href="../feed.xml" target="_blank" rel="noopener">RSS</a> ·
          <a href="../atom.xml" target="_blank" rel="noopener">Atom</a>
        </span>
      </div>
    </div>
  </footer>

  <script>
    window.__STATIC_POSTS = [{"id": 238, "media": [{"kind": "photo", "path": "../assets/media/238.jpg", "thumb": "../assets/media/thumbs/238_480.webp", "size": 58819, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/239.jpg", "thumb": "../assets/media/thumbs/239_480.webp", "size": 59517, "mime": "image/jpeg", "name": null}]}, {"id": 237, "media": [{"kind": "photo", "path": "../assets/media/237.jpg", "thumb": "../assets/media/thumbs/237_480.webp", "size": 78205, "mime": "image/jpeg", "name": null}]}, {"id": 236, "media": [{"kind": "photo", "path": "../assets/media/236.jpg", "thumb": "../assets/media/thumbs/236_480.webp", "size": 64524, "mime": "image/jpeg", "name": null}]}, {"id": 235, "media": [{"kind": "photo", "path": "../assets/media/235.jpg", "thumb": "../assets/media/thumbs/235_480.webp", "size": 88133, "mime": "image/jpeg", "name": null}]}, {"id": 230, "media": [{"kind": "photo", "path": "../assets/media/230.jpg", "thumb": "../assets/media/thumbs/230_480.webp", "size": 54313, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/231.jpg", "thumb": "../assets/media/thumbs/231_480.webp", "size": 45328, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/232.jpg", "thumb": "../assets/media/thumbs/232_480.webp", "size": 73413, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/233.jpg", "thumb": "../assets/media/thumbs/233_480.webp", "size": 56629, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/234.jpg", "thumb": "../assets/media/thumbs/234_480.webp", "size": 59858, "mime": "image/jpeg", "name": null}]}, {"id": 229, "media": []}, {"id": 228, "media": []}, {"id": 227, "media": []}, {"id": 226, "media": []}, {"id": 225, "media": [{"kind": "photo", "path": "../assets/media/225.jpg", "thumb": "../assets/media/thumbs/225_480.webp", "size": 47634, "mime": "image/jpeg", "name": null}]}, {"id": 221, "media": [{"kind": "photo", "path": "../assets/media/221.jpg", "thumb": "../assets/media/thumbs/221_480.webp", "size": 88449, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/222.jpg", "thumb": "../assets/media/thumbs/222_480.webp", "size": 108982, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/223.jpg", "thumb": "../assets/media/thumbs/223_480.webp", "size": 45297, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/224.jpg", "thumb": "../assets/media/thumbs/224_480.webp", "size": 68299, "mime": "image/jpeg", "name": null}]}, {"id": 220, "media": []}, {"id": 213, "media": [{"kind": "photo", "path": "../assets/media/213.jpg", "thumb": "../assets/media/thumbs/213_480.webp", "size": 141772, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/214.jpg", "thumb": "../assets/media/thumbs/214_480.webp", "size": 100079, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/215.jpg", "thumb": "../assets/media/thumbs/215_480.webp", "size": 116158, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/216.jpg", "thumb": "../assets/media/thumbs/216_480.webp", "size": 118314, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/217.jpg", "thumb": "../assets/media/thumbs/217_480.webp", "size": 107892, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/218.jpg", "thumb": "../assets/media/thumbs/218_480.webp", "size": 113354, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/219.jpg", "thumb": "../assets/media/thumbs/219_480.webp", "size": 133328, "mime": "image/jpeg", "name": null}]}, {"id": 203, "media": [{"kind": "photo", "path": "../assets/media/203.jpg", "thumb": "../assets/media/thumbs/203_480.webp", "size": 92671, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/204.jpg", "thumb": "../assets/media/thumbs/204_480.webp", "size": 101131, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/205.jpg", "thumb": "../assets/media/thumbs/205_480.webp", "size": 127595, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/206.jpg", "thumb": "../assets/media/thumbs/206_480.webp", "size": 85216, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/207.jpg", "thumb": "../assets/media/thumbs/207_480.webp", "size": 117393, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/208.jpg", "thumb": "../assets/media/thumbs/208_480.webp", "size": 88380, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/209.jpg", "thumb": "../assets/media/thumbs/209_480.webp", "size": 96877, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/210.jpg", "thumb": "../assets/media/thumbs/210_480.webp", "size": 114214, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/211.jpg", "thumb": "../assets/media/thumbs/211_480.webp", "size": 105441, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/212.jpg", "thumb": "../assets/media/thumbs/212_480.webp", "size": 114014, "mime": "image/jpeg", "name": null}]}, {"id": 195, "media": [{"kind": "photo", "path": "../assets/media/195.jpg", "thumb": "../assets/media/thumbs/195_480.webp", "size": 106022, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/196.jpg", "thumb": "../assets/media/thumbs/196_480.webp", "size": 80551, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/197.jpg", "thumb": "../assets/media/thumbs/197_480.webp", "size": 73777, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/198.jpg", "thumb": "../assets/media/thumbs/198_480.webp", "size": 69597, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/199.jpg", "thumb": "../assets/media/thumbs/199_480.webp", "size": 120976, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/200.jpg", "thumb": "../assets/media/thumbs/200_480.webp", "size": 74039, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/201.jpg", "thumb": "../assets/media/thumbs/201_480.webp", "size": 90234, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/202.jpg", "thumb": "../assets/media/thumbs/202_480.webp", "size": 96401, "mime": "image/jpeg", "name": null}]}, {"id": 194, "media": [{"kind": "photo", "path": "../assets/media/194.jpg", "thumb": "../assets/media/thumbs/194_480.webp", "size": 68830, "mime": "image/jpeg", "name": null}]}, {"id": 193, "media": [{"kind": "photo", "path": "../assets/media/193.jpg", "thumb": "../assets/media/thumbs/193_480.webp", "size": 34829, "mime": "image/jpeg", "name": null}]}, {"id": 192, "media": [{"kind": "photo", "path": "../assets/media/192.jpg", "thumb": "../assets/media/thumbs/192_480.webp", "size": 88835, "mime": "image/jpeg", "name": null}]}, {"id": 191, "media": []}, {"id": 190, "media": [{"kind": "photo", "path": "../assets/media/190.jpg", "thumb": "../assets/media/thumbs/190_480.webp", "size": 115861, "mime": "image/jpeg", "name": null}]}, {"id": 189, "media": [{"kind": "video", "path": "../assets/media/189_fish.mp4", "thumb": null, "size": 10144880, "mime": "video/mp4", "name": "fish.mp4"}]}, {"id": 188, "media": []}, {"id": 187, "media": [{"kind": "photo", "path": "../assets/media/187.jpg", "thumb": "../assets/media/thumbs/187_480.webp", "size": 109952, "mime": "image/jpeg", "name": null}]}, {"id": 186, "media": []}, {"id": 184, "media": [{"kind": "photo", "path": "../assets/media/184.jpg", "thumb": "../assets/media/thumbs/184_480.webp", "size": 95479, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/185.jpg", "thumb": "../assets/media/thumbs/185_480.webp", "size": 93896, "mime": "image/jpeg", "name": null}]}, {"id": 183, "media": []}, {"id": 182, "media": []}, {"id": 180, "media": [{"kind": "photo", "path": "../assets/media/180.jpg", "thumb": "../assets/media/thumbs/180_480.webp", "size": 61163, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/181.jpg", "thumb": "../assets/media/thumbs/181_480.webp", "size": 47922, "mime": "image/jpeg", "name": null}]}, {"id": 179, "media": [{"kind": "photo", "path": "../assets/media/179.jpg", "thumb": "../assets/media/thumbs/179_480.webp", "size": 117046, "mime": "image/jpeg", "name": null}]}, {"id": 176, "media": [{"kind": "photo", "path": "../assets/media/176.jpg", "thumb": "../assets/media/thumbs/176_480.webp", "size": 52246, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/177.jpg", "thumb": "../assets/media/thumbs/177_480.webp", "size": 66442, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/178.jpg", "thumb": "../assets/media/thumbs/178_480.webp", "size": 71979, "mime": "image/jpeg", "name": null}]}];
    window.__STATIC_META = {"title": "CV Time", "username": "timeforcv", "channel": "timeforcv", "last_sync_utc": "2026-02-11T17:39:52Z", "posts_count": 107, "last_seen_message_id": 239, "stats": {"new": 122, "updated": 16, "media_downloaded": 122}, "avatar": "assets/channel_avatar.jpg", "meta_schema_version": "1.0.0", "posts_schema_version": "1.0.0"};
  </script>
  <script src="../common.js"></script>
  <script src="../static.js"></script>
</body>
</html>
