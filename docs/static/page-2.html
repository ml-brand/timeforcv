<!doctype html>
<html lang="ru">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>CV Time — статическая версия (стр. 2/4)</title>
  <meta name="description" content="Статическая версия зеркала Telegram-канала" />
  <link rel="icon" href="../favicon.ico?v=2026-02-10T16%3A45%3A42Z" sizes="any" />
  <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32.png?v=2026-02-10T16%3A45%3A42Z" />
  <link rel="apple-touch-icon" href="../apple-touch-icon.png?v=2026-02-10T16%3A45%3A42Z" />

  <link rel="stylesheet" href="../style.css" />
  <script src="../metrika.js"></script>
</head>
<body>
  <header class="header">
    <div class="container">
      <div class="title-grid">
        <a class="grid-avatar" href="#" target="_blank" rel="noopener">
          <img id="channelAvatar" class="channel-avatar" src="../assets/channel_avatar.jpg" alt="Аватар канала"  />
        </a>
        <div class="grid-main">
          <div class="title-head">
            <div class="title-left">
              <a class="badge-chip" id="siteTitleWrap" href="#" target="_blank" rel="noopener"><h1 id="siteTitle">CV Time</h1></a>
            </div>
            <div class="hero-actions">
              <a id="subscribeBtn" class="subscribe-btn" href="https://t.me/+JoULEedmHyE5MmYy" target="_blank" rel="noopener" >Подписаться</a>
              <a class="icon-btn" href="../" aria-label="Перейти к динамической версии">↺</a>
              <button id="themeToggle" class="icon-btn" type="button" aria-label="Переключить тему"></button>
            </div>
          </div>
        </div>
        <div class="controls"></div>
      </div>
    </div>
  </header>

  
  <div id="promoBanner" class="promo-banner" hidden>
    <div class="container promo-inner">
      <span class="promo-text"><a href="https://t.me/addlist/5NH3RoVejEI1MGEy">Подпишись на все наши ML каналы. Они классные, отвечаем!</a></span>
      <button id="promoClose" class="promo-close" type="button" aria-label="Скрыть плашку">×</button>
    </div>
  </div>
  

  <main class="container">
    
    <div class="pager static-pager" style="justify-content:center">
      <div class="page-links">
        <a class="nav-link" href="index.html">←</a>
        <a class="page-link" href="index.html">1</a> <a class="page-link current" href="page-2.html">2</a> <a class="page-link" href="page-3.html">3</a> <a class="page-link" href="page-4.html">4</a>
        <a class="nav-link" href="page-3.html">→</a>
      </div>
    </div>
    
    <div id="posts" class="posts">
      
    <article class="post" data-post-id="175" data-search="emerging properties in unified multimodal pretraining сегодня разбираем работу о модели bagel, способной генерировать и редактировать изображения, а также работать с последовательностями кадров. авторы заявляют результаты, местами превосходящие flux.1-dev, и позиционируют bagel как одну из сильнейших открытых vlm. в своё время команда bytedance занимала топ-1 на text-to-image arena, сейчас уступают gpt, но остаются в числе лидеров. свойства мультимодальных моделей понятие vlm постепенно меняется: от простых связок «текст-картинка» к системам, где на вход и выход можно подавать любые комбинации текста и изображений. ключевые свойства таких моделей: — дискретное vs непрерывное представление. дискретные токенизаторы (например, vq) ограничены размером словаря, из-за чего страдает качество. bagel использует непрерывные представления. — количество энкодеров. эксперименты показывают, что для понимания и генерации нужны разные свойства эмбеддингов. поэтому лучше использовать отдельные энкодеры: один для understanding-задач, другой для генерации. — авторегрессивность. в bagel отдельные патчи каждого изображения предсказываются параллельно, а не последовательно. — интегрированный или внешний генератор. возможны два подхода: всё в едином трансформере или через адаптер + внешнюю диффузионную модель. bagel реализует первый вариант. — open vs closed source. отличительная черта bagel — это открытый код, редкость среди моделей с непрерывными токенами. архитектура в основе bagel — крупный трансформер с двумя башнями для задач понимания и генерации. для понимания используется siglip2, а для генерации — flux vae. чтобы согласовать размеры представлений, добавлены mlp-адаптеры. архитектура реализует принцип mixture of transformers: параллельно работают два трансформера (каждый на ~7b параметров). токены разделяются между ними, а на отдельных шагах self-attention их представления смешиваются. ключевой момент: вместо дискретного next-token prediction используется flow matching, где модель предсказывает векторы скорости в непрерывном пространстве. эксперименты показывают, что эта стратегия даёт ощутимое преимущество. обучающие данные в основе обучения триплет-схема данных: чистый текст, пары «текст-картинка» для задач понимания и мультимодальные примеры, где текст и изображения перемешаны. основные источники данных — видео и веб-контент. большая часть разметки сгенерирована синтетически с помощью qwen-моделей (до 14b параметров) и deepseek для reasoning-трейсов. ​​для задачи редактирования авторы собирают данные за счёт аннотации различий между кадрами видео. также берут связанные по смыслу последовательности изображений из веба, например из step-by-step-инструкций. обучение обучение проходило в четыре стадии. сначала проводился алайнмент энкодера. обучался небольшой mlp-адаптер на выходах siglip2, тогда как остальные компоненты оставались замороженными. затем претрейн: почти все части модели размораживались (кроме vae), задачи понимания и генерации смешивались —причём оптимальным оказалось соотношение 4:1 в пользу генеративных задач. на стадии continued training разрешения увеличивались, а набор задач становился разнообразнее. завершающий шаг — sft и дообучение. здесь использовали промты, переформулированные с помощью deepseek, и внедряли reasoning-трейсы. результаты в задачах на понимание изображений bagel показывает топовые результаты почти во всех бенчмарках, уступая лишь qwen-2.5-vl на mmmu. в генерации модель на geneval превосходит flux и делит второе место со своей облегчённой версией, а в более сложном бенчмарке wice занимает второе место сразу после gpt-image. разбор подготовил ❣ александр устюжанин cv time emerging properties in unified multimodal pretraining сегодня разбираем работу о модели bagel, способной генерировать и редактировать изображения, а также работать с последовательностями кадров. авторы заявляют результаты, местами превосходящие flux.1-dev, и позиционируют bagel как одну из сильнейших открытых vlm. в своё время команда bytedance занимала топ-1 на text-to-image arena, сейчас уступают gpt, но остаются в числе лидеров. свойства мультимодальных моделей понятие vlm постепенно меняется: от простых связок «текст-картинка» к системам, где на вход и выход можно подавать любые комбинации текста и изображений. ключевые свойства таких моделей: — дискретное vs непрерывное представление. дискретные токенизаторы (например, vq) ограничены размером словаря, из-за чего страдает качество. bagel использует непрерывные представления. — количество энкодеров. эксперименты показывают, что для понимания и генерации нужны разные свойства эмбеддингов. поэтому лучше использовать отдельные энкодеры: один для understanding-задач, другой для генерации. — авторегрессивность. в bagel отдельные патчи каждого изображения предсказываются параллельно, а не последовательно. — интегрированный или внешний генератор. возможны два подхода: всё в едином трансформере или через адаптер + внешнюю диффузионную модель. bagel реализует первый вариант. — open vs closed source. отличительная черта bagel — это открытый код, редкость среди моделей с непрерывными токенами. архитектура в основе bagel — крупный трансформер с двумя башнями для задач понимания и генерации. для понимания используется siglip2, а для генерации — flux vae. чтобы согласовать размеры представлений, добавлены mlp-адаптеры. архитектура реализует принцип mixture of transformers: параллельно работают два трансформера (каждый на ~7b параметров). токены разделяются между ними, а на отдельных шагах self-attention их представления смешиваются. ключевой момент: вместо дискретного next-token prediction используется flow matching, где модель предсказывает векторы скорости в непрерывном пространстве. эксперименты показывают, что эта стратегия даёт ощутимое преимущество. обучающие данные в основе обучения триплет-схема данных: чистый текст, пары «текст-картинка» для задач понимания и мультимодальные примеры, где текст и изображения перемешаны. основные источники данных — видео и веб-контент. большая часть разметки сгенерирована синтетически с помощью qwen-моделей (до 14b параметров) и deepseek для reasoning-трейсов. ​​для задачи редактирования авторы собирают данные за счёт аннотации различий между кадрами видео. также берут связанные по смыслу последовательности изображений из веба, например из step-by-step-инструкций. обучение обучение проходило в четыре стадии. сначала проводился алайнмент энкодера. обучался небольшой mlp-адаптер на выходах siglip2, тогда как остальные компоненты оставались замороженными. затем претрейн: почти все части модели размораживались (кроме vae), задачи понимания и генерации смешивались —причём оптимальным оказалось соотношение 4:1 в пользу генеративных задач. на стадии continued training разрешения увеличивались, а набор задач становился разнообразнее. завершающий шаг — sft и дообучение. здесь использовали промты, переформулированные с помощью deepseek, и внедряли reasoning-трейсы. результаты в задачах на понимание изображений bagel показывает топовые результаты почти во всех бенчмарках, уступая лишь qwen-2.5-vl на mmmu. в генерации модель на geneval превосходит flux и делит второе место со своей облегчённой версией, а в более сложном бенчмарке wice занимает второе место сразу после gpt-image. разбор подготовил ❣ александр устюжанин cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-08-26T09:05:15+00:00" href="./posts/175.html">2025-08-26 09:05 UTC</a></div>
      </div>
      <div class="post-body"><strong>Emerging Properties in Unified Multimodal Pretraining</strong><br><br>Сегодня разбираем <a href="https://arxiv.org/abs/2505.14683" rel="nofollow noopener noreferrer">работу</a> о модели Bagel, способной генерировать и редактировать изображения, а также работать с последовательностями кадров. Авторы заявляют результаты, местами превосходящие Flux.1-dev, и позиционируют Bagel как одну из сильнейших открытых VLM. В своё время команда Bytedance занимала топ-1 на Text-to-Image Arena, сейчас уступают GPT, но остаются в числе лидеров.<br><br><strong>Свойства мультимодальных моделей</strong><br><br>Понятие VLM постепенно меняется: от простых связок «текст-картинка» к системам, где на вход и выход можно подавать любые комбинации текста и изображений. Ключевые свойства таких моделей:<br><br>— Дискретное vs непрерывное представление. Дискретные токенизаторы (например, VQ) ограничены размером словаря, из-за чего страдает качество. Bagel использует непрерывные представления.<br><br>— Количество энкодеров. Эксперименты показывают, что для понимания и генерации нужны разные свойства эмбеддингов. Поэтому лучше использовать отдельные энкодеры: один для understanding-задач, другой для генерации.<br><br>— Авторегрессивность. В Bagel отдельные патчи каждого изображения предсказываются параллельно, а не последовательно.<br><br>— Интегрированный или внешний генератор. Возможны два подхода: всё в едином трансформере или через адаптер + внешнюю диффузионную модель. Bagel реализует первый вариант.<br><br>— Open vs closed source. Отличительная черта Bagel — это открытый код, редкость среди моделей с непрерывными токенами.<br><br><strong>Архитектура</strong><br><br>В основе Bagel — крупный трансформер с двумя башнями для задач понимания и генерации. Для понимания используется SigLIP2, а для генерации — Flux VAE. Чтобы согласовать размеры представлений, добавлены MLP-адаптеры.<br><br>Архитектура реализует принцип Mixture of Transformers: параллельно работают два трансформера (каждый на ~7B параметров). Токены разделяются между ними, а на отдельных шагах self-attention их представления смешиваются.<br><br>Ключевой момент: вместо дискретного next-token prediction используется flow matching, где модель предсказывает векторы скорости в непрерывном пространстве. Эксперименты показывают, что эта стратегия даёт ощутимое преимущество.<br><br><strong>Обучающие данные</strong><br><br>В основе обучения триплет-схема данных: чистый текст, пары «текст-картинка» для задач понимания и мультимодальные примеры, где текст и изображения перемешаны. Основные источники данных — видео и веб-контент. Большая часть разметки сгенерирована синтетически с помощью Qwen-моделей (до 14B параметров) и DeepSeek для reasoning-трейсов. <br><br>​​Для задачи редактирования авторы собирают данные за счёт аннотации различий между кадрами видео. Также берут связанные по смыслу последовательности изображений из веба, например из step-by-step-инструкций.<br><br><strong>Обучение</strong><br><br>Обучение проходило в четыре стадии. Сначала проводился алайнмент энкодера. Обучался небольшой MLP-адаптер на выходах SigLIP2, тогда как остальные компоненты оставались замороженными.<br><br>Затем претрейн: почти все части модели размораживались (кроме VAE), задачи понимания и генерации смешивались —причём оптимальным оказалось соотношение 4:1 в пользу генеративных задач. <br><br>На стадии Continued Training разрешения увеличивались, а набор задач становился  разнообразнее. <br><br>Завершающий шаг — SFT и дообучение. Здесь использовали промты, переформулированные с помощью DeepSeek, и внедряли reasoning-трейсы.<br><br><strong>Результаты</strong><br><strong><br></strong>В задачах на понимание изображений Bagel показывает топовые результаты почти во всех бенчмарках, уступая лишь Qwen-2.5-VL на MMMU. В генерации модель на GenEval превосходит Flux и делит второе место со своей облегчённой версией, а в более сложном бенчмарке WICE занимает второе место сразу после GPT-Image.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Александр Устюжанин</em><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/175_480.webp" srcset="../assets/media/thumbs/175_480.webp 480w, ../assets/media/175.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="175" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 027 просмотров · 17 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/175" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/175.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="173" data-search="nexus-gen: a unified model for image understanding, generation, and editing сегодня разбираем статью о nexus-gen — мультимодальной модели от alibaba, которая задумывалась как полностью открытая: авторы выложили не только код и веса, но и датасет. модель умеет генерировать и редактировать изображения по текстовым запросам. качество картинок в целом достойное, хотя не всегда удаётся сохранить идентичность объектов при редактировании: при простых изменениях могут искажаться второстепенные детали — например, у человека слегка меняются черты лица, а в интерьере исчезают или трансформируются объекты, которые трогать не просили. архитектура в основе модели авторегрессор (qwen-2.5-vl) в связке с визуальным энкодером и декодером на базе flux. архитектура вдохновлена unifluid: текст и изображение проходят через общий авторегрессор, а для визуальной части используется отдельный визуальный декодер. в новой версии также добавлен декодер для редактирования изображений, который работает вместе с генеративным. главное улучшение модели связано с проблемой накопления ошибок на непрерывных визуальных токенах. в отличие от текста, где токены дискретны и ошибки не накапливаются, изображения страдают от смещения при последовательной генерации патчей. авторы предложили решение: ввести специальный обучаемый токен, который обозначает места для генерации визуальных патчей. при обучении он вставляется в последовательность, а при инференсе автоматически генерируется и подаётся в диффузионную голову. таким образом, модель всегда работает с фиксированным токеном, не накапливая ошибок с предыдущих шагов. для обучения используется комбинация лоссов: кросс-энтропия для текстовых токенов, mse и косинусная близость — для визуальных. это позволяет согласовать пространство визуального энкодера и выходы авторегрессора, сохраняя совместимость с диффузионной частью. этапы обучения сначала модель училась на задачах image understanding и image generation без учёта редактирования. на втором этапе задачи редактирования добавлялись в небольшом количестве. на третьем — к обучению подключили новый декодер для задач редактирования, а баланс сместился в сторону таких задач. на заключительном шаге проводили элайнмент между визуальными представлениями на входе и выходе авторегрессора, чтобы стабилизировать работу с диффузией и улучшить согласованность между генеративным и редактирующим декодерами. результаты в новой версии nexus-gen авторы, наконец, показали количественные результаты: модель на 7b параметров занимает первое место на ряде бенчмарков по пониманию изображений, включая mme-p (1602,3) и textvqa (75,5). также она показывает высокий уровень на vqav2 (79,3) и seed (77,1), сопоставимый или превосходящий конкурентов ощутимо больших размеров. при этом она сохраняет баланс между пониманием, генерацией и редактированием. разбор подготовил ❣ михаил колтаков cv time nexus-gen: a unified model for image understanding, generation, and editing сегодня разбираем статью о nexus-gen — мультимодальной модели от alibaba, которая задумывалась как полностью открытая: авторы выложили не только код и веса, но и датасет. модель умеет генерировать и редактировать изображения по текстовым запросам. качество картинок в целом достойное, хотя не всегда удаётся сохранить идентичность объектов при редактировании: при простых изменениях могут искажаться второстепенные детали — например, у человека слегка меняются черты лица, а в интерьере исчезают или трансформируются объекты, которые трогать не просили. архитектура в основе модели авторегрессор (qwen-2.5-vl) в связке с визуальным энкодером и декодером на базе flux. архитектура вдохновлена unifluid : текст и изображение проходят через общий авторегрессор, а для визуальной части используется отдельный визуальный декодер. в новой версии также добавлен декодер для редактирования изображений, который работает вместе с генеративным. главное улучшение модели связано с проблемой накопления ошибок на непрерывных визуальных токенах. в отличие от текста, где токены дискретны и ошибки не накапливаются, изображения страдают от смещения при последовательной генерации патчей. авторы предложили решение: ввести специальный обучаемый токен, который обозначает места для генерации визуальных патчей. при обучении он вставляется в последовательность, а при инференсе автоматически генерируется и подаётся в диффузионную голову. таким образом, модель всегда работает с фиксированным токеном, не накапливая ошибок с предыдущих шагов. для обучения используется комбинация лоссов: кросс-энтропия для текстовых токенов, mse и косинусная близость — для визуальных. это позволяет согласовать пространство визуального энкодера и выходы авторегрессора, сохраняя совместимость с диффузионной частью. этапы обучения сначала модель училась на задачах image understanding и image generation без учёта редактирования. на втором этапе задачи редактирования добавлялись в небольшом количестве. на третьем — к обучению подключили новый декодер для задач редактирования, а баланс сместился в сторону таких задач. на заключительном шаге проводили элайнмент между визуальными представлениями на входе и выходе авторегрессора, чтобы стабилизировать работу с диффузией и улучшить согласованность между генеративным и редактирующим декодерами. результаты в новой версии nexus-gen авторы, наконец, показали количественные результаты: модель на 7b параметров занимает первое место на ряде бенчмарков по пониманию изображений, включая mme-p (1602,3) и textvqa (75,5). также она показывает высокий уровень на vqav2 (79,3) и seed (77,1), сопоставимый или превосходящий конкурентов ощутимо больших размеров. при этом она сохраняет баланс между пониманием, генерацией и редактированием. разбор подготовил ❣ михаил колтаков cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-08-21T08:02:29+00:00" href="./posts/173.html">2025-08-21 08:02 UTC</a></div>
      </div>
      <div class="post-body"><strong>Nexus-Gen: A Unified Model for Image Understanding, Generation, and Editing</strong><br><br>Сегодня разбираем <a href="https://arxiv.org/pdf/2504.21356" rel="nofollow noopener noreferrer">статью</a> о Nexus-Gen — мультимодальной модели от Alibaba, которая задумывалась как полностью открытая: авторы выложили не только код и веса, но и датасет. Модель умеет генерировать и редактировать изображения по текстовым запросам. <br><br>Качество картинок в целом достойное, хотя не всегда удаётся сохранить идентичность объектов при редактировании: при простых изменениях могут искажаться второстепенные детали — например, у человека слегка меняются черты лица, а в интерьере исчезают или трансформируются объекты, которые трогать не просили.<br><br><strong>Архитектура</strong><br><br>В основе модели авторегрессор (Qwen-2.5-VL) в связке с визуальным энкодером и декодером на базе Flux. Архитектура вдохновлена <a href="https://arxiv.org/html/2503.13436v1" rel="nofollow noopener noreferrer">UniFLUID</a>: текст и изображение проходят через общий авторегрессор, а для визуальной части используется отдельный визуальный декодер. В новой версии также добавлен декодер для редактирования изображений, который работает вместе с генеративным.<br><br>Главное улучшение модели связано с проблемой накопления ошибок на непрерывных визуальных токенах. В отличие от текста, где токены дискретны и ошибки не накапливаются, изображения страдают от смещения при последовательной генерации патчей. Авторы предложили решение: ввести специальный обучаемый токен, который обозначает места для генерации визуальных патчей. При обучении он вставляется в последовательность, а при инференсе автоматически генерируется и подаётся в диффузионную голову. Таким образом, модель всегда работает с фиксированным токеном, не накапливая ошибок с предыдущих шагов.<br><br>Для обучения используется комбинация лоссов: кросс-энтропия для текстовых токенов, MSE и косинусная близость — для визуальных. Это позволяет согласовать пространство визуального энкодера и выходы авторегрессора, сохраняя совместимость с диффузионной частью.<br><br><strong>Этапы обучения<br></strong><br>Сначала модель училась на задачах image understanding и image generation без учёта редактирования. На втором этапе задачи редактирования добавлялись в небольшом количестве. На третьем — к обучению подключили новый декодер для задач редактирования, а баланс сместился в сторону таких задач. На заключительном шаге проводили элайнмент между визуальными представлениями на входе и выходе авторегрессора, чтобы стабилизировать работу с диффузией и улучшить согласованность между генеративным и редактирующим декодерами.<br><br><strong>Результаты </strong><br><br>В новой версии Nexus-Gen авторы, наконец, показали количественные результаты: модель на 7B параметров занимает первое место на ряде бенчмарков по пониманию изображений, включая MME-P (1602,3) и TextVQA (75,5). Также она показывает высокий уровень на VQAv2 (79,3) и SEED (77,1), сопоставимый или превосходящий конкурентов ощутимо больших размеров. При этом она сохраняет баланс между пониманием, генерацией и редактированием.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em>  Михаил Колтаков<br></em><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/173_480.webp" srcset="../assets/media/thumbs/173_480.webp 480w, ../assets/media/173.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="173" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/174_480.webp" srcset="../assets/media/thumbs/174_480.webp 480w, ../assets/media/174.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="173" data-image-index="1" /></div></div>
      <div class="actions">
        <span>1 937 просмотров · 14 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/173" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/173.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="172" data-search="тематическая подборка статей: генерация с эдитингом и vlm с генерацией сегодня подборка объединяет два актуальных направления в cv: развитие генеративных моделей с возможностью редактирования изображений и интеграцию генерации в vlm. генерация со встроенным эдитингом hidream-i1: a high-efficient image generative foundation model with sparse diffusion transformer неплохая модель по меркам опенсорса. авторы используют трансформер с mixture-of-experts-блоками и гибридной архитектурой mm-dit: текстовые и картиночные токены сначала процессятся отдельными слоями, затем — общими. в решении применяются четыре разных текстовых энкодера — выглядит как рекорд. также авторы делают дообучение модели под задачи эдитинга — самое горячее направление в генерации картинок, которому посвящены и следующие работы. imgedit: a unified image editing dataset and benchmark в статье предлагают полный набор для задачи эдитинга: датасет для обучения (автоматический пайплайн, которым сгенерировали 1,2 млн сэмплов, в том числе с многошаговым сценарием); обученную на нём модель (соединили vlm и dit, переиспользовав qwen и flux) и бенчмарк для оценки качества (также обучили qwen-as-a-judge, чтобы избежать разметки людьми). r-genie: reasoning-guided generative image editing модель для редактирования изображений с упором на задачи, требующие рассуждений (пример: «замени самого сонного человека на изображении на кота»). авторы предлагают свой бенчмарк под такую задачу. архитектурно соединяют vlm и dit, но с хитрыми блоками-перемычками между ними. vlm со встроенной генерацией diagnosing and mitigating modality interference in multimodal large language models предлагают набор регуляризаций, чтобы vlm лучше связывала текстовую и картиночную модальности. в частности, при обучении в текстовых задачах авторы подают случайную (мусорную) картинку и требуют, чтобы предсказание модели не изменилось; добавляют adversarial-шум к картиночным токенам. openuni: a simple baseline for unified multimodal understanding and generation соединяют vlm (internvl3, веса заморожены) и диффузионный генератор (sana, дообучается на второй стадии на 60 тыс. изображениях) через шестислойный трансформер (обучается на первой и второй стадиях). пайплайн выглядит просто, качество сравнимо с другими открытыми аналогами. muddit: liberating generation beyond text-to-image with a unified discrete diffusion model особенность работы в том, что для генерации изображений и текстов авторы используют дискретную диффузию. в качестве бэкбона берут предобученный mm-dit, и добавляют энкодер/декодер для картинок и текстов. качество не топовое, работа имеет скорее концептуальную ценность. are unified vision-language models necessary: generalization across understanding and generation авторы показывают, что задачи генерации и дискриминации могут обогащать друг друга при совместном обучении. особенно хорошо работает, когда вход и выход имеют схожую природу: «siglip in / siglip out» или «vqa in / vqa out». co-reinforcement learning for unified multimodal understanding and generation статья о том, как делать rl для архитектуры вроде janus-pro. интересная идея — использовать grpo с cycle consistency reward: модель учится и на дискриминации, и на генерации, проверяя, насколько хорошо восстановленный ввод совпадает с исходным. подборку подготовил ❣ артём конев cv time тематическая подборка статей: генерация с эдитингом и vlm с генерацией сегодня подборка объединяет два актуальных направления в cv: развитие генеративных моделей с возможностью редактирования изображений и интеграцию генерации в vlm. генерация со встроенным эдитингом hidream-i1: a high-efficient image generative foundation model with sparse diffusion transformer неплохая модель по меркам опенсорса. авторы используют трансформер с mixture-of-experts-блоками и гибридной архитектурой mm-dit: текстовые и картиночные токены сначала процессятся отдельными слоями, затем — общими. в решении применяются четыре разных текстовых энкодера — выглядит как рекорд. также авторы делают дообучение модели под задачи эдитинга — самое горячее направление в генерации картинок, которому посвящены и следующие работы. imgedit: a unified image editing dataset and benchmark в статье предлагают полный набор для задачи эдитинга: датасет для обучения (автоматический пайплайн, которым сгенерировали 1,2 млн сэмплов, в том числе с многошаговым сценарием); обученную на нём модель (соединили vlm и dit, переиспользовав qwen и flux) и бенчмарк для оценки качества (также обучили qwen-as-a-judge, чтобы избежать разметки людьми). r-genie: reasoning-guided generative image editing модель для редактирования изображений с упором на задачи, требующие рассуждений (пример: «замени самого сонного человека на изображении на кота»). авторы предлагают свой бенчмарк под такую задачу. архитектурно соединяют vlm и dit, но с хитрыми блоками-перемычками между ними. vlm со встроенной генерацией diagnosing and mitigating modality interference in multimodal large language models предлагают набор регуляризаций, чтобы vlm лучше связывала текстовую и картиночную модальности. в частности, при обучении в текстовых задачах авторы подают случайную (мусорную) картинку и требуют, чтобы предсказание модели не изменилось; добавляют adversarial-шум к картиночным токенам. openuni: a simple baseline for unified multimodal understanding and generation соединяют vlm (internvl3, веса заморожены) и диффузионный генератор (sana, дообучается на второй стадии на 60 тыс. изображениях) через шестислойный трансформер (обучается на первой и второй стадиях). пайплайн выглядит просто, качество сравнимо с другими открытыми аналогами. muddit: liberating generation beyond text-to-image with a unified discrete diffusion model особенность работы в том, что для генерации изображений и текстов авторы используют дискретную диффузию. в качестве бэкбона берут предобученный mm-dit, и добавляют энкодер/декодер для картинок и текстов. качество не топовое, работа имеет скорее концептуальную ценность. are unified vision-language models necessary: generalization across understanding and generation авторы показывают, что задачи генерации и дискриминации могут обогащать друг друга при совместном обучении. особенно хорошо работает, когда вход и выход имеют схожую природу: «siglip in / siglip out» или «vqa in / vqa out». co-reinforcement learning for unified multimodal understanding and generation статья о том, как делать rl для архитектуры вроде janus-pro. интересная идея — использовать grpo с cycle consistency reward: модель учится и на дискриминации, и на генерации, проверяя, насколько хорошо восстановленный ввод совпадает с исходным. подборку подготовил ❣ артём конев cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-08-13T08:03:22+00:00" href="./posts/172.html">2025-08-13 08:03 UTC</a></div>
      </div>
      <div class="post-body"><strong>Тематическая подборка статей: генерация с эдитингом и VLM с генерацией<br></strong><br>Сегодня подборка объединяет два актуальных направления в CV: развитие генеративных моделей с возможностью редактирования изображений и интеграцию генерации в VLM. <br><br><strong>Генерация со встроенным эдитингом<br></strong><br><a href="https://arxiv.org/abs/2505.22705" rel="nofollow noopener noreferrer"><strong>HiDream-I1: A High-Efficient Image Generative Foundation Model with Sparse Diffusion Transformer</strong></a><br>Неплохая модель по меркам опенсорса. Авторы используют трансформер с mixture-of-experts-блоками и гибридной архитектурой MM-DiT: текстовые и картиночные токены сначала процессятся отдельными слоями, затем — общими. В решении применяются четыре разных текстовых энкодера — выглядит как рекорд. Также авторы делают дообучение модели под задачи эдитинга — самое горячее направление в генерации картинок, которому посвящены и следующие работы.<br><br><a href="https://arxiv.org/abs/2505.20275" rel="nofollow noopener noreferrer"><strong>ImgEdit: A Unified Image Editing Dataset and Benchmark</strong></a><br>В статье предлагают полный набор для задачи эдитинга: датасет для обучения (автоматический пайплайн, которым сгенерировали 1,2 млн сэмплов, в том числе с многошаговым сценарием); обученную на нём модель (соединили VLM и DiT, переиспользовав Qwen и Flux) и бенчмарк для оценки качества (также обучили Qwen-as-a-judge, чтобы избежать разметки людьми).<br><br><a href="https://arxiv.org/abs/2505.17768" rel="nofollow noopener noreferrer"><strong>R-Genie: Reasoning-Guided Generative Image Editing</strong></a><br>Модель для редактирования изображений с упором на задачи, требующие рассуждений (пример: «Замени самого сонного человека на изображении на кота»). Авторы предлагают свой бенчмарк под такую задачу. Архитектурно соединяют VLM и DiT, но с хитрыми блоками-перемычками между ними.<br><br><strong>VLM со встроенной генерацией<br></strong><br><a href="https://arxiv.org/abs/2505.19616" rel="nofollow noopener noreferrer"><strong>Diagnosing and Mitigating Modality Interference in Multimodal Large Language Models</strong></a><br>Предлагают набор регуляризаций, чтобы VLM лучше связывала текстовую и картиночную модальности. В частности, при обучении в текстовых задачах авторы подают случайную (мусорную) картинку и требуют, чтобы предсказание модели не изменилось; добавляют adversarial-шум к картиночным токенам.<br><br><a href="https://arxiv.org/abs/2505.23661" rel="nofollow noopener noreferrer"><strong>OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation</strong></a><br>Соединяют VLM (InternVL3, веса заморожены) и диффузионный генератор (SANA, дообучается на второй стадии на 60 тыс. изображениях) через шестислойный трансформер (обучается на первой и второй стадиях). Пайплайн выглядит просто, качество сравнимо с другими открытыми аналогами.<br><br><a href="https://arxiv.org/abs/2505.23606" rel="nofollow noopener noreferrer"><strong>Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model<br></strong></a>Особенность работы в том, что для генерации изображений и текстов авторы используют дискретную диффузию. В качестве бэкбона берут предобученный MM-DiT, и добавляют энкодер/декодер для картинок и текстов. Качество не топовое, работа имеет скорее концептуальную ценность.<br><br><a href="https://arxiv.org/abs/2505.23043" rel="nofollow noopener noreferrer"><strong>Are Unified Vision-Language Models Necessary: Generalization Across Understanding and Generation</strong></a><br>Авторы показывают, что задачи генерации и дискриминации могут обогащать друг друга при совместном обучении. Особенно хорошо работает, когда вход и выход имеют схожую природу: «SigLIP in / SigLIP out» или «VQA in / VQA out».<br><br><a href="https://arxiv.org/abs/2505.17534" rel="nofollow noopener noreferrer"><strong>Co-Reinforcement Learning for Unified Multimodal Understanding and Generation</strong></a><br>Статья о том, как делать RL для архитектуры вроде Janus-Pro. Интересная идея — использовать GRPO с Cycle Consistency Reward: модель учится и на дискриминации, и на генерации, проверяя, насколько хорошо восстановленный ввод совпадает с исходным.<br><br><em>Подборку подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Артём Конев<br></em><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a></div>
      <div class="actions">
        <span>2 525 просмотров · 15 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/172" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/172.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="170" data-search="cross-frame representation alignment for fine-tuning video diffusion models сегодня речь пойдёт об улучшении генерации видео. разберём статью о cross-frame representation alignment (crepa) — адаптированной версии repa. метод repa разработан для генерации изображений. он считает similarity-score между промежуточным представлением диффузионной модели и предподсчитанными визуальными фичами (например, dino). чтобы приблизить фичи, в модели similarity-score добавляется к диффузионному лоссу. именно в этом кроется потенциал repa для тонкой настройки диффузионной модели. авторы предлагают два способа обобщения картиночного repa на видео: 1. применять repa для каждого из кадров. но repa-составляющая никак не учитывает темпоральную связь между кадрами, что может порождать неконсистентные генерации. 2. crepa. в лосс для каждого кадра добавляются similarity-score соседних представлений (с некоторым коэффициентом) — темпоральная связь появляется, проблема решена! для апробации crepa авторы использовали две модели cogvideox-5b и hunyuan video. результаты их работы можно оценить на иллюстрациях (первая генерация — от cogvideox-5b). визуально консистентность растёт. а авторы отмечают динамику fvd 305-291-281 для vanilla-repa-crepa. разбор подготовил ❣ андрей чернов cv time cross-frame representation alignment for fine-tuning video diffusion models сегодня речь пойдёт об улучшении генерации видео. разберём статью о cross-frame representation alignment (crepa) — адаптированной версии repa. метод repa разработан для генерации изображений. он считает similarity-score между промежуточным представлением диффузионной модели и предподсчитанными визуальными фичами (например, dino). чтобы приблизить фичи, в модели similarity-score добавляется к диффузионному лоссу. именно в этом кроется потенциал repa для тонкой настройки диффузионной модели. авторы предлагают два способа обобщения картиночного repa на видео: 1. применять repa для каждого из кадров. но repa-составляющая никак не учитывает темпоральную связь между кадрами, что может порождать неконсистентные генерации. 2. crepa. в лосс для каждого кадра добавляются similarity-score соседних представлений (с некоторым коэффициентом) — темпоральная связь появляется, проблема решена! для апробации crepa авторы использовали две модели cogvideox-5b и hunyuan video. результаты их работы можно оценить на иллюстрациях (первая генерация — от cogvideox-5b). визуально консистентность растёт. а авторы отмечают динамику fvd 305-291-281 для vanilla-repa-crepa. разбор подготовил ❣ андрей чернов cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-08-07T09:29:46+00:00" href="./posts/170.html">2025-08-07 09:29 UTC</a></div>
      </div>
      <div class="post-body"><strong>Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion Models</strong><br><br>Сегодня речь пойдёт об улучшении генерации видео. Разберём <a href="https://arxiv.org/abs/2506.09229" rel="nofollow noopener noreferrer">статью</a> о Cross-frame Representation Alignment (CREPA) — адаптированной версии REPA.<br><br>Метод REPA разработан для генерации изображений. Он считает similarity-score между промежуточным представлением диффузионной модели и предподсчитанными визуальными фичами (например, DINO). Чтобы приблизить фичи, в модели similarity-score добавляется к диффузионному лоссу. Именно в этом кроется потенциал REPA для тонкой настройки диффузионной модели.<br><br>Авторы предлагают два способа обобщения картиночного REPA на видео: <br><br>1. Применять REPA для каждого из кадров. Но REPA-составляющая никак не учитывает темпоральную связь между кадрами, что может порождать неконсистентные генерации.<br><br>2. CREPA. В лосс для каждого кадра добавляются similarity-score соседних представлений (с некоторым коэффициентом) — темпоральная связь появляется, проблема решена! <br><br>Для апробации CREPA авторы использовали две модели CogVideoX-5B и Hunyuan Video. Результаты их работы можно оценить на иллюстрациях (первая генерация — от CogVideoX-5B). Визуально консистентность растёт. А авторы отмечают динамику FVD 305-291-281 для Vanilla-REPA-CREPA.<br> <br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Андрей Чернов</em><br><a href="https://t.me/timeforcv" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/170_480.webp" srcset="../assets/media/thumbs/170_480.webp 480w, ../assets/media/170.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="170" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/171_480.webp" srcset="../assets/media/thumbs/171_480.webp 480w, ../assets/media/171.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="170" data-image-index="1" /></div></div>
      <div class="actions">
        <span>2 456 просмотров · 9 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/170" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/170.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="169" data-search="forte: finding outliers with representation typicality estimation сегодня разбираем статью, в которой авторы представляют новый метод обнаружения выбросов (out-of-distribution) для картиночных датасетов. метод показал лучшие результаты в задаче out-of-distribution detection on imagenet-1k vs ninco (auroc = 98.34, fpr@95 = 5.18). в работе утверждается, что низкое значение likelihood не всегда эффективно для обнаружения аутлаеров в пространствах высокой размерности. вместо likelihood предлагается использовать оценку typicality, по аналогии с подходом из density of states estimator (dose): для каждого изображения собираются статистики эмбеддинга, после чего на этих признаках обучается модель оценки плотности. авторы тестируют one-class svm, gaussian kernel density estimation и gaussian mixture model. полученные оценки плотности используются для вычисления typicality каждого изображения. при этом для обучения используются только in-distribution-данные. для получения статистик применяются локальные геометрические признаки из работ по manifold estimation (например, recall per point — доля in-distribution-семплов в радиусе, равном расстоянию до ближайшего соседа). авторы показывают, что метод позволяет успешно обнаруживать сгенерированные изображения. например, при модификации изображений с помощью stable diffusion 2.0 при strength=0.5 (умеренное изменение оригинала) достигаются auroc = 82.93 и fpr@95 = 46.80. этот алгоритм оказался интересен ml-разработке яндекс карт в задаче поиска фотографий, которые пользователи по ошибке загрузили в неправильную организацию. его применили для нахождения аутлаеров на двух датасетах: один разметили вручную, второй — автоматически. для автоматической разметки использовали косинус между изображением и строкой, состоящей из {название организации} + {рубрика организации}. на размеченном датасете forte показал auroc = 91.68 и fpr@95tpr = 20.95, а на синтетическом — auroc = 85.24 и fpr@95tpr = 93.24. при этом текущий бейзлайн, который фильтрует аутлайеры по значению косинуса, набирает auroc = 81.02 и fpr@95tpr = 82.87. пока преимущество forte над нашим бейзлайном не выглядит значительным, но идея использования методов из manifold estimation кажется перспективной. разбор подготовил ❣ иван балашов cv time forte: finding outliers with representation typicality estimation сегодня разбираем статью , в которой авторы представляют новый метод обнаружения выбросов (out-of-distribution) для картиночных датасетов. метод показал лучшие результаты в задаче out-of-distribution detection on imagenet-1k vs ninco (auroc = 98.34, fpr@95 = 5.18). в работе утверждается, что низкое значение likelihood не всегда эффективно для обнаружения аутлаеров в пространствах высокой размерности. вместо likelihood предлагается использовать оценку typicality, по аналогии с подходом из density of states estimator (dose) : для каждого изображения собираются статистики эмбеддинга, после чего на этих признаках обучается модель оценки плотности. авторы тестируют one-class svm, gaussian kernel density estimation и gaussian mixture model. полученные оценки плотности используются для вычисления typicality каждого изображения. при этом для обучения используются только in-distribution-данные. для получения статистик применяются локальные геометрические признаки из работ по manifold estimation (например, recall per point — доля in-distribution-семплов в радиусе, равном расстоянию до ближайшего соседа). авторы показывают, что метод позволяет успешно обнаруживать сгенерированные изображения. например, при модификации изображений с помощью stable diffusion 2.0 при strength=0.5 (умеренное изменение оригинала) достигаются auroc = 82.93 и fpr@95 = 46.80. этот алгоритм оказался интересен ml-разработке яндекс карт в задаче поиска фотографий, которые пользователи по ошибке загрузили в неправильную организацию. его применили для нахождения аутлаеров на двух датасетах: один разметили вручную, второй — автоматически. для автоматической разметки использовали косинус между изображением и строкой, состоящей из {название организации} + {рубрика организации}. на размеченном датасете forte показал auroc = 91.68 и fpr@95tpr = 20.95, а на синтетическом — auroc = 85.24 и fpr@95tpr = 93.24. при этом текущий бейзлайн, который фильтрует аутлайеры по значению косинуса, набирает auroc = 81.02 и fpr@95tpr = 82.87. пока преимущество forte над нашим бейзлайном не выглядит значительным, но идея использования методов из manifold estimation кажется перспективной. разбор подготовил ❣ иван балашов cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-07-31T11:59:18+00:00" href="./posts/169.html">2025-07-31 11:59 UTC</a></div>
      </div>
      <div class="post-body"><strong>Forte: Finding Outliers with Representation Typicality Estimation<br></strong><br>Сегодня разбираем <a href="https://arxiv.org/abs/2410.01322" rel="nofollow noopener noreferrer">статью</a>, в которой авторы представляют новый метод обнаружения выбросов (out-of-distribution) для картиночных датасетов. Метод показал лучшие результаты в задаче <a href="https://paperswithcode.com/sota/out-of-distribution-detection-on-imagenet-1k-13" rel="nofollow noopener noreferrer">Out-of-Distribution Detection on ImageNet-1k vs NINCO</a> (AUROC = 98.34, FPR@95 = 5.18).<br><br>В работе утверждается, что низкое значение likelihood не всегда эффективно для обнаружения аутлаеров в пространствах высокой размерности. Вместо likelihood предлагается использовать оценку typicality, по аналогии с подходом из <a href="https://arxiv.org/abs/2006.09273" rel="nofollow noopener noreferrer">Density of States Estimator (DoSE)</a>: для каждого изображения собираются статистики эмбеддинга, после чего на этих признаках обучается модель оценки плотности. Авторы тестируют One-Class SVM, Gaussian Kernel Density Estimation и Gaussian Mixture Model. Полученные оценки плотности используются для вычисления typicality каждого изображения. При этом для обучения используются только in-distribution-данные. Для получения статистик применяются локальные геометрические признаки из работ по manifold estimation (например, Recall per point — доля in-distribution-семплов в радиусе, равном расстоянию до ближайшего соседа).<br><br>Авторы показывают, что метод позволяет успешно обнаруживать сгенерированные изображения. Например, при модификации изображений с помощью Stable Diffusion 2.0 при strength=0.5 (умеренное изменение оригинала) достигаются AUROC = 82.93 и FPR@95 = 46.80.<br><br>Этот алгоритм оказался интересен ML-разработке Яндекс Карт в задаче поиска фотографий, которые пользователи по ошибке загрузили в неправильную организацию. Его применили для нахождения аутлаеров на двух датасетах: один разметили вручную, второй — автоматически. Для автоматической разметки использовали косинус между изображением и строкой, состоящей из {название организации} + {рубрика организации}.<br><br>На размеченном датасете Forte показал AUROC = 91.68 и FPR@95TPR = 20.95, а на синтетическом — AUROC = 85.24 и FPR@95TPR = 93.24. При этом текущий бейзлайн, который фильтрует аутлайеры по значению косинуса, набирает AUROC = 81.02 и FPR@95TPR = 82.87.<br><br>Пока преимущество Forte над нашим бейзлайном не выглядит значительным, но идея использования методов из manifold estimation кажется перспективной.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em>  Иван Балашов<br></em><a href="https://t.me/timeforcv" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/169_480.webp" srcset="../assets/media/thumbs/169_480.webp 480w, ../assets/media/169.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="169" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 189 просмотров · 16 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/169" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/169.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="168" data-search="perception encoder: the best visual embeddings are not at the output of the network сегодня разбираем статью, авторы которой предлагают простой визуальный энкодер, обученный только на открытых данных, без сложных архитектур и языковых моделей. всё обучение — это contrastive learning между изображениями и подписями. исследователи показывают, что даже в таком режиме можно получить эмбеддинги, которые превосходят существующие модели на стандартных бенчмарках. главная идея: сильные визуальные представления появляются не обязательно в последнем слое модели, а где-то внутри. в архитектуре используется базовая vit-модель с разрешением 224. при обучении применяются стандартные аугментации, attention pooling через cls-токен и несколько инженерных приёмов: прогрессивное увеличение разрешения, обучение с большим batch size, оптимизатор lamb вместо adamw, маскирование части изображений с регуляризацией (maskfit), rope вместе с позиционными эмбеддингами. вся модель обучается на contrastive loss — пары «изображение-текст» берут из общедоступных коллекций. чтобы сэкономить вычисления, сначала обучают на низком разрешении, потом повышают до 336. такой подход не только ускоряет обучение, но и, как утверждают авторы, помогает избежать переобучения позиционных эмбеддингов. после обучения на изображениях авторы подключают видео. они берут небольшой датасет с роликами и описаниями, прогоняют по 8 кадров через perception encoder, усредняют эмбеддинги и обучают contrastive loss на парах «видео-текст». часть описаний взяли из открытых источников, часть — сгенерировали своей моделью. для этого они собрали отдельную vlm (plm), в которую встроили perception encoder и дообучили на видео и картинках с подписями. модель даёт черновой текст, который потом правят вручную и добавляют метаинформацию — действия, объекты, временные сегменты. эти описания идут в обучение. авторы пишут, что это помогает даже в задачах классификации изображений. на бенчмарках perception encoder показывает хорошие результаты. авторы замечают: если взять не последний слой, а, например, 47-й, то на многих задачах это даёт лучший результат. у других моделей эмбеддинги либо слабее в середине, либо не меняются от увеличения модели. у perception encoder эффект усиления заметен. чтобы подключить этот энкодер к языковой модели, обучают projection head на выбранном слое — с температурой и двухслойным mlp. такой подход даёт выигрыш по качеству по сравнению с head&#x27;ами на других слоях. чем больше языковая модель — тем выше метрики. однако есть несколько моментов, которые вызывают вопросы. во-первых, сравнение с конкурентами неполное: в основной статье нет упоминания qwen, хотя в другом материале от тех же авторов сравнение с ней есть — и qwen выигрывает по ряду задач. во-вторых, идея, что видеоданные помогают классификации изображений, не объяснена, авторы не предлагают гипотезу, почему так происходит. в-третьих, подход с выбором «лучшего» слоя работает у их модели, но неясно, насколько он универсален. отдельно хочется понять, насколько perception encoder стабилен вне тех задач, которые выбрали для оценки. в целом статья показывает, что простая архитектура с грамотной инженерией и небольшим дообучением может дать представления, которые хорошо работают на downstream-задачах. авторы не предлагают революции, но аккуратно исследуют поведение модели и дают полезные практические выводы — особенно про выбор слоя и влияние видеоданных. разбор подготовил ❣ малик газизуллин cv time perception encoder: the best visual embeddings are not at the output of the network сегодня разбираем статью , авторы которой предлагают простой визуальный энкодер, обученный только на открытых данных, без сложных архитектур и языковых моделей. всё обучение — это contrastive learning между изображениями и подписями. исследователи показывают, что даже в таком режиме можно получить эмбеддинги, которые превосходят существующие модели на стандартных бенчмарках. главная идея: сильные визуальные представления появляются не обязательно в последнем слое модели, а где-то внутри. в архитектуре используется базовая vit-модель с разрешением 224. при обучении применяются стандартные аугментации, attention pooling через cls-токен и несколько инженерных приёмов: прогрессивное увеличение разрешения, обучение с большим batch size, оптимизатор lamb вместо adamw, маскирование части изображений с регуляризацией (maskfit), rope вместе с позиционными эмбеддингами. вся модель обучается на contrastive loss — пары «изображение-текст» берут из общедоступных коллекций. чтобы сэкономить вычисления, сначала обучают на низком разрешении, потом повышают до 336. такой подход не только ускоряет обучение, но и, как утверждают авторы, помогает избежать переобучения позиционных эмбеддингов. после обучения на изображениях авторы подключают видео. они берут небольшой датасет с роликами и описаниями, прогоняют по 8 кадров через perception encoder, усредняют эмбеддинги и обучают contrastive loss на парах «видео-текст». часть описаний взяли из открытых источников, часть — сгенерировали своей моделью. для этого они собрали отдельную vlm (plm), в которую встроили perception encoder и дообучили на видео и картинках с подписями. модель даёт черновой текст, который потом правят вручную и добавляют метаинформацию — действия, объекты, временные сегменты. эти описания идут в обучение. авторы пишут, что это помогает даже в задачах классификации изображений. на бенчмарках perception encoder показывает хорошие результаты. авторы замечают: если взять не последний слой, а, например, 47-й, то на многих задачах это даёт лучший результат. у других моделей эмбеддинги либо слабее в середине, либо не меняются от увеличения модели. у perception encoder эффект усиления заметен. чтобы подключить этот энкодер к языковой модели, обучают projection head на выбранном слое — с температурой и двухслойным mlp. такой подход даёт выигрыш по качеству по сравнению с head&amp;#x27;ами на других слоях. чем больше языковая модель — тем выше метрики. однако есть несколько моментов, которые вызывают вопросы. во-первых, сравнение с конкурентами неполное: в основной статье нет упоминания qwen, хотя в другом материале от тех же авторов сравнение с ней есть — и qwen выигрывает по ряду задач. во-вторых, идея, что видеоданные помогают классификации изображений, не объяснена, авторы не предлагают гипотезу, почему так происходит. в-третьих, подход с выбором «лучшего» слоя работает у их модели, но неясно, насколько он универсален. отдельно хочется понять, насколько perception encoder стабилен вне тех задач, которые выбрали для оценки. в целом статья показывает, что простая архитектура с грамотной инженерией и небольшим дообучением может дать представления, которые хорошо работают на downstream-задачах. авторы не предлагают революции, но аккуратно исследуют поведение модели и дают полезные практические выводы — особенно про выбор слоя и влияние видеоданных. разбор подготовил ❣ малик газизуллин cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-07-22T10:05:45+00:00" href="./posts/168.html">2025-07-22 10:05 UTC</a></div>
      </div>
      <div class="post-body"><strong>Perception Encoder: The best visual embeddings are not at the output of the network </strong><br><br>Сегодня разбираем <a href="https://arxiv.org/abs/2504.13181" rel="nofollow noopener noreferrer">статью</a>, авторы которой предлагают простой визуальный энкодер, обученный только на открытых данных, без сложных архитектур и языковых моделей. Всё обучение — это contrastive learning между изображениями и подписями. Исследователи показывают, что даже в таком режиме можно получить эмбеддинги, которые превосходят существующие модели на стандартных бенчмарках. Главная идея: сильные визуальные представления появляются не обязательно в последнем слое модели, а где-то внутри.<br><br>В архитектуре используется базовая ViT-модель с разрешением 224. При обучении применяются стандартные аугментации, attention pooling через CLS-токен и несколько инженерных приёмов: прогрессивное увеличение разрешения, обучение с большим batch size, оптимизатор LAMB вместо AdamW, маскирование части изображений с регуляризацией (maskfit), RoPE вместе с позиционными эмбеддингами. Вся модель обучается на contrastive loss — пары «изображение-текст» берут из общедоступных коллекций. Чтобы сэкономить вычисления, сначала обучают на низком разрешении, потом повышают до 336. Такой подход не только ускоряет обучение, но и, как утверждают авторы, помогает избежать переобучения позиционных эмбеддингов.<br><br>После обучения на изображениях авторы подключают видео. Они берут небольшой датасет с роликами и описаниями, прогоняют по 8 кадров через perception encoder, усредняют эмбеддинги и обучают contrastive loss на парах «видео-текст». Часть описаний взяли из открытых источников, часть — сгенерировали своей моделью. Для этого они собрали отдельную VLM (PLM), в которую встроили perception encoder и дообучили на видео и картинках с подписями. Модель даёт черновой текст, который потом правят вручную и добавляют метаинформацию — действия, объекты, временные сегменты. Эти описания идут в обучение. Авторы пишут, что это помогает даже в задачах классификации изображений. <br><br>На бенчмарках perception encoder показывает хорошие результаты. Авторы замечают: если взять не последний слой, а, например, 47-й, то на многих задачах это даёт лучший результат. У других моделей эмбеддинги либо слабее в середине, либо не меняются от увеличения модели. У perception encoder эффект усиления заметен.<br><br>Чтобы подключить этот энкодер к языковой модели, обучают projection head на выбранном слое — с температурой и двухслойным MLP. Такой подход даёт выигрыш по качеству по сравнению с head&#x27;ами на других слоях. Чем больше языковая модель — тем выше метрики.<br><br>Однако есть несколько моментов, которые вызывают вопросы. Во-первых, сравнение с конкурентами неполное: в основной статье нет упоминания Qwen, хотя в другом материале от тех же авторов сравнение с ней есть — и Qwen выигрывает по ряду задач. Во-вторых, идея, что видеоданные помогают классификации изображений, не объяснена, авторы не предлагают гипотезу, почему так происходит. В-третьих, подход с выбором «лучшего» слоя работает у их модели, но неясно, насколько он универсален. Отдельно хочется понять, насколько perception encoder стабилен вне тех задач, которые выбрали для оценки.<br><br>В целом статья показывает, что простая архитектура с грамотной инженерией и небольшим дообучением может дать представления, которые хорошо работают на downstream-задачах. Авторы не предлагают революции, но аккуратно исследуют поведение модели и дают полезные практические выводы — особенно про выбор слоя и влияние видеоданных.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Малик Газизуллин</em><br><a href="https://t.me/timeforcv" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/168_480.webp" srcset="../assets/media/thumbs/168_480.webp 480w, ../assets/media/168.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="168" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 317 просмотров · 26 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/168" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/168.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="167" data-search="тематическая подборка статей: дискриминативные модели свежая подборка статей о методах улучшения взаимодействия текста и изображений в мультимодальных моделях. в центре внимания — файнтюн clip для понимания отрицаний, новые подходы к retrieval, оптимизации архитектур vision transformer и многое другое. дообучение clip-моделей tng-clip: training-time negation data generation for negation awareness of clip предлагают пайплайн файнтюна текстовой части clip на понимание отрицаний: на лету для батча генерируют новые тексты, содержащие отрицания, используя тексты с похожих картинок для усложнения задачи. также показывают, что можно подменить текстовый энкодер в предобученной диффузионной модели, и генерации с отрицаниями в промпте тоже станут лучше. visualized text-to-image retrieval авторы говорят, что вместо text-to-image retrieval можно сначала сгенерировать картинку по текстовому запросу, а потом уже делать image-to-image retrieval чисто по картиночным фичам. тестируются на специфических постановках задач типа rag, но идея интересная. hard negative contrastive learning for fine-grained geometric understanding in large multimodal models доливают в обучение clip датасет с геометрией и используют полученную модель как энкодер в vlm. геометрические датасеты добавляют и в другие стадии обучения vlm, но основная новизна в том, как сделать файнтюн на геометрию в clip-постановке. distill clip (dclip): enhancing image-text retrieval via cross-modal transformer distillation для дообучения clip собирают модель-учитель, которая извлекает картиночные фичи по выделенным через yolo областям и агрегирует их через cross-attention с текстовыми фичами; затем этот учитель используется для дистилляции. с ростом качества на retrieval-задачах метод просаживает точность zero-shot-классификации. vision transformers with self-distilled registers изучают проблему токенов-аутлаеров в трансформерных моделях, описанную в статье vision transformers need registers. в ней предложили на вход модели подавать токены-регистры. также авторы пишут, что такие токены можно добавлять в уже обученную модель и файнтюнить её так, чтобы аутлаеры «перетекали» в добавленные токены. архитектура дискриминативных моделей taming transformer without using learning rate warmup связывают нестабильность в обучении трансформеров с тем, что матрица аттеншена становится низкоранговой и разреженной. предлагают добавить в adam ограничение на learning rate для апдейтов, которые имеют высокую спектральную норму по сравнению с текущей матрицей. показывают, что в этом случае возможно обучение без lr-warmup&#x27;а. repavit: scalable vision transformer acceleration via structural reparameterization on feedforward network layers применяют идеи из shufflenet к vit: в ffn-блоке делают нелинейность только для части нейронов промежуточного слоя — вторую часть можно после обучения вмерджить в одну линейную операцию. также заменяют layernorm на batchnorm и его тоже вмердживают после обучения. но тестируют всё это только на imagenet, есть подозрение, что на более сложных датасетах профита не будет. textregion: text-aligned region tokens from frozen image-text models решают zero-shot-сегментацию и смежные задачи, предлагают пайплайн, в котором объединяют sam и clip-модель: через sam находят области с объектами, и в clip-модели модифицируют аттеншен последнего слоя, чтобы он смотрел на каждую область по отдельности — таким образом получают токены для областей, которые уже можно сопоставлять с текстовыми представлениями класса и делать сегментацию. ren: fast and efficient region encodings from patch-based image encoders отличие от предыдущей статьи в том, что теперь область интереса на картинке кодируем координатами точки. имея предобученный бэкбон, добавляем к нему голову, которая по координатам точки смотрит на карту признаков и возвращает эмбеддинг соответствующего ей объекта; sam теперь используется только на этапе обучения. подборку подготовил ❣ артём конев cv time тематическая подборка статей: дискриминативные модели свежая подборка статей о методах улучшения взаимодействия текста и изображений в мультимодальных моделях. в центре внимания — файнтюн clip для понимания отрицаний, новые подходы к retrieval, оптимизации архитектур vision transformer и многое другое. дообучение clip-моделей tng-clip: training-time negation data generation for negation awareness of clip предлагают пайплайн файнтюна текстовой части clip на понимание отрицаний: на лету для батча генерируют новые тексты, содержащие отрицания, используя тексты с похожих картинок для усложнения задачи. также показывают, что можно подменить текстовый энкодер в предобученной диффузионной модели, и генерации с отрицаниями в промпте тоже станут лучше. visualized text-to-image retrieval авторы говорят, что вместо text-to-image retrieval можно сначала сгенерировать картинку по текстовому запросу, а потом уже делать image-to-image retrieval чисто по картиночным фичам. тестируются на специфических постановках задач типа rag, но идея интересная. hard negative contrastive learning for fine-grained geometric understanding in large multimodal models доливают в обучение clip датасет с геометрией и используют полученную модель как энкодер в vlm. геометрические датасеты добавляют и в другие стадии обучения vlm, но основная новизна в том, как сделать файнтюн на геометрию в clip-постановке. distill clip (dclip): enhancing image-text retrieval via cross-modal transformer distillation для дообучения clip собирают модель-учитель, которая извлекает картиночные фичи по выделенным через yolo областям и агрегирует их через cross-attention с текстовыми фичами; затем этот учитель используется для дистилляции. с ростом качества на retrieval-задачах метод просаживает точность zero-shot-классификации. vision transformers with self-distilled registers изучают проблему токенов-аутлаеров в трансформерных моделях, описанную в статье vision transformers need registers . в ней предложили на вход модели подавать токены-регистры. также авторы пишут, что такие токены можно добавлять в уже обученную модель и файнтюнить её так, чтобы аутлаеры «перетекали» в добавленные токены. архитектура дискриминативных моделей taming transformer without using learning rate warmup связывают нестабильность в обучении трансформеров с тем, что матрица аттеншена становится низкоранговой и разреженной. предлагают добавить в adam ограничение на learning rate для апдейтов, которые имеют высокую спектральную норму по сравнению с текущей матрицей. показывают, что в этом случае возможно обучение без lr-warmup&amp;#x27;а. repavit: scalable vision transformer acceleration via structural reparameterization on feedforward network layers применяют идеи из shufflenet к vit: в ffn-блоке делают нелинейность только для части нейронов промежуточного слоя — вторую часть можно после обучения вмерджить в одну линейную операцию. также заменяют layernorm на batchnorm и его тоже вмердживают после обучения. но тестируют всё это только на imagenet, есть подозрение, что на более сложных датасетах профита не будет. textregion: text-aligned region tokens from frozen image-text models решают zero-shot-сегментацию и смежные задачи, предлагают пайплайн, в котором объединяют sam и clip-модель: через sam находят области с объектами, и в clip-модели модифицируют аттеншен последнего слоя, чтобы он смотрел на каждую область по отдельности — таким образом получают токены для областей, которые уже можно сопоставлять с текстовыми представлениями класса и делать сегментацию. ren: fast and efficient region encodings from patch-based image encoders отличие от предыдущей статьи в том, что теперь область интереса на картинке кодируем координатами точки. имея предобученный бэкбон, добавляем к нему голову, которая по координатам точки смотрит на карту признаков и возвращает эмбеддинг соответствующего ей объекта; sam теперь используется только на этапе обучения. подборку подготовил ❣ артём конев cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-07-17T07:48:44+00:00" href="./posts/167.html">2025-07-17 07:48 UTC</a></div>
      </div>
      <div class="post-body"><strong>Тематическая подборка статей: дискриминативные модели<br></strong><br>Свежая подборка статей о методах улучшения взаимодействия текста и изображений в мультимодальных моделях. В центре внимания — файнтюн CLIP для понимания отрицаний, новые подходы к retrieval, оптимизации архитектур Vision Transformer и многое другое.<br><br><strong>Дообучение CLIP-моделей<br></strong> <br><a href="https://arxiv.org/abs/2505.18434" rel="nofollow noopener noreferrer"><strong>TNG-CLIP: Training-Time Negation Data Generation for Negation Awareness of CLIP</strong></a><strong><br></strong><br>Предлагают пайплайн файнтюна текстовой части CLIP на понимание отрицаний: на лету для батча генерируют новые тексты, содержащие отрицания, используя тексты с похожих картинок для усложнения задачи. Также показывают, что можно подменить текстовый энкодер в предобученной диффузионной модели, и генерации с отрицаниями в промпте тоже станут лучше.<br><br><a href="https://www.arxiv.org/abs/2505.20291" rel="nofollow noopener noreferrer"><strong>Visualized Text-to-Image Retrieval</strong><br></a><br>Авторы говорят, что вместо text-to-image retrieval можно сначала сгенерировать картинку по текстовому запросу, а потом уже делать image-to-image retrieval чисто по картиночным фичам. Тестируются на специфических постановках задач типа RAG, но идея интересная.<br><br><a href="https://arxiv.org/abs/2505.20152" rel="nofollow noopener noreferrer"><strong>Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models</strong></a><strong><br></strong><br>Доливают в обучение CLIP датасет с геометрией и используют полученную модель как энкодер в VLM. Геометрические датасеты добавляют и в другие стадии обучения VLM, но основная новизна в том, как сделать файнтюн на геометрию в CLIP-постановке.<br><br><a href="https://arxiv.org/abs/2505.21549" rel="nofollow noopener noreferrer"><strong>Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal Transformer Distillation</strong></a><br><br>Для дообучения CLIP собирают модель-учитель, которая извлекает картиночные фичи по выделенным через YOLO областям и агрегирует их через cross-attention с текстовыми фичами; затем этот учитель используется для дистилляции. С ростом качества на retrieval-задачах метод просаживает точность zero-shot-классификации.<br><br><a href="https://arxiv.org/abs/2505.21501" rel="nofollow noopener noreferrer"><strong>Vision Transformers with Self-Distilled Registers</strong></a><strong><br></strong><br>Изучают проблему токенов-аутлаеров в трансформерных моделях, описанную в статье <a href="https://arxiv.org/html/2309.16588v2" rel="nofollow noopener noreferrer">Vision Transformers Need Registers</a>. В ней предложили на вход модели подавать токены-регистры. Также авторы пишут, что такие токены можно добавлять в уже обученную модель и файнтюнить её так, чтобы аутлаеры «перетекали» в добавленные токены.<br><br><strong>Архитектура дискриминативных моделей</strong><br> <br><a href="https://arxiv.org/abs/2505.21910" rel="nofollow noopener noreferrer"><strong>Taming Transformer Without Using Learning Rate Warmup</strong></a><br><a href="https://arxiv.org/abs/2505.21910" rel="nofollow noopener noreferrer"><br></a>Связывают нестабильность в обучении трансформеров с тем, что матрица аттеншена становится низкоранговой и разреженной. Предлагают добавить в Adam ограничение на learning rate для апдейтов, которые имеют высокую спектральную норму по сравнению с текущей матрицей. Показывают, что в этом случае возможно обучение без lr-warmup&#x27;а.<br><br><a href="https://arxiv.org/abs/2505.21847" rel="nofollow noopener noreferrer"><strong>RePaViT: Scalable Vision Transformer Acceleration via Structural Reparameterization on Feedforward Network Layers</strong></a><br><br>Применяют идеи из ShuffleNet к ViT: в FFN-блоке делают нелинейность только для части нейронов промежуточного слоя — вторую часть можно после обучения вмерджить в одну линейную операцию. Также заменяют LayerNorm на BatchNorm и его тоже вмердживают после обучения. Но тестируют всё это только на ImageNet, есть подозрение, что на более сложных датасетах профита не будет.<br><br><a href="https://arxiv.org/abs/2505.23769" rel="nofollow noopener noreferrer"><strong>TextRegion: Text-Aligned Region Tokens from Frozen Image-Text Models</strong></a><br><br>Решают zero-shot-сегментацию и смежные задачи, предлагают пайплайн, в котором объединяют SAM и CLIP-модель: через SAM находят области с объектами, и в CLIP-модели модифицируют аттеншен последнего слоя, чтобы он смотрел на каждую область по отдельности — таким образом получают токены для областей, которые уже можно сопоставлять с текстовыми представлениями класса и делать сегментацию.<br><br><a href="https://arxiv.org/abs/2505.18153" rel="nofollow noopener noreferrer"><strong>REN: Fast and Efficient Region Encodings from Patch-Based Image Encoders</strong></a><strong><br></strong><br>Отличие от предыдущей статьи в том, что теперь область интереса на картинке кодируем координатами точки. Имея предобученный бэкбон, добавляем к нему голову, которая по координатам точки смотрит на карту признаков и возвращает эмбеддинг соответствующего ей объекта; SAM теперь используется только на этапе обучения.<br><br><em>Подборку подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Артём Конев<br></em><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a></div>
      <div class="actions">
        <span>2 098 просмотров · 27 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/167" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/167.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="157" data-search="впечатления от конференции iclr 2025 iclr 2025 принесла много полезных работ на тему cv. мы попросили инженеров яндекса подвести личные итоги конференции и рассказать, чем она запомнилась. о трендах в индустрии, интересных статьях и многом другом — в наших карточках. работы, которые упоминаются в посте: — building safe and robust ai systems — pursue the nature of intelligence — adam: a method for stochastic optimization — neural machine translation by jointly learning to align and translate — finding outliers using representations typicality estimation — mrag-bench: vision-centric evaluation for retrieval-augmented multimodal models — benchmarking multimodal retrieval augmented generation with dynamic vqa dataset and self-adaptive planning agent — mmsearch: unveiling the potential of large models as multi-modal search engines — mm-embed: universal multimodal retrieval with multimodal llms — vlm в нейро: как мы создавали мультимодальную нейросеть для поиска по картинкам cv time #yaiclr впечатления от конференции iclr 2025 iclr 2025 принесла много полезных работ на тему cv. мы попросили инженеров яндекса подвести личные итоги конференции и рассказать, чем она запомнилась. о трендах в индустрии, интересных статьях и многом другом — в наших карточках. работы, которые упоминаются в посте: — building safe and robust ai systems — pursue the nature of intelligence — adam: a method for stochastic optimization — neural machine translation by jointly learning to align and translate — finding outliers using representations typicality estimation — mrag-bench: vision-centric evaluation for retrieval-augmented multimodal models — benchmarking multimodal retrieval augmented generation with dynamic vqa dataset and self-adaptive planning agent — mmsearch: unveiling the potential of large models as multi-modal search engines — mm-embed: universal multimodal retrieval with multimodal llms — vlm в нейро: как мы создавали мультимодальную нейросеть для поиска по картинкам cv time #yaiclr">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-07-11T11:59:15+00:00" href="./posts/157.html">2025-07-11 11:59 UTC</a></div>
      </div>
      <div class="post-body"><strong>Впечатления от конференции ICLR 2025</strong><br><br>ICLR 2025 принесла много полезных работ на тему CV. Мы попросили инженеров Яндекса подвести личные итоги конференции и рассказать, чем она запомнилась. О трендах в индустрии, интересных статьях и многом другом — в наших карточках.<br><br>Работы, которые упоминаются в посте:<br>— <a href="https://iclr.cc/virtual/2025/invited-talk/36782" rel="nofollow noopener noreferrer">Building Safe and Robust AI Systems </a><br>— <a href="https://iclr.cc/virtual/2025/invited-talk/36785" rel="nofollow noopener noreferrer">Pursue the Nature of Intelligence</a><br>— <a href="https://arxiv.org/abs/1412.6980" rel="nofollow noopener noreferrer">Adam: A Method for Stochastic Optimization</a><br>— <a href="https://arxiv.org/abs/1409.0473" rel="nofollow noopener noreferrer">Neural Machine Translation by Jointly Learning to Align and Translate</a><br>— <a href="https://arxiv.org/abs/2410.01322" rel="nofollow noopener noreferrer">Finding Outliers Using Representations Typicality Estimation</a><br>— <a href="https://arxiv.org/pdf/2410.08182" rel="nofollow noopener noreferrer">MRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented Multimodal Models</a><br>— <a href="https://arxiv.org/pdf/2411.02937" rel="nofollow noopener noreferrer">Benchmarking Multimodal Retrieval Augmented Generation with Dynamic VQA Dataset and Self-adaptive Planning Agent</a><br>— <a href="https://arxiv.org/pdf/2409.12959" rel="nofollow noopener noreferrer">MMSEARCH: Unveiling the Potential of Large Models as Multi-modal Search Engines</a><br>— <a href="https://arxiv.org/abs/2411.02571" rel="nofollow noopener noreferrer">MM-Embed: Universal Multimodal Retrieval with Multimodal LLMs</a><br>— <a href="https://habr.com/ru/companies/yandex/articles/847706/" rel="nofollow noopener noreferrer">VLM в Нейро: как мы создавали мультимодальную нейросеть для поиска по картинкам</a><br><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><br><br>#YaICLR<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/157_480.webp" srcset="../assets/media/thumbs/157_480.webp 480w, ../assets/media/157.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="157" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/158_480.webp" srcset="../assets/media/thumbs/158_480.webp 480w, ../assets/media/158.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="157" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/159_480.webp" srcset="../assets/media/thumbs/159_480.webp 480w, ../assets/media/159.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="157" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/160_480.webp" srcset="../assets/media/thumbs/160_480.webp 480w, ../assets/media/160.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="157" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/161_480.webp" srcset="../assets/media/thumbs/161_480.webp 480w, ../assets/media/161.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="157" data-image-index="4" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/162_480.webp" srcset="../assets/media/thumbs/162_480.webp 480w, ../assets/media/162.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="157" data-image-index="5" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/163_480.webp" srcset="../assets/media/thumbs/163_480.webp 480w, ../assets/media/163.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="157" data-image-index="6" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/164_480.webp" srcset="../assets/media/thumbs/164_480.webp 480w, ../assets/media/164.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="157" data-image-index="7" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/165_480.webp" srcset="../assets/media/thumbs/165_480.webp 480w, ../assets/media/165.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="157" data-image-index="8" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/166_480.webp" srcset="../assets/media/thumbs/166_480.webp 480w, ../assets/media/166.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="157" data-image-index="9" /></div></div>
      <div class="actions">
        <span>1 895 просмотров · 21 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/157" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/157.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="150" data-search="iclr 2025: полезные статьи на тему cv конференция прошла, а интересные статьи, которые мы не успели упомянуть в наших подборках, — остались. александр шишеня, ведущий разработчик службы компьютерного зрения, отобрал и прокомментировал несколько работ, заслуживающих внимания. symbolic reasoning about llms - подход ctrl-g позволяет модели генерировать ответ, который подчиняется жёстким условиям (например, валидный json). основан на использовании детерминистического конечного автомата и скрытой марковской цепи в дополнение к обученной llm. - можно навешивать мягкие ограничения в виде дополнительной llm, заточенной на сдвиг генерации в нужное направление (например, убирать токсичность). neural networks as graphs используют графовую нейросеть для генерации апдейтов весов сети при обучении. лучший результат получается, если чередовать такие нейросетевые апдейты с итерациями adam. один из авторов работы — борис князев. training language models in academia: challenge or calling? у академии на несколько порядков меньше ресурсов, чем у индустрии. какую же роль в таком случае может играть академия в современном dl? автор даёт свой ответ: возможностей академии хватает, чтобы делать полезный ресерч, а жёсткие ограничения диктуют направление развития — это оптимизация ресурсов и поиск подходов по ускорению обучения. в качестве доказательства приводится список работ best paper awards icml 2025, где большинство работ выполнено академией. сомнительное доказательство — ведь можно предположить, что индустрии просто не так важно публиковаться, да и коммерческую тайну никто не отменял. how much is a noisy image worth? data scaling laws for ambient diffusion эффективно используются шумные данные для обучения диффузии. выведен специальный лосс, который применяется к шумным сэмплам, а для чистых данных используется обычный лосс. hart: efficient visual generation with hybrid autoregressive transformer статья от mit и nvidia. предлагается картиночный токенизатор, который генерирует дискретные токены и непрерывные поправки к ним. далее дискретные токены предсказываются авторегрессионной моделью, а непрерывные — легковесной диффузионной моделью. zigzag diffusion sampling: diffusion models can self-improve via self-reflection улучшают качество генерации изображений, чередуя прямую генерацию с высоким гайденсом и обратную генерацию с низким гайденсом. gooddrag: towards good practices for drag editing with diffusion models редактирование изображений с помощью варпа. фишка в том, что итерации варпа и денойзинга применяются попеременно — это позволяет достичь лучшего качества, чем последовательное применение сначала полного варпа, а потом расшумления. test-time alignment of diffusion models without reward over-optimization элайнмент диффузионной модели на этапе сэмплирования. rl-objective можно явно оптимизировать и выразить целевую плотность вероятности через плотность вероятности претренированной модели и реворд-функцию. дальше сэмплируются сразу несколько траекторий, попутно отсеивая траектории с низким ревордом, добавляя новые и постепенно уменьшая силу гайденса. cv time #yaiclr iclr 2025: полезные статьи на тему cv конференция прошла, а интересные статьи, которые мы не успели упомянуть в наших подборках, — остались. александр шишеня, ведущий разработчик службы компьютерного зрения, отобрал и прокомментировал несколько работ, заслуживающих внимания. symbolic reasoning about llms - подход ctrl-g позволяет модели генерировать ответ, который подчиняется жёстким условиям (например, валидный json). основан на использовании детерминистического конечного автомата и скрытой марковской цепи в дополнение к обученной llm. - можно навешивать мягкие ограничения в виде дополнительной llm, заточенной на сдвиг генерации в нужное направление (например, убирать токсичность). neural networks as graphs используют графовую нейросеть для генерации апдейтов весов сети при обучении. лучший результат получается, если чередовать такие нейросетевые апдейты с итерациями adam. один из авторов работы — борис князев. training language models in academia: challenge or calling? у академии на несколько порядков меньше ресурсов, чем у индустрии. какую же роль в таком случае может играть академия в современном dl? автор даёт свой ответ: возможностей академии хватает, чтобы делать полезный ресерч, а жёсткие ограничения диктуют направление развития — это оптимизация ресурсов и поиск подходов по ускорению обучения. в качестве доказательства приводится список работ best paper awards icml 2025, где большинство работ выполнено академией. сомнительное доказательство — ведь можно предположить, что индустрии просто не так важно публиковаться, да и коммерческую тайну никто не отменял. how much is a noisy image worth? data scaling laws for ambient diffusion эффективно используются шумные данные для обучения диффузии. выведен специальный лосс, который применяется к шумным сэмплам, а для чистых данных используется обычный лосс. hart: efficient visual generation with hybrid autoregressive transformer статья от mit и nvidia. предлагается картиночный токенизатор, который генерирует дискретные токены и непрерывные поправки к ним. далее дискретные токены предсказываются авторегрессионной моделью, а непрерывные — легковесной диффузионной моделью. zigzag diffusion sampling: diffusion models can self-improve via self-reflection улучшают качество генерации изображений, чередуя прямую генерацию с высоким гайденсом и обратную генерацию с низким гайденсом. gooddrag: towards good practices for drag editing with diffusion models редактирование изображений с помощью варпа. фишка в том, что итерации варпа и денойзинга применяются попеременно — это позволяет достичь лучшего качества, чем последовательное применение сначала полного варпа, а потом расшумления. test-time alignment of diffusion models without reward over-optimization элайнмент диффузионной модели на этапе сэмплирования. rl-objective можно явно оптимизировать и выразить целевую плотность вероятности через плотность вероятности претренированной модели и реворд-функцию. дальше сэмплируются сразу несколько траекторий, попутно отсеивая траектории с низким ревордом, добавляя новые и постепенно уменьшая силу гайденса. cv time #yaiclr">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-07-08T08:02:44+00:00" href="./posts/150.html">2025-07-08 08:02 UTC</a></div>
      </div>
      <div class="post-body"><strong>ICLR 2025: полезные статьи на тему CV<br></strong><br>Конференция прошла, а интересные статьи, которые мы не успели упомянуть в наших подборках, — остались. Александр Шишеня, ведущий разработчик службы компьютерного зрения, отобрал и прокомментировал несколько работ, заслуживающих внимания.<br><br><a href="https://web.cs.ucla.edu/~guyvdb/slides/LLMCP24.pdf" rel="nofollow noopener noreferrer"><strong>Symbolic reasoning about LLMs</strong></a><br><a href="https://web.cs.ucla.edu/~guyvdb/slides/LLMCP24.pdf" rel="nofollow noopener noreferrer"><strong><br></strong></a>- Подход <a href="https://arxiv.org/pdf/2406.13892" rel="nofollow noopener noreferrer">Ctrl-G</a> позволяет модели генерировать ответ, который подчиняется жёстким условиям (например, валидный JSON). Основан на использовании детерминистического конечного автомата и скрытой марковской цепи в дополнение к обученной LLM.<br>- Можно навешивать мягкие ограничения в виде дополнительной LLM, заточенной на сдвиг генерации в нужное направление (например, убирать токсичность).<br><br><a href="https://iclr.cc/virtual/2025/10000308" rel="nofollow noopener noreferrer"><strong>Neural Networks as Graphs</strong></a><br><br>Используют графовую нейросеть для генерации апдейтов весов сети при обучении. Лучший результат получается, если чередовать такие нейросетевые апдейты с итерациями Adam. Один из авторов работы — Борис Князев. <br><br><a href="https://iclr.cc/virtual/2025/invited-talk/10000532" rel="nofollow noopener noreferrer"><strong>Training Language Models in Academia: Challenge or Calling?</strong></a><br><br>У академии на несколько порядков меньше ресурсов, чем у индустрии. Какую же роль в таком случае может играть академия в современном DL? Автор даёт свой ответ: возможностей академии хватает, чтобы делать полезный ресерч, а жёсткие ограничения диктуют направление развития — это оптимизация ресурсов и поиск подходов по ускорению обучения. В качестве доказательства приводится список работ Best Paper Awards ICML 2025, где большинство работ выполнено академией. Сомнительное доказательство — ведь можно предположить, что индустрии просто не так важно публиковаться, да и коммерческую тайну никто не отменял.<br><br><a href="https://arxiv.org/abs/2411.02780" rel="nofollow noopener noreferrer"><strong>How much is a noisy image worth? Data Scaling Laws for Ambient Diffusion</strong></a><br><br>Эффективно используются шумные данные для обучения диффузии. Выведен специальный лосс, который применяется к шумным сэмплам, а для чистых данных используется обычный лосс.<br><br><a href="https://arxiv.org/html/2410.10812v1" rel="nofollow noopener noreferrer"><strong>HART: Efficient Visual Generation with Hybrid Autoregressive Transformer</strong></a><br><br>Статья от MIT и NVIDIA. Предлагается картиночный токенизатор, который генерирует дискретные токены и непрерывные поправки к ним. Далее дискретные токены предсказываются авторегрессионной моделью, а непрерывные — легковесной диффузионной моделью.<br><br><a href="https://arxiv.org/abs/2412.10891" rel="nofollow noopener noreferrer"><strong>Zigzag Diffusion Sampling: Diffusion Models Can Self-Improve via Self-Reflection</strong></a><br><br>Улучшают качество генерации изображений, чередуя прямую генерацию с высоким гайденсом и обратную генерацию с низким гайденсом.<br><br><a href="https://arxiv.org/abs/2404.07206" rel="nofollow noopener noreferrer"><strong>GoodDrag: Towards Good Practices for Drag Editing with Diffusion Models</strong></a><br><br>Редактирование изображений с помощью варпа. Фишка в том, что итерации варпа и денойзинга применяются попеременно — это позволяет достичь лучшего качества, чем последовательное применение сначала полного варпа, а потом расшумления.<br><br><a href="https://arxiv.org/abs/2501.05803" rel="nofollow noopener noreferrer"><strong>Test-time Alignment of Diffusion Models without Reward Over-optimization</strong></a><br><br>Элайнмент диффузионной модели на этапе сэмплирования. RL-Objective можно явно оптимизировать и выразить целевую плотность вероятности через плотность вероятности претренированной модели и реворд-функцию. Дальше сэмплируются сразу несколько траекторий, попутно отсеивая траектории с низким ревордом, добавляя новые и постепенно уменьшая силу гайденса.<br><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><br><br>#YaICLR<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/150_480.webp" srcset="../assets/media/thumbs/150_480.webp 480w, ../assets/media/150.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="150" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/151_480.webp" srcset="../assets/media/thumbs/151_480.webp 480w, ../assets/media/151.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="150" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/152_480.webp" srcset="../assets/media/thumbs/152_480.webp 480w, ../assets/media/152.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="150" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/153_480.webp" srcset="../assets/media/thumbs/153_480.webp 480w, ../assets/media/153.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="150" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/154_480.webp" srcset="../assets/media/thumbs/154_480.webp 480w, ../assets/media/154.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="150" data-image-index="4" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/155_480.webp" srcset="../assets/media/thumbs/155_480.webp 480w, ../assets/media/155.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="150" data-image-index="5" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/156_480.webp" srcset="../assets/media/thumbs/156_480.webp 480w, ../assets/media/156.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="150" data-image-index="6" /></div></div>
      <div class="actions">
        <span>1 750 просмотров · 19 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/150" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/150.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="149" data-search="что читает команда распознавания текста в vlm: подборка актуальных статей инженеры vlm-команды яндекса поделились статьями, которые они в последнее время читали и обсуждали. в сегодняшней подборке: новые подходы к генерации инфографики, свежие бенчмарки для мультимодальных моделей, работающие пайплайны генерации кода по графику и попытки добавить зрение в диффузионки. chartgalaxy: a dataset for infographic chart understanding and generation статья о том, как сгенерировать около миллиона инфографик. авторы подробно описали каждую стадию процесса: сбор шаблонов, индексирование описаний, иконок и других элементов для заполнения шаблонов, фильтрацию и проверку качества. infochartqa: a benchmark for multimodal question answering on infographic charts авторы собрали новый бенчмарк позволяющий проверить, как vlm-модели понимают инфографику. для каждой инфографики сделали упрощённую версию в виде обычного графика с теми же данными — модели справляются с таким заметно лучше, чем с визуально перегруженным оригиналом. также добавили новый тип вопросов по отдельным кропам из изображения инфографики — на понимание мелких визуальных деталей. chartcoder: advancing multimodal large language model for chart-to-code generation авторы обучили модель понимать графики: она получает изображение и возвращает код на python (matplotlib), чтобы построить такой же график. для этого использовали стратегию snippet-of-thoughts (sot) — пошаговое рассуждение перед финальной генерацией кода. взяли llm, способную писать код, собрали датасет под задачу (160 тысяч картинок, на каждую — один вопрос и ответ). кратко описали пайплайн его создания. модель показывает лучшие результаты среди аналогов такого же размера (включая почти самые свежие qwen и internvl). в ablation-экспериментах дообучили qwen на своём датасете — получили прирост; 384 px + anyres почти хватает для большинства графиков. relation-rich visual document generator for visual information extraction статья с cvpr 2025 о генерации синтетических text-rich-документов с логической структурой (таких, как формы). пайплайн генерации любопытен тем, что в нём сначала генерируют текст с помощью chatgpt, а уже потом — структуру документа (laytout). чаще встречается обратный вариант, когда структуру документа заполняют текстом. авторы показывают, что обучение qwen2-vl и llava-next-mistral на таких данных улучшает метрики распознавания текста и извлечения информации на публичных бенчмарках. llada-v: large language diffusion models with visual instruction tuning авторы попытались расширить предобученную текстовую диффузию llada на мультимодальность, добавив визуальный вход через siglip2 и mlp-проекцию в языковое пространство. итоговая модель зафайнтюнена на визуальных и reasoning-focused-инструкциях mammoth-vl и visualwebinstruct и бьёт автогрессионные и диффузионные бейзлайны по ряду мультидисциплинарных и визуально-математических бенчмарков. sft memorizes, rl generalizes: a comparative study of foundation model post-training интересная статья, авторы которой подтверждают тезис из названия: sft хорошо запоминает жёсткие форматы и правила, но плохо справляется с out-of-distribution-задачами. в то же время rl реально улучшает генерализацию и показывает заметный прирост на ood-случаях. но sft всё равно нужен, чтобы rl вообще завёлся. в противном случае модель не умеет нормально реагировать на инструкции или генерирует неконтролируемый выход. rl-обучение не получает положительного сигнала. это справедливо как для llm, так и для vlm. подборку подготовила ❣ команда распознавания текста в vlm cv time что читает команда распознавания текста в vlm: подборка актуальных статей инженеры vlm-команды яндекса поделились статьями, которые они в последнее время читали и обсуждали. в сегодняшней подборке: новые подходы к генерации инфографики, свежие бенчмарки для мультимодальных моделей, работающие пайплайны генерации кода по графику и попытки добавить зрение в диффузионки. chartgalaxy: a dataset for infographic chart understanding and generation статья о том, как сгенерировать около миллиона инфографик. авторы подробно описали каждую стадию процесса: сбор шаблонов, индексирование описаний, иконок и других элементов для заполнения шаблонов, фильтрацию и проверку качества. infochartqa: a benchmark for multimodal question answering on infographic charts авторы собрали новый бенчмарк позволяющий проверить, как vlm-модели понимают инфографику. для каждой инфографики сделали упрощённую версию в виде обычного графика с теми же данными — модели справляются с таким заметно лучше, чем с визуально перегруженным оригиналом. также добавили новый тип вопросов по отдельным кропам из изображения инфографики — на понимание мелких визуальных деталей. chartcoder: advancing multimodal large language model for chart-to-code generation авторы обучили модель понимать графики: она получает изображение и возвращает код на python (matplotlib), чтобы построить такой же график. для этого использовали стратегию snippet-of-thoughts (sot) — пошаговое рассуждение перед финальной генерацией кода. взяли llm, способную писать код, собрали датасет под задачу (160 тысяч картинок, на каждую — один вопрос и ответ). кратко описали пайплайн его создания. модель показывает лучшие результаты среди аналогов такого же размера (включая почти самые свежие qwen и internvl). в ablation-экспериментах дообучили qwen на своём датасете — получили прирост; 384 px + anyres почти хватает для большинства графиков. relation-rich visual document generator for visual information extraction статья с cvpr 2025 о генерации синтетических text-rich-документов с логической структурой (таких, как формы). пайплайн генерации любопытен тем, что в нём сначала генерируют текст с помощью chatgpt, а уже потом — структуру документа (laytout). чаще встречается обратный вариант, когда структуру документа заполняют текстом. авторы показывают, что обучение qwen2-vl и llava-next-mistral на таких данных улучшает метрики распознавания текста и извлечения информации на публичных бенчмарках. llada-v: large language diffusion models with visual instruction tuning авторы попытались расширить предобученную текстовую диффузию llada на мультимодальность, добавив визуальный вход через siglip2 и mlp-проекцию в языковое пространство. итоговая модель зафайнтюнена на визуальных и reasoning-focused-инструкциях mammoth-vl и visualwebinstruct и бьёт автогрессионные и диффузионные бейзлайны по ряду мультидисциплинарных и визуально-математических бенчмарков. sft memorizes, rl generalizes: a comparative study of foundation model post-training интересная статья, авторы которой подтверждают тезис из названия: sft хорошо запоминает жёсткие форматы и правила, но плохо справляется с out-of-distribution-задачами. в то же время rl реально улучшает генерализацию и показывает заметный прирост на ood-случаях. но sft всё равно нужен, чтобы rl вообще завёлся. в противном случае модель не умеет нормально реагировать на инструкции или генерирует неконтролируемый выход. rl-обучение не получает положительного сигнала. это справедливо как для llm, так и для vlm. подборку подготовила ❣ команда распознавания текста в vlm cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-07-01T08:35:01+00:00" href="./posts/149.html">2025-07-01 08:35 UTC</a></div>
      </div>
      <div class="post-body"><strong>Что читает команда распознавания текста в VLM: подборка актуальных статей </strong><br><br>Инженеры VLM-команды Яндекса поделились статьями, которые они в последнее время читали и обсуждали. В сегодняшней подборке: новые подходы к генерации инфографики, свежие бенчмарки для мультимодальных моделей, работающие пайплайны генерации кода по графику и попытки добавить зрение в диффузионки. <br><br><a href="https://arxiv.org/abs/2505.18668" rel="nofollow noopener noreferrer"><strong>ChartGalaxy: A Dataset for Infographic Chart Understanding and Generation</strong></a><br>Статья о том, как сгенерировать около миллиона инфографик. Авторы подробно описали каждую стадию процесса: сбор шаблонов, индексирование описаний, иконок и других элементов для заполнения шаблонов, фильтрацию и проверку качества.<br><br><a href="https://arxiv.org/abs/2505.19028" rel="nofollow noopener noreferrer"><strong>InfoChartQA: A Benchmark for Multimodal Question Answering on Infographic Charts</strong></a><br>Авторы собрали новый бенчмарк позволяющий проверить, как VLM-модели понимают инфографику. Для каждой инфографики сделали упрощённую версию в виде обычного графика с теми же данными — модели справляются с таким заметно лучше, чем с визуально перегруженным оригиналом. Также добавили новый тип вопросов по отдельным кропам из изображения инфографики — на понимание мелких визуальных деталей.<br><br><a href="https://arxiv.org/abs/2501.06598" rel="nofollow noopener noreferrer"><strong>ChartCoder: Advancing Multimodal Large Language Model for Chart-to-Code Generation</strong></a><br>Авторы обучили модель понимать графики: она получает изображение и возвращает код на Python (Matplotlib), чтобы построить такой же график. Для этого использовали стратегию Snippet-of-Thoughts (SoT) — пошаговое рассуждение перед финальной генерацией кода. Взяли LLM, способную писать код, собрали датасет под задачу (160 тысяч картинок, на каждую — один вопрос и ответ). Кратко описали пайплайн его создания. Модель показывает лучшие результаты среди аналогов такого же размера (включая почти самые свежие Qwen и InternVL). В ablation-экспериментах дообучили Qwen на своём датасете — получили прирост; 384 px + Anyres почти хватает для большинства графиков.<br><br><a href="https://arxiv.org/pdf/2504.10659" rel="nofollow noopener noreferrer"><strong>Relation-Rich Visual Document Generator for Visual Information Extraction </strong></a><br>Статья с CVPR 2025 о генерации синтетических text-rich-документов с логической структурой (таких, как формы). Пайплайн генерации любопытен тем, что в нём сначала генерируют текст с помощью ChatGPT, а уже потом — структуру документа (laytout). Чаще встречается обратный вариант, когда структуру документа заполняют текстом. Авторы показывают, что обучение Qwen2-VL и Llava-NexT-mistral на таких данных улучшает метрики распознавания текста и извлечения информации на публичных бенчмарках.<br><br><a href="https://www.arxiv.org/abs/2505.16933" rel="nofollow noopener noreferrer"><strong>LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning</strong></a><br>Авторы попытались расширить предобученную текстовую диффузию LLaDA на мультимодальность, добавив визуальный вход через SigLIP2 и MLP-проекцию в языковое пространство. Итоговая модель зафайнтюнена на визуальных и reasoning-focused-инструкциях MAmmoTH-VL и VisualWebInstruct и бьёт автогрессионные и диффузионные бейзлайны по ряду мультидисциплинарных и визуально-математических бенчмарков.<br><br><a href="https://arxiv.org/abs/2501.17161" rel="nofollow noopener noreferrer"><strong>SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training</strong></a><br>Интересная статья, авторы которой подтверждают тезис из названия: SFT хорошо запоминает жёсткие форматы и правила, но плохо справляется с out-of-distribution-задачами. В то же время RL реально улучшает генерализацию и показывает заметный прирост на OOD-случаях. Но SFT всё равно нужен, чтобы RL вообще завёлся. В противном случае модель не умеет нормально реагировать на инструкции или генерирует неконтролируемый выход. RL-обучение не получает положительного сигнала. Это справедливо как для LLM, так и для VLM.<br><br><em>Подборку подготовила </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Команда распознавания текста в VLM</em><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a></div>
      <div class="actions">
        <span>4 166 просмотров · 40 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/149" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/149.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="148" data-search="scaling vision pre-training to 4k resolution большинство доступных визуальных энкодеров предобучено на изображениях низкого разрешения: например, на 378✕378, как siglip. это становится серьёзной проблемой, если вы хотите обрабатывать изображения высокого разрешения с мелкими деталями. дорожный знак stop будет неразличим, если сжать кадры записи видеорегистратора до 378✕378. то же касается распознавания текста, где много мелких деталей. авторы сегодняшней статьи отмечают, что в индустрии уже борются с этой проблемой. методом anyres режут большое изображение на части поменьше — тайлы без пересечений. или, как в s2, одновременно ресайзят изображение до нужного размера и делят его оригинал на тайлы, чтобы добавить каналы для описания одних и тех же участков изображения в более высоком разрешении. но эти методы кодируют картинку заранее — не учитывая запрос пользователя. логично предположить, что для вопроса, например, про одежду человека, не нужно кодировать автомобили и здания. новое решение, которое предлагают авторы сегодняшней статьи, учитывает промпты пользователя. они предлагают подбирать куски изображения, которые подходят под запрос, и подмешивать их в инпут. сделать это можно в два шага: 1. предобучить энкодер ps3, который сможет угадывать подходящие области изображения; 2. обучить vlm отвечать на запросы пользователя вместе c энкодером ps3. а если промпта нет и top-down-selection невозможен, можно подключить bottom-up-selection: попросить нейросеть самостоятельно выбрать интересные области. «интересность» при этом определяется данными, на которых обучалась модель. архитектура ps3 изображена на схеме. на входе — предобученный siglip. энкодим им изображение и получаем low-res-фичи. из-за ресайза теряются все высокоуровневые фичи. авторы предлагают исправить это с помощью дополнительного так называемого light-weight-high-res-энкодера (обучаемая урезанная cnn). третьей фичой будет либо эмбеддинг текста, чтобы выбрать интересный образ, либо обучаемый эмбеддинг, который заменит промпт. по этой тройке для каждой позиции предсказывается вероятность её релевантности: вырезают топ-k областей и энкодят через siglip (несколько раз в разных разрешениях). итоговые фичи картинки — исходные low-res и вырезанные топ-k областей. чтобы подключить ps3 к vlm, понадобится llm: достаточно передать последний токен из запроса к ней в ps3. отобрав топ выученных с энкодом позиционных эмбеддов, можно переходить к тренировке language modeling. для эффективного обучения vlm вместе с ps3 нужно дотюнить выбор региона, чтобы подмена не ощущалась. а дальше можно тренировать модель как обычно. модель, которая получилась после подключения ps3 к мультимодальной llm, авторы назвали vila-hd. по их замерам, она значительно превосходит по качеству anyres и s2, используя при этом в 4,3 раза меньше токенов. разбор подготовил ❣ егор шестопалов cv time scaling vision pre-training to 4k resolution большинство доступных визуальных энкодеров предобучено на изображениях низкого разрешения: например, на 378✕378, как siglip. это становится серьёзной проблемой, если вы хотите обрабатывать изображения высокого разрешения с мелкими деталями. дорожный знак stop будет неразличим, если сжать кадры записи видеорегистратора до 378✕378. то же касается распознавания текста, где много мелких деталей. авторы сегодняшней статьи отмечают, что в индустрии уже борются с этой проблемой. методом anyres режут большое изображение на части поменьше — тайлы без пересечений. или, как в s2 , одновременно ресайзят изображение до нужного размера и делят его оригинал на тайлы, чтобы добавить каналы для описания одних и тех же участков изображения в более высоком разрешении. но эти методы кодируют картинку заранее — не учитывая запрос пользователя. логично предположить, что для вопроса, например, про одежду человека, не нужно кодировать автомобили и здания. новое решение, которое предлагают авторы сегодняшней статьи, учитывает промпты пользователя. они предлагают подбирать куски изображения, которые подходят под запрос, и подмешивать их в инпут. сделать это можно в два шага: 1. предобучить энкодер ps3, который сможет угадывать подходящие области изображения; 2. обучить vlm отвечать на запросы пользователя вместе c энкодером ps3. а если промпта нет и top-down-selection невозможен, можно подключить bottom-up-selection: попросить нейросеть самостоятельно выбрать интересные области. «интересность» при этом определяется данными, на которых обучалась модель. архитектура ps3 изображена на схеме. на входе — предобученный siglip. энкодим им изображение и получаем low-res-фичи. из-за ресайза теряются все высокоуровневые фичи. авторы предлагают исправить это с помощью дополнительного так называемого light-weight-high-res-энкодера (обучаемая урезанная cnn). третьей фичой будет либо эмбеддинг текста, чтобы выбрать интересный образ, либо обучаемый эмбеддинг, который заменит промпт. по этой тройке для каждой позиции предсказывается вероятность её релевантности: вырезают топ-k областей и энкодят через siglip (несколько раз в разных разрешениях). итоговые фичи картинки — исходные low-res и вырезанные топ-k областей. чтобы подключить ps3 к vlm, понадобится llm: достаточно передать последний токен из запроса к ней в ps3. отобрав топ выученных с энкодом позиционных эмбеддов, можно переходить к тренировке language modeling. для эффективного обучения vlm вместе с ps3 нужно дотюнить выбор региона, чтобы подмена не ощущалась. а дальше можно тренировать модель как обычно. модель, которая получилась после подключения ps3 к мультимодальной llm, авторы назвали vila-hd. по их замерам, она значительно превосходит по качеству anyres и s2, используя при этом в 4,3 раза меньше токенов. разбор подготовил ❣ егор шестопалов cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-06-26T08:04:55+00:00" href="./posts/148.html">2025-06-26 08:04 UTC</a></div>
      </div>
      <div class="post-body"><strong>Scaling Vision Pre-Training to 4K Resolution</strong><br><br>Большинство доступных визуальных энкодеров предобучено на изображениях низкого разрешения: например, на 378✕378, как SigLIP. Это становится серьёзной проблемой, если вы хотите обрабатывать изображения высокого разрешения с мелкими деталями. Дорожный знак STOP будет неразличим, если сжать кадры записи видеорегистратора до 378✕378. То же касается распознавания текста, где много мелких деталей.<br><br>Авторы сегодняшней <a href="https://arxiv.org/abs/2503.19903v1" rel="nofollow noopener noreferrer">статьи</a> отмечают, что в индустрии уже борются с этой проблемой. Методом <a href="https://arxiv.org/pdf/2404.16821" rel="nofollow noopener noreferrer">AnyRes</a> режут большое изображение на части поменьше — тайлы без пересечений. Или, как в <a href="https://arxiv.org/pdf/2403.13043v2" rel="nofollow noopener noreferrer">S2</a>, одновременно ресайзят изображение до нужного размера и делят его оригинал на тайлы, чтобы добавить каналы для описания одних и тех же участков изображения в более высоком разрешении. Но эти методы кодируют картинку заранее — не учитывая запрос пользователя. Логично предположить, что для вопроса, например, про одежду человека, не нужно кодировать автомобили и здания.<br><br>Новое решение, которое предлагают авторы сегодняшней статьи, учитывает промпты пользователя. Они предлагают подбирать куски изображения, которые подходят под запрос, и подмешивать их в инпут. Сделать это можно в два шага:<br><br>1. предобучить энкодер PS3, который сможет угадывать подходящие области изображения;<br>2. обучить VLM отвечать на запросы пользователя вместе c энкодером PS3.<br><br>А если промпта нет и top-down-selection невозможен, можно подключить bottom-up-selection: попросить нейросеть самостоятельно выбрать интересные области. «Интересность» при этом определяется данными, на которых обучалась модель.<br><br>Архитектура PS3 изображена на схеме. На входе — предобученный SigLIP. Энкодим им изображение и получаем low-res-фичи. Из-за ресайза теряются все высокоуровневые фичи. Авторы предлагают исправить это с помощью дополнительного так называемого light-weight-high-res-энкодера (обучаемая урезанная CNN). Третьей фичой будет либо эмбеддинг текста, чтобы выбрать интересный образ, либо обучаемый эмбеддинг, который заменит промпт. По этой тройке для каждой позиции предсказывается вероятность её релевантности: вырезают топ-K областей и энкодят через SigLIP (несколько раз в разных разрешениях). <br><br>Итоговые фичи картинки — исходные low-res и вырезанные топ-K областей. Чтобы подключить PS3 к VLM, понадобится LLM: достаточно передать последний токен из запроса к ней в PS3. Отобрав топ выученных с энкодом позиционных эмбеддов, можно переходить к тренировке language modeling.<br><br>Для эффективного обучения VLM вместе с PS3 нужно дотюнить выбор региона, чтобы подмена не ощущалась. А дальше можно тренировать модель как обычно. <br><br>Модель, которая получилась после подключения PS3 к мультимодальной LLM, авторы назвали VILA-HD. По их замерам, она значительно превосходит по качеству AnyRes и S2, используя при этом в 4,3 раза меньше токенов.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Егор Шестопалов</em><br><em><br></em><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/148_480.webp" srcset="../assets/media/thumbs/148_480.webp 480w, ../assets/media/148.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="148" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 004 просмотров · 24 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/148" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/148.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="147" data-search="seedream 3.0 technical report сегодняшняя статья — описание модели seedream 3.0, которая генерирует изображения по текстовым запросам. был момент, когда по замерам artificial analysis arena она обогнала все существующие модели, сейчас — топ-2 модель, уступающая только openai. правда, с нестатзначимой разницей. кажется, что создатели третьей версии seedream проделали огромную техническую работу и потратили очень много человекочасов, разрабатывая свои модели. статья вышла всего лишь через месяц после seedream 2.0, так что её можно воспринимать как набор доработок к прошлой модели, уже неплохо показавшей себя. обучая seedream 3.0, авторы уделили много внимания специфике китайского языка — у многих моделей-конкурентов проблемы с рисованием иероглифов. в частности, обучающие датасеты seedream обогатили набором объектов китайской культуры. ещё одна интересная деталь: после первой стадии обучения на изображениях размером 256 пикселей, модель обучается уже на целом диапазоне разрешений — от 512 до 2048 пикселей. а чтобы выкидывать из обучающего датасета меньше картинок с дефектами и вотермарками, авторы просто маскируют в лоссе проблемные области. в статье упоминается, что авторы обучили собственный vae, но деталей, к сожалению, нет. диффузионный трансформер принимает на вход картинку и закодированный текст, но токены для них обрабатываются отдельными mlp. собственная разработка авторов — расширение 2d rope, которое они назвали scaling rope, позволяет генерировать изображения с размером, отличным от того, на чём обучали модель. стабильность обучения обеспечивает qk-norm. текстовый энкодер дофайнтьюнили из llm, тренируя её на парах текст-изображение. так llm лучше мэтчится с доменом картинок. закодированные текстовые энкодеры она передаёт в диффузионный трансформер. тексты, которые нужно зарендерить на картинке, обрабатывает byt5 — модель работает на уровне unicode. не делит тексты на токены по несколько символов, а кодирует их как последовательность кодов unicode, чтобы генерировать текст было проще. кроме того, в seedream 3.0 авторы использовали новую парадигму ускорения. используя разнообразные техники, такие как квантование, консистентное зашумление и семплирование временных шагов с ранжированием по важности, они достигли существенного ускорения при сохранении качества изображения. а встроенный вывод изображений в высоком разрешении (до 2k), делает новую модель ещё более удобной и практичной. разбор подготовил ❣ артём конев cv time seedream 3.0 technical report сегодняшняя статья — описание модели seedream 3.0, которая генерирует изображения по текстовым запросам. был момент, когда по замерам artificial analysis arena она обогнала все существующие модели, сейчас — топ-2 модель, уступающая только openai. правда, с нестатзначимой разницей. кажется, что создатели третьей версии seedream проделали огромную техническую работу и потратили очень много человекочасов, разрабатывая свои модели. статья вышла всего лишь через месяц после seedream 2.0, так что её можно воспринимать как набор доработок к прошлой модели, уже неплохо показавшей себя. обучая seedream 3.0, авторы уделили много внимания специфике китайского языка — у многих моделей-конкурентов проблемы с рисованием иероглифов. в частности, обучающие датасеты seedream обогатили набором объектов китайской культуры. ещё одна интересная деталь: после первой стадии обучения на изображениях размером 256 пикселей, модель обучается уже на целом диапазоне разрешений — от 512 до 2048 пикселей. а чтобы выкидывать из обучающего датасета меньше картинок с дефектами и вотермарками, авторы просто маскируют в лоссе проблемные области. в статье упоминается, что авторы обучили собственный vae, но деталей, к сожалению, нет. диффузионный трансформер принимает на вход картинку и закодированный текст, но токены для них обрабатываются отдельными mlp. собственная разработка авторов — расширение 2d rope, которое они назвали scaling rope, позволяет генерировать изображения с размером, отличным от того, на чём обучали модель. стабильность обучения обеспечивает qk-norm. текстовый энкодер дофайнтьюнили из llm, тренируя её на парах текст-изображение. так llm лучше мэтчится с доменом картинок. закодированные текстовые энкодеры она передаёт в диффузионный трансформер. тексты, которые нужно зарендерить на картинке, обрабатывает byt5 — модель работает на уровне unicode. не делит тексты на токены по несколько символов, а кодирует их как последовательность кодов unicode, чтобы генерировать текст было проще. кроме того, в seedream 3.0 авторы использовали новую парадигму ускорения. используя разнообразные техники, такие как квантование, консистентное зашумление и семплирование временных шагов с ранжированием по важности, они достигли существенного ускорения при сохранении качества изображения. а встроенный вывод изображений в высоком разрешении (до 2k), делает новую модель ещё более удобной и практичной. разбор подготовил ❣ артём конев cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-06-18T08:02:58+00:00" href="./posts/147.html">2025-06-18 08:02 UTC</a></div>
      </div>
      <div class="post-body"><strong>Seedream 3.0 Technical Report</strong><br><br>Сегодняшняя <a href="https://arxiv.org/abs/2504.11346" rel="nofollow noopener noreferrer">статья</a> — описание модели Seedream 3.0, которая генерирует изображения по текстовым запросам. Был момент, когда по замерам Artificial Analysis Arena она обогнала все существующие модели, сейчас — топ-2 модель, уступающая только OpenAI. Правда, с нестатзначимой разницей.<br><br>Кажется, что создатели третьей версии Seedream проделали огромную техническую работу и потратили очень много человекочасов, разрабатывая свои модели. Статья вышла всего лишь через месяц после Seedream 2.0, так что её можно воспринимать как набор доработок к прошлой модели, уже неплохо показавшей себя.<br><br>Обучая Seedream 3.0, авторы уделили много внимания специфике китайского языка — у многих моделей-конкурентов проблемы с рисованием иероглифов. В частности, обучающие датасеты Seedream обогатили набором объектов китайской культуры. Ещё одна интересная деталь: после первой стадии обучения на изображениях размером 256 пикселей, модель обучается уже на целом диапазоне разрешений — от 512 до 2048 пикселей. А чтобы выкидывать из обучающего датасета меньше картинок с дефектами и вотермарками, авторы просто маскируют в лоссе проблемные области.<br><br>В статье упоминается, что авторы обучили собственный VAE, но деталей, к сожалению, нет. Диффузионный трансформер принимает на вход картинку и закодированный текст, но токены для них обрабатываются отдельными MLP. Собственная разработка авторов — расширение 2D RoPE, которое они назвали Scaling RoPE, позволяет генерировать изображения с размером, отличным от того, на чём обучали модель. Стабильность обучения обеспечивает QK-Norm.<br><br>Текстовый энкодер дофайнтьюнили из LLM, тренируя её на парах текст-изображение. Так LLM лучше мэтчится с доменом картинок. Закодированные текстовые энкодеры она передаёт в диффузионный трансформер.<br><br>Тексты, которые нужно зарендерить на картинке, обрабатывает ByT5 — модель работает на уровне Unicode. Не делит тексты на токены по несколько символов, а кодирует их как последовательность кодов Unicode, чтобы генерировать текст было проще. <br> <br>Кроме того, в Seedream 3.0 авторы использовали новую парадигму ускорения. Используя разнообразные техники, такие как квантование, консистентное зашумление и семплирование временных шагов с ранжированием по важности, они достигли существенного ускорения при сохранении качества изображения. А встроенный вывод изображений в высоком разрешении (до 2K), делает новую модель ещё более удобной и практичной. <br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Артём Конев<br></em><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/147_480.webp" srcset="../assets/media/thumbs/147_480.webp 480w, ../assets/media/147.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="147" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 934 просмотров · 19 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/147" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/147.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="146" data-search="kimi-vl technical report сегодня разбираем статью про kimi-vl — yet another vlm, интересная тем, что умеет понимать очень длинные контексты, активируя всего 2,8b параметров. это не мешает ей получать результаты лучше, чем qwen2.5-vl-7b, and gemma-3-12b-it и даже gpt-4o-mini на некоторых тасках. kimi-vl под силу контексты размером в 128k токенов и работа с изображениями разного разрешения — для этого у неё под капотом специальный визуальный энкодер. авторы говорят, что они разработали две версии нейросети: обычную и thinking, которая кроме всего вышеперечисленного справляется с reasoning — длинными рассуждениями. на картинке — сравнение kimi-vl с другими популярными нейросетями: сколько параметров активируется на бенчмарке mathvision. авторы считают, что будущее — за moe и cot (как у deepseek и других llm), а плотная архитектура, которую использует большинство опенсорс-vlm (например, qwen2.5-vl и gemma-3), устарела. kimi-vl, по их словам, догоняет по способностям llm. на основе siglip-so-400m они создали собственный визуальный энкодер — moonvit. он может обрабатывать картинки разного разрешения: по аналогии с текстовыми последовательностями разбирает их на батчи, вытягивает и превращает в 1d-векторы. чем выше разрешение — тем больше векторов в последовательности. каждый батч локализуют по ширине и по высоте. энкодер и llm соединяет двухслойный mlp. для претрейна используется много текстовых данных: судя по всему, именно это позволяет активировать меньше параметров для её работы. само обучение состоит из нескольких частей: 1. предобучение энкодера vit (2t + 0,1t токенов): moonvit обучается работать с картинками на парах изображение+текст. 2. joint pre-training (1,4t токенов). модель тренируется обрабатывать запросы на чисто текстовых данных. 3. joint cooldown (0,6t токенов). оптимизация производительности модели: обучение на высококачественных языковых и мультимодальных наборах данных. 4. joint long-context (0,3t токенов). увеличение длины контекста модели с 8k до 128k. чтобы модель лучше понимала длинный контекст и одновременно хорошо работала с коротким, на каждом подэтапе этой стадии обучения авторы фильтруют и увеличивают соотношение длинных данных до 25%. хотя текущая модель эффективно справляется со многими стандартными задачами, она всё ещё слишком мала для решения узкоспециализированных задач. возможности рассуждений kimi-vl ещё не достигли теоретического максимума, особенно для сложных задач, требующих многоступенчатых выводов или более глубокого контекстного понимания. путь к преодолению этих сложностей — масштабирование модели и совершенствование алгоритмов обучения (в том числе обогащение и увеличение тренировочных датасетов). разбор подготовила ❣ дарья виноградова cv time kimi-vl technical report сегодня разбираем статью про kimi-vl — yet another vlm, интересная тем, что умеет понимать очень длинные контексты, активируя всего 2,8b параметров. это не мешает ей получать результаты лучше, чем qwen2.5-vl-7b, and gemma-3-12b-it и даже gpt-4o-mini на некоторых тасках. kimi-vl под силу контексты размером в 128k токенов и работа с изображениями разного разрешения — для этого у неё под капотом специальный визуальный энкодер. авторы говорят, что они разработали две версии нейросети: обычную и thinking, которая кроме всего вышеперечисленного справляется с reasoning — длинными рассуждениями. на картинке — сравнение kimi-vl с другими популярными нейросетями: сколько параметров активируется на бенчмарке mathvision. авторы считают, что будущее — за moe и cot (как у deepseek и других llm), а плотная архитектура, которую использует большинство опенсорс-vlm (например, qwen2.5-vl и gemma-3), устарела. kimi-vl, по их словам, догоняет по способностям llm. на основе siglip-so-400m они создали собственный визуальный энкодер — moonvit. он может обрабатывать картинки разного разрешения: по аналогии с текстовыми последовательностями разбирает их на батчи, вытягивает и превращает в 1d-векторы. чем выше разрешение — тем больше векторов в последовательности. каждый батч локализуют по ширине и по высоте. энкодер и llm соединяет двухслойный mlp. для претрейна используется много текстовых данных: судя по всему, именно это позволяет активировать меньше параметров для её работы. само обучение состоит из нескольких частей: 1. предобучение энкодера vit (2t + 0,1t токенов): moonvit обучается работать с картинками на парах изображение+текст. 2. joint pre-training (1,4t токенов). модель тренируется обрабатывать запросы на чисто текстовых данных. 3. joint cooldown (0,6t токенов). оптимизация производительности модели: обучение на высококачественных языковых и мультимодальных наборах данных. 4. joint long-context (0,3t токенов). увеличение длины контекста модели с 8k до 128k. чтобы модель лучше понимала длинный контекст и одновременно хорошо работала с коротким, на каждом подэтапе этой стадии обучения авторы фильтруют и увеличивают соотношение длинных данных до 25%. хотя текущая модель эффективно справляется со многими стандартными задачами, она всё ещё слишком мала для решения узкоспециализированных задач. возможности рассуждений kimi-vl ещё не достигли теоретического максимума, особенно для сложных задач, требующих многоступенчатых выводов или более глубокого контекстного понимания. путь к преодолению этих сложностей — масштабирование модели и совершенствование алгоритмов обучения (в том числе обогащение и увеличение тренировочных датасетов). разбор подготовила ❣ дарья виноградова cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-06-13T09:06:13+00:00" href="./posts/146.html">2025-06-13 09:06 UTC</a></div>
      </div>
      <div class="post-body"><strong>Kimi-VL technical report<br></strong><br>Сегодня разбираем <a href="https://arxiv.org/abs/2504.07491" rel="nofollow noopener noreferrer">статью</a> про Kimi-VL — yet another VLM, интересная тем, что умеет понимать очень длинные контексты, активируя всего 2,8B параметров. Это не мешает ей получать результаты лучше, чем Qwen2.5-VL-7B, and Gemma-3-12B-IT и даже GPT-4o-mini на некоторых тасках.<br><br>Kimi-VL под силу контексты размером в 128K токенов и работа с изображениями разного разрешения — для этого у неё под капотом специальный визуальный энкодер. Авторы говорят, что они разработали две версии нейросети: обычную и thinking, которая кроме всего вышеперечисленного справляется с reasoning — длинными рассуждениями. На картинке — сравнение Kimi-VL с другими популярными нейросетями: сколько параметров активируется на бенчмарке MathVision. <br><br>Авторы считают, что будущее — за MoE и CoT (как у DeepSeek и других LLM), а плотная архитектура, которую использует большинство опенсорс-VLM (например, Qwen2.5-VL и Gemma-3), устарела. <br><br>Kimi-VL, по их словам, догоняет по способностям LLM. На основе SigLIP-SO-400M они создали собственный визуальный энкодер — MoonViT. Он может обрабатывать картинки разного разрешения: по аналогии с текстовыми последовательностями разбирает их на батчи, вытягивает и превращает в 1D-векторы. Чем выше разрешение — тем больше векторов в последовательности. Каждый батч локализуют по ширине и по высоте. Энкодер и LLM соединяет двухслойный MLP. <br>  <br>Для претрейна используется много текстовых данных: судя по всему, именно это позволяет активировать меньше параметров для её работы. Само обучение состоит из нескольких частей: <br><br>1. Предобучение энкодера ViT (2T + 0,1T токенов): MoonViT обучается работать с картинками на парах изображение+текст. <br>2. Joint Pre-training (1,4T токенов). Модель тренируется обрабатывать запросы на чисто текстовых данных.<br>3. Joint Cooldown (0,6T токенов). Оптимизация производительности модели: обучение на высококачественных языковых и мультимодальных наборах данных.<br>4. Joint Long-context (0,3T токенов). Увеличение длины контекста модели с 8K до 128K. Чтобы модель лучше понимала длинный контекст и одновременно хорошо работала с коротким, на каждом подэтапе этой стадии обучения авторы фильтруют и увеличивают соотношение длинных данных до 25%. <br><br>Хотя текущая модель эффективно справляется со многими стандартными задачами, она всё ещё слишком мала для решения узкоспециализированных задач. Возможности рассуждений Kimi-VL ещё не достигли теоретического максимума, особенно для сложных задач, требующих многоступенчатых выводов или более глубокого контекстного понимания. Путь к преодолению этих сложностей — масштабирование модели и совершенствование алгоритмов обучения (в том числе обогащение и увеличение тренировочных датасетов). <br><br><em>Разбор подготовила </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em>  Дарья Виноградова<br></em><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/146_480.webp" srcset="../assets/media/thumbs/146_480.webp 480w, ../assets/media/146.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="146" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 061 просмотров · 33 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/146" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/146.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="144" data-search="no pose, no problem: surprisingly simple 3d gaussian splats from sparse unposed images сегодня коротко разбираем работу noposplat, в которой предлагается метод 3d-реконструкции по rgb-изображениям без информации об их позах. модель noposplat выдаёт 3d gaussian splatting (3dgs) облако, которое можно рендерить (novel view synthesis, nvs) и использовать для оценки относительного положения камер (relative pose estimation). ключевое достижение статьи — демонстрация того, что простая архитектура, основанная на трансформерах (vit, dpt), обученная исключительно с использованием фотометрических функций потерь, может решать задачи реконструкции за один прямой проход. метод полагается на наличие внутренних параметров (intrinsics) камеры, однако обычно получить их легче, чем внешние (extrinsics). это интересно потому, что традиционные методы 3d-реконструкции и синтеза изображений часто требуют большого числа изображений, информации о параметрах камер и полагаются на многоэтапные structure from motion-пайплайны. если количество входных изображений ограничено — так называемый sparse view — возникает проблема плохого перекрытия контента. методы, которые полагаются на геометрические прайоры, например, cost volumes (например, mvsplat) или epipolar geometry (например, pixelsplat), перестают работать. знание поз камер — существенное ограничение для in-the-wild приложений, например, обработки user generated content. архитектура модели «многобашенная» и состоит из трёх основных компонентов: vit энкодера и декодера и dpt-голов, предсказывающих параметры 3dgs-облака, и повторяет широко известные dust3r и mast3r. веса энкодеров общие, а в декодерах применяется cross view attention. у модели две головы. первая предсказывает центроиды гауссиан, а вторая — оставшиеся параметры: поворот, масштаб, цвет. чтобы лучше предсказывать цвет, в модели есть rgb shortcut — вместе с токенами из декодера в голову через свёртку пробрасывается патч из входного изображения. в качестве канонического пространства фиксируется система координат относительно первого входного изображения, и головы выдают параметры гауссиан в этой единой системе координат. для решения проблемы неоднозначности масштаба noposplat делают camera intrinsic embedding. интринсики преобразуются в токен и конкатенируются в энкодере с токенами картиночных патчей. в статье рассматривается ещё два способа добавления интринсиков в модель, но они оказались немного хуже. если описывать метод одним предложением, то можно сказать, что это mast3r c примочками для предсказания 3dgs-облака. обучение модели проводится с использованием mse- и lpips-лоссов, то есть для супервизии используют только rgb-изображения. обучаемая модель предсказывает параметры 3dgs по входным изображениям. затем 3dgs отрисовывается дифференцируемым рендером в нескольких новых известных позах из обучающего датасета и рендеры сравниваются с gt-изображениями. groundtruth-позы используются только для рендеринга в процессе обучения. обучают на датасетах realestate10k, acid и dl3dv. они включают rgb-изображения, а положения камер оценены с помощью colmap. модель может быть инициализирована случайно, но поскольку архитектура повторяет crocov2, dust3r и mast3r, попробовали частично инициализировать веса из них и это дало лучшие результаты. поскольку mast3r был обучен на данных с gt-информацией о глубине, то нельзя сказать, что лучшая модель noposplat обучена только на rgb-данных. для решения задачи оценки относительной позы между входными изображениями сначала находят приближение с использованием pnp + ransac, затем её уточняют, используя ssim loss относительно предсказанного 3dgs-облака. качество nvs зависит от количества картинок на входе и степени их взаимного пересечения, psnr варьируется от 22 до 27. с одной стороны, не так уж много, а с другой — удивительно хорошо при такой постановке задачи. разбор подготовил ❣ расим ахунзянов cv time #yaiclr no pose, no problem: surprisingly simple 3d gaussian splats from sparse unposed images сегодня коротко разбираем работу noposplat , в которой предлагается метод 3d-реконструкции по rgb-изображениям без информации об их позах. модель noposplat выдаёт 3d gaussian splatting (3dgs) облако, которое можно рендерить (novel view synthesis, nvs) и использовать для оценки относительного положения камер (relative pose estimation). ключевое достижение статьи — демонстрация того, что простая архитектура, основанная на трансформерах (vit, dpt), обученная исключительно с использованием фотометрических функций потерь, может решать задачи реконструкции за один прямой проход. метод полагается на наличие внутренних параметров (intrinsics) камеры, однако обычно получить их легче, чем внешние (extrinsics). это интересно потому, что традиционные методы 3d-реконструкции и синтеза изображений часто требуют большого числа изображений, информации о параметрах камер и полагаются на многоэтапные structure from motion-пайплайны. если количество входных изображений ограничено — так называемый sparse view — возникает проблема плохого перекрытия контента. методы, которые полагаются на геометрические прайоры, например, cost volumes (например, mvsplat ) или epipolar geometry (например, pixelsplat ), перестают работать. знание поз камер — существенное ограничение для in-the-wild приложений, например, обработки user generated content. архитектура модели «многобашенная» и состоит из трёх основных компонентов: vit энкодера и декодера и dpt-голов, предсказывающих параметры 3dgs-облака, и повторяет широко известные dust3r и mast3r . веса энкодеров общие, а в декодерах применяется cross view attention. у модели две головы. первая предсказывает центроиды гауссиан, а вторая — оставшиеся параметры: поворот, масштаб, цвет. чтобы лучше предсказывать цвет, в модели есть rgb shortcut — вместе с токенами из декодера в голову через свёртку пробрасывается патч из входного изображения. в качестве канонического пространства фиксируется система координат относительно первого входного изображения, и головы выдают параметры гауссиан в этой единой системе координат. для решения проблемы неоднозначности масштаба noposplat делают camera intrinsic embedding. интринсики преобразуются в токен и конкатенируются в энкодере с токенами картиночных патчей. в статье рассматривается ещё два способа добавления интринсиков в модель, но они оказались немного хуже. если описывать метод одним предложением, то можно сказать, что это mast3r c примочками для предсказания 3dgs-облака. обучение модели проводится с использованием mse- и lpips-лоссов, то есть для супервизии используют только rgb-изображения. обучаемая модель предсказывает параметры 3dgs по входным изображениям. затем 3dgs отрисовывается дифференцируемым рендером в нескольких новых известных позах из обучающего датасета и рендеры сравниваются с gt-изображениями. groundtruth-позы используются только для рендеринга в процессе обучения. обучают на датасетах realestate10k, acid и dl3dv. они включают rgb-изображения, а положения камер оценены с помощью colmap. модель может быть инициализирована случайно, но поскольку архитектура повторяет crocov2, dust3r и mast3r, попробовали частично инициализировать веса из них и это дало лучшие результаты. поскольку mast3r был обучен на данных с gt-информацией о глубине, то нельзя сказать, что лучшая модель noposplat обучена только на rgb-данных. для решения задачи оценки относительной позы между входными изображениями сначала находят приближение с использованием pnp + ransac, затем её уточняют, используя ssim loss относительно предсказанного 3dgs-облака. качество nvs зависит от количества картинок на входе и степени их взаимного пересечения, psnr варьируется от 22 до 27. с одной стороны, не так уж много, а с другой — удивительно хорошо при такой постановке задачи. разбор подготовил ❣ расим ахунзянов cv time #yaiclr">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-06-10T08:02:30+00:00" href="./posts/144.html">2025-06-10 08:02 UTC</a></div>
      </div>
      <div class="post-body"><strong>No Pose, No Problem: Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images<br></strong><br>Сегодня коротко разбираем работу <a href="https://noposplat.github.io/" rel="nofollow noopener noreferrer">NoPoSplat</a>, в которой предлагается метод 3D-реконструкции по RGB-изображениям без информации об их позах. Модель NoPoSplat выдаёт <a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/" rel="nofollow noopener noreferrer">3D Gaussian Splatting</a> (3DGS) облако, которое можно рендерить (novel view synthesis, NVS) и использовать для оценки относительного положения камер (relative pose estimation).<br><br>Ключевое достижение статьи — демонстрация того, что простая архитектура, основанная на трансформерах (ViT, DPT), обученная исключительно с использованием фотометрических функций потерь, может решать задачи реконструкции за один прямой проход. Метод полагается на наличие внутренних параметров (intrinsics) камеры, однако обычно получить их легче, чем внешние (extrinsics).<br><br>Это интересно потому, что традиционные методы 3D-реконструкции и синтеза изображений часто требуют большого числа изображений, информации о параметрах камер и полагаются на многоэтапные structure from motion-пайплайны. Если количество входных изображений ограничено — так называемый sparse view — возникает проблема плохого перекрытия контента. Методы, которые полагаются на геометрические прайоры, например, cost volumes (например, <a href="https://arxiv.org/abs/2403.14627" rel="nofollow noopener noreferrer">MVSplat</a>) или epipolar geometry (например, <a href="https://arxiv.org/abs/2312.12337" rel="nofollow noopener noreferrer">PixelSplat</a>), перестают работать. Знание поз камер — существенное ограничение для in-the-wild приложений, например, обработки user generated content.<br><br>Архитектура модели «многобашенная» и состоит из трёх основных компонентов: ViT энкодера и декодера и DPT-голов, предсказывающих параметры 3DGS-облака, и повторяет широко известные <a href="https://arxiv.org/abs/2312.14132" rel="nofollow noopener noreferrer">DUSt3R</a> и <a href="https://arxiv.org/abs/2406.09756" rel="nofollow noopener noreferrer">MASt3R</a>. Веса энкодеров общие, а в декодерах применяется cross view attention.<br><br>У модели две головы. Первая предсказывает центроиды гауссиан, а вторая — оставшиеся параметры: поворот, масштаб, цвет. Чтобы лучше предсказывать цвет, в модели есть RGB shortcut — вместе с токенами из декодера в голову <a href="https://github.com/cvg/NoPoSplat/blob/9b04307ebe179d610c04208db8d69f7c3106d03b/src/model/encoder/heads/dpt_gs_head.py#L148" rel="nofollow noopener noreferrer">через свёртку пробрасывается</a> патч из входного изображения. В качестве канонического пространства фиксируется система координат относительно первого входного изображения, и головы выдают параметры гауссиан в этой единой системе координат.<br><br>Для решения проблемы неоднозначности масштаба NoPoSplat делают camera intrinsic embedding. Интринсики преобразуются в токен и конкатенируются в энкодере с токенами картиночных патчей. В статье рассматривается ещё два способа добавления интринсиков в модель, но они оказались немного хуже.<br><br>Если описывать метод одним предложением, то можно сказать, что это MASt3R c примочками для предсказания 3DGS-облака.<br><br>Обучение модели проводится с использованием MSE- и LPIPS-лоссов, то есть для супервизии используют только RGB-изображения. Обучаемая модель предсказывает параметры 3DGS по входным изображениям. Затем 3DGS отрисовывается дифференцируемым рендером в нескольких новых известных позах из обучающего датасета и рендеры сравниваются с GT-изображениями. Groundtruth-позы используются только для рендеринга в процессе обучения. Обучают на датасетах RealEstate10k, ACID и DL3DV. Они включают RGB-изображения, а положения камер оценены с помощью COLMAP.<br><br>Модель может быть инициализирована случайно, но поскольку архитектура повторяет CroCoV2, DUSt3R и MASt3R, попробовали частично инициализировать веса из них и это дало лучшие результаты. Поскольку MASt3R был обучен на данных с GT-информацией о глубине, то нельзя сказать, что лучшая модель NoPoSplat обучена только на RGB-данных.<br><br>Для решения задачи оценки относительной позы между входными изображениями сначала находят приближение с использованием PnP + RANSAC, затем <a href="https://arxiv.org/abs/2312.06741" rel="nofollow noopener noreferrer">её уточняют, используя SSIM loss</a> относительно предсказанного 3DGS-облака.<br><br>Качество NVS зависит от количества картинок на входе и степени их взаимного пересечения, PSNR варьируется от 22 до 27. С одной стороны, не так уж много, а с другой — удивительно хорошо при такой постановке задачи.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Расим Ахунзянов<br></em><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><br><br>#YaICLR<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/144_480.webp" srcset="../assets/media/thumbs/144_480.webp 480w, ../assets/media/144.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="144" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/145_480.webp" srcset="../assets/media/thumbs/145_480.webp 480w, ../assets/media/145.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="144" data-image-index="1" /></div></div>
      <div class="actions">
        <span>2 061 просмотров · 29 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/144" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/144.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="143" data-search="improving the diffusability of autoencoders сегодня разбираем статью, в которой обсуждается то, что авторы называют diffusability латентного пространства: насколько легко диффузионной модели учиться на латентах автоэнкодера. в латентных диффузионных моделях (например, stable diffusion) генерация происходит не в пикселях, а в сжатом представлении. это ускоряет обучение, но вводит зависимость от свойств автоэнкодера. обычно смотрят только на качество реконструкции: насколько хорошо декодер восстанавливает изображение. но есть вторая характеристика — diffusability, и именно её авторы рассматривают в этой работе. что такое diffusability и почему это важно если латенты имеют сложное распределение или содержат неинформативные шумовые компоненты, диффузии приходится подстраиваться под это распределение — обучаться дольше и потенциально упираться в потолок качества. поэтому автоэнкодер задаёт не только качество реконструкции, но и удобство обучения вместе с последующей генерацией. авторы смотрят на латенты от обычных автоэнкодеров и замечают, что они визуально шумные: в них много высокочастотных деталей, особенно в фоне. чтобы разобраться, применяют дискретное косинусное преобразование (dct), как в jpeg. разбивают картинку или латент на блоки 8×8, считают dct по каждому из них, усредняют спектры и строят частотный профиль. выясняется, что латенты содержат больше высокочастотных компонентов, чем изображения, и это особенно заметно при увеличении числа каналов. даже если латент визуально похож на картинку, его частотный профиль сильно отличается. а если обнулить высокие частоты и попробовать восстановить изображение, латент теряет качество гораздо сильнее, чем обычное изображение — там такие потери почти незаметны. это говорит о том, что латенты слишком зависят от высокочастотной части и не обладают масштабной эквивариантностью. тогда авторы добавляют к лоссу автоэнкодера простую компоненту: берут исходное изображение и соответствующий латент, уменьшают их разрешение (в 2 или 4 раза), затем реконструируют картинку из сжатого латента и считают дополнительный лосс между даунскейленным изображением и полученной реконструкцией. таким образом они обеспечивают соблюдения свойства масштабной инвариантности (потому что лосс буквально это и делает), что, в свою очередь, регуляризует латенты, убирая из них лишние высокие частоты. результат — латенты становятся менее шумными, частотные профили ближе к тем, что у изображений. и, что важно, визуально структура латента сохраняется. согласно метрикам, качество реконструкции почти не падает. эксперименты метод протестировали на imagenet-1k (изображения) и kinetics-700 (видео). сравнивали обучение диффузионной модели на обычных и исправленных латентах. в статье diffusability измеряют через скорость обучения: берут автоэнкодер, обучают на нём диффузионную модель и смотрят, насколько быстро растёт метрика качества (например, fid для изображений и fvd для видео). сравнивались базовые модели и те же архитектуры, но обученные на автоэнкодерах с исходным и улучшенным diffusability. оказалось, что последние учатся быстрее и дают лучшее финальное качество. результаты: — генерация изображений: fid улучшился на 19%; — генерация видео: fvd улучшился на 44%; — модели обучаются быстрее; — psnr немного растёт (за счёт блюра), но визуально картинки выглядят нормально. визуализация того, как выглядят латенты до и после (см. картинку), взята из другой работы, посвященной этой же теме: шум действительно уходит, но структура остаётся. частотные кривые тоже приближаются к тем, что у изображений. в целом статья посвящена довольно локальной проблеме, но в ней есть понятная идея и измеримый эффект. разбор подготовил ❣ сергей кастрюлин cv time improving the diffusability of autoencoders сегодня разбираем статью , в которой обсуждается то, что авторы называют diffusability латентного пространства: насколько легко диффузионной модели учиться на латентах автоэнкодера. в латентных диффузионных моделях (например, stable diffusion) генерация происходит не в пикселях, а в сжатом представлении. это ускоряет обучение, но вводит зависимость от свойств автоэнкодера. обычно смотрят только на качество реконструкции: насколько хорошо декодер восстанавливает изображение. но есть вторая характеристика — diffusability, и именно её авторы рассматривают в этой работе. что такое diffusability и почему это важно если латенты имеют сложное распределение или содержат неинформативные шумовые компоненты, диффузии приходится подстраиваться под это распределение — обучаться дольше и потенциально упираться в потолок качества. поэтому автоэнкодер задаёт не только качество реконструкции, но и удобство обучения вместе с последующей генерацией. авторы смотрят на латенты от обычных автоэнкодеров и замечают, что они визуально шумные: в них много высокочастотных деталей, особенно в фоне. чтобы разобраться, применяют дискретное косинусное преобразование (dct), как в jpeg. разбивают картинку или латент на блоки 8×8, считают dct по каждому из них, усредняют спектры и строят частотный профиль. выясняется, что латенты содержат больше высокочастотных компонентов, чем изображения, и это особенно заметно при увеличении числа каналов. даже если латент визуально похож на картинку, его частотный профиль сильно отличается. а если обнулить высокие частоты и попробовать восстановить изображение, латент теряет качество гораздо сильнее, чем обычное изображение — там такие потери почти незаметны. это говорит о том, что латенты слишком зависят от высокочастотной части и не обладают масштабной эквивариантностью. тогда авторы добавляют к лоссу автоэнкодера простую компоненту: берут исходное изображение и соответствующий латент, уменьшают их разрешение (в 2 или 4 раза), затем реконструируют картинку из сжатого латента и считают дополнительный лосс между даунскейленным изображением и полученной реконструкцией. таким образом они обеспечивают соблюдения свойства масштабной инвариантности (потому что лосс буквально это и делает), что, в свою очередь, регуляризует латенты, убирая из них лишние высокие частоты. результат — латенты становятся менее шумными, частотные профили ближе к тем, что у изображений. и, что важно, визуально структура латента сохраняется. согласно метрикам, качество реконструкции почти не падает. эксперименты метод протестировали на imagenet-1k (изображения) и kinetics-700 (видео). сравнивали обучение диффузионной модели на обычных и исправленных латентах. в статье diffusability измеряют через скорость обучения: берут автоэнкодер, обучают на нём диффузионную модель и смотрят, насколько быстро растёт метрика качества (например, fid для изображений и fvd для видео). сравнивались базовые модели и те же архитектуры, но обученные на автоэнкодерах с исходным и улучшенным diffusability. оказалось, что последние учатся быстрее и дают лучшее финальное качество. результаты: — генерация изображений: fid улучшился на 19%; — генерация видео: fvd улучшился на 44%; — модели обучаются быстрее; — psnr немного растёт (за счёт блюра), но визуально картинки выглядят нормально. визуализация того, как выглядят латенты до и после ( см. картинку ), взята из другой работы , посвященной этой же теме: шум действительно уходит, но структура остаётся. частотные кривые тоже приближаются к тем, что у изображений. в целом статья посвящена довольно локальной проблеме, но в ней есть понятная идея и измеримый эффект. разбор подготовил ❣ сергей кастрюлин cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-06-04T08:06:18+00:00" href="./posts/143.html">2025-06-04 08:06 UTC</a></div>
      </div>
      <div class="post-body"><strong>Improving the Diffusability of Autoencoders</strong><br><br>Сегодня разбираем <a href="https://arxiv.org/abs/2502.14831" rel="nofollow noopener noreferrer">статью</a>, в которой обсуждается то, что авторы называют diffusability латентного пространства: насколько легко диффузионной модели учиться на латентах автоэнкодера.<br><br>В латентных диффузионных моделях (например, Stable Diffusion) генерация происходит не в пикселях, а в сжатом представлении. Это ускоряет обучение, но вводит зависимость от свойств автоэнкодера. Обычно смотрят только на качество реконструкции: насколько хорошо декодер восстанавливает изображение. Но есть вторая характеристика — diffusability, и именно её авторы рассматривают в этой работе.<br><br><strong>Что такое diffusability и почему это важно</strong><br><br>Если латенты имеют сложное распределение или содержат неинформативные шумовые компоненты, диффузии приходится подстраиваться под это распределение — обучаться дольше и потенциально упираться в потолок качества. Поэтому автоэнкодер задаёт не только качество реконструкции, но и удобство обучения вместе с последующей генерацией.<br><br>Авторы смотрят на латенты от обычных автоэнкодеров и замечают, что они визуально шумные: в них много высокочастотных деталей, особенно в фоне. Чтобы разобраться, применяют дискретное косинусное преобразование (DCT), как в JPEG. Разбивают картинку или латент на блоки 8×8, считают DCT по каждому из них, усредняют спектры и строят частотный профиль.<br><br>Выясняется, что латенты содержат больше высокочастотных компонентов, чем изображения, и это особенно заметно при увеличении числа каналов. Даже если латент визуально похож на картинку, его частотный профиль сильно отличается. А если обнулить высокие частоты и попробовать восстановить изображение, латент теряет качество гораздо сильнее, чем обычное изображение — там такие потери почти незаметны. Это говорит о том, что латенты слишком зависят от высокочастотной части  и не обладают масштабной эквивариантностью.<br><br>Тогда авторы добавляют к лоссу автоэнкодера простую компоненту: берут исходное изображение и соответствующий латент, уменьшают их разрешение (в 2 или 4 раза), затем реконструируют картинку из сжатого латента и считают дополнительный лосс между даунскейленным изображением и полученной реконструкцией.<br><br>Таким образом они обеспечивают соблюдения свойства масштабной инвариантности (потому что лосс буквально это и делает), что, в свою очередь, регуляризует латенты, убирая из них лишние высокие частоты. <br><br>Результат — латенты становятся менее шумными, частотные профили ближе к тем, что у изображений. И, что важно, визуально структура латента сохраняется. Согласно метрикам, качество реконструкции почти не падает.<br><br><strong>Эксперименты</strong><br><strong><br></strong>Метод протестировали на ImageNet-1K (изображения) и Kinetics-700 (видео). Сравнивали обучение диффузионной модели на обычных и исправленных латентах.<br><br>В статье diffusability измеряют через скорость обучения: берут автоэнкодер, обучают на нём диффузионную модель и смотрят, насколько быстро растёт метрика качества (например, FID для изображений и FVD для видео). Сравнивались базовые модели и те же архитектуры, но обученные на автоэнкодерах с исходным и улучшенным diffusability. Оказалось, что последние учатся быстрее и дают лучшее финальное качество.<br><br>Результаты:<br> — генерация изображений: FID улучшился на 19%;<br> — генерация видео: FVD улучшился на 44%;<br> — модели обучаются быстрее;<br> — PSNR немного растёт (за счёт блюра), но визуально картинки выглядят нормально.<br><br>Визуализация того, как выглядят латенты до и после (<em>см. картинку</em>), взята <a href="https://arxiv.org/pdf/2502.09509" rel="nofollow noopener noreferrer">из другой работы</a>, посвященной этой же теме: шум действительно уходит, но структура остаётся. Частотные кривые тоже приближаются к тем, что у изображений.<br><br>В целом статья посвящена довольно локальной проблеме, но в ней есть понятная идея и измеримый эффект. <br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> </em>Сергей Кастрюлин<br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/143_480.webp" srcset="../assets/media/thumbs/143_480.webp 480w, ../assets/media/143.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="143" data-image-index="0" /></div></div>
      <div class="actions">
        <span>5 778 просмотров · 37 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/143" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/143.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="140" data-search="yandex alchemist: открытый датасет для буста text-to-image генерации раньше t2i-модели обучали в один этап — претрейн на большом, довольно грязном датасете интернет-данных. в 2023 году meta в техрепорте emu предложили делать файнтюн на маленьком датасете исключительного качества и за счёт этого существенно бустить результат генерации. правда, они ничего не сказали о том, как такой датасет собрать. команда yandexart тоже занималась этой задачей, и сегодня мы делимся результатами своей работы — датасетом alchemist. он состоит из 3 350 пар «картинка-текст» и имеет лицензию apache 2.0, пользуйтесь. alchemist сокращает дистанцию между крутыми потюненными закрытыми моделями и открытыми, для которых такой тюнинг недоступен. ранее сообществу был доступен только пофильтрованный на эстетичность кусочек laion и файнтюн-датасеты под узкий домен, например аниме или живопись. laion часто не давал существенного прироста качества, а файнтюны под узкий домен ограничивали возможности генерации за его пределами. ниже мы подробно рассказываем, как получить датасет уровня alchemist, имея лишь сырой набор интернет-данных. отметим, что весь пайплайн — про картинки. мы считаем, что так правильно: тексты потом лучше сгенерировать синтетические. итак, стартуя с датасета на 10 млрд примеров, мы выбрали картинки высокого разрешения без nsfw-контента и удалили те, что содержали вотермарки, имели низкое качество и были неэстетичны. когда осталось примерно 300 млн изображений, дальнейшее выкручивание порогов фильтрации не помогало: модели недостаточно чувствительны, чтобы отделять хорошие картинки от великолепных. выбирать руками лучшее из такого большого набора — тоже сомнительная затея. на этом этапе мы предположили, что предобученная диффузионка может сама знать, какие картинки хорошие, а какие — не очень. пробовали подходы из области dataset pruning, например, пропускать картинки через модель и смотреть на значение лосса. оказалось, что так отбираются только самые простые изображения — абстрактные иллюстрации, вроде обоев на рабочий стол. в них немного деталей и их легко моделировать, но на файнтюне от них мало толку. в итоге нам пришлось придумать свой метод, суть которого в следующем. 1. возьмём 1000 картинок из наших 300 млн и разметим на условно плохие (lq) и хорошие (hq). хорошими будем считать те, у которых высокие эстетичность и техническое качество, умеренная наполненность контентом. 2. смастерим общий промт, который будет содержать перечисление желаемых характеристик: “aesthetic”, “high quality” и т. д. 3. дальше будем брать lq- и hq-картинки, зашумлять их до какого-то t, подавать в нашу предобученую диффузионку вместе с промтом и смотреть, что происходит со значениями в cross-attention. оказывается, что на основе нашей небольшой и грубой разметки можно выделить комбинации активаций в cross-attn и токенов, которые будут хорошо отделять изображения с нужными нам свойствами. если просуммировать эти значения, получим скаляр, который и будет нашим скором качества изображения. проскорив таким образом 300 млн картинок, мы выбрали топ-3350 — это картинки из нашего датасета. дальше осталось сделать тексты — исходные из интернета могут быть ошибочны, содержать лишнюю или упускать нужную информацию. наше наблюдение: лучше всего работают умеренно подробные промты, похожие на те, которые пишет скорее увлечённый пользователь, чем профессиональный промпт-инженер. yandexvlm как раз умеет подстраиваться под нужный формат. с её помощью мы сгенерировали тексты для каждой картинки, получив датасет alchemist. чтобы убедиться в обобщаемости датасета и метода, мы сделали и выложили файнтюны sd 1.5, sd 2.1, sdxl-base 1.0, sd 3.5 medium и large. у всех файнтюнов растёт эстетичность и наполненность генераций, которую мы называем “image complexity”. подробнее о методике и экспериментах читайте в препринте. статью подготовили ❣ валерий старцев, александр устюжанин, алексей кириллов, дмитрий баранчук, сергей кастрюлин cv time ___ meta признана экстремистской организацией, а facebook и instagram запрещены на территории рф yandex alchemist: открытый датасет для буста text-to-image генерации раньше t2i-модели обучали в один этап — претрейн на большом, довольно грязном датасете интернет-данных. в 2023 году meta в техрепорте emu предложили делать файнтюн на маленьком датасете исключительного качества и за счёт этого существенно бустить результат генерации. правда, они ничего не сказали о том, как такой датасет собрать. команда yandexart тоже занималась этой задачей, и сегодня мы делимся результатами своей работы — датасетом alchemist . он состоит из 3 350 пар «картинка-текст» и имеет лицензию apache 2.0, пользуйтесь. alchemist сокращает дистанцию между крутыми потюненными закрытыми моделями и открытыми, для которых такой тюнинг недоступен. ранее сообществу был доступен только пофильтрованный на эстетичность кусочек laion и файнтюн-датасеты под узкий домен, например аниме или живопись. laion часто не давал существенного прироста качества, а файнтюны под узкий домен ограничивали возможности генерации за его пределами. ниже мы подробно рассказываем, как получить датасет уровня alchemist, имея лишь сырой набор интернет-данных. отметим, что весь пайплайн — про картинки. мы считаем, что так правильно: тексты потом лучше сгенерировать синтетические. итак, стартуя с датасета на 10 млрд примеров, мы выбрали картинки высокого разрешения без nsfw-контента и удалили те, что содержали вотермарки, имели низкое качество и были неэстетичны. когда осталось примерно 300 млн изображений, дальнейшее выкручивание порогов фильтрации не помогало: модели недостаточно чувствительны, чтобы отделять хорошие картинки от великолепных. выбирать руками лучшее из такого большого набора — тоже сомнительная затея. на этом этапе мы предположили, что предобученная диффузионка может сама знать, какие картинки хорошие, а какие — не очень. пробовали подходы из области dataset pruning , например, пропускать картинки через модель и смотреть на значение лосса. оказалось, что так отбираются только самые простые изображения — абстрактные иллюстрации, вроде обоев на рабочий стол. в них немного деталей и их легко моделировать, но на файнтюне от них мало толку. в итоге нам пришлось придумать свой метод, суть которого в следующем. 1. возьмём 1000 картинок из наших 300 млн и разметим на условно плохие (lq) и хорошие (hq). хорошими будем считать те, у которых высокие эстетичность и техническое качество, умеренная наполненность контентом. 2. смастерим общий промт, который будет содержать перечисление желаемых характеристик: “aesthetic”, “high quality” и т. д. 3. дальше будем брать lq- и hq-картинки, зашумлять их до какого-то t, подавать в нашу предобученую диффузионку вместе с промтом и смотреть, что происходит со значениями в cross-attention. оказывается, что на основе нашей небольшой и грубой разметки можно выделить комбинации активаций в cross-attn и токенов, которые будут хорошо отделять изображения с нужными нам свойствами. если просуммировать эти значения, получим скаляр, который и будет нашим скором качества изображения. проскорив таким образом 300 млн картинок, мы выбрали топ-3350 — это картинки из нашего датасета. дальше осталось сделать тексты — исходные из интернета могут быть ошибочны, содержать лишнюю или упускать нужную информацию. наше наблюдение: лучше всего работают умеренно подробные промты, похожие на те, которые пишет скорее увлечённый пользователь, чем профессиональный промпт-инженер. yandexvlm как раз умеет подстраиваться под нужный формат. с её помощью мы сгенерировали тексты для каждой картинки, получив датасет alchemist. чтобы убедиться в обобщаемости датасета и метода, мы сделали и выложили файнтюны sd 1.5, sd 2.1, sdxl-base 1.0, sd 3.5 medium и large. у всех файнтюнов растёт эстетичность и наполненность генераций, которую мы называем “image complexity”. подробнее о методике и экспериментах читайте в препринте . статью подготовили ❣ валерий старцев, александр устюжанин, алексей кириллов, дмитрий баранчук, сергей кастрюлин cv time ___ meta признана экстремистской организацией, а facebook и instagram запрещены на территории рф">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-05-27T09:12:54+00:00" href="./posts/140.html">2025-05-27 09:12 UTC</a></div>
      </div>
      <div class="post-body"><strong>Yandex Alchemist: открытый датасет для буста text-to-image генерации</strong><br><br>Раньше T2I-модели обучали в один этап — претрейн на большом, довольно грязном датасете интернет-данных. В 2023 году <a href="https://arxiv.org/abs/2309.15807" rel="nofollow noopener noreferrer">Meta в техрепорте EMU</a> предложили делать файнтюн на маленьком датасете исключительного качества и за счёт этого существенно бустить результат генерации. Правда, они ничего не сказали о том, как такой датасет собрать.<br><br>Команда YandexART тоже занималась этой задачей, и сегодня мы делимся результатами своей работы — <a href="https://huggingface.co/datasets/yandex/alchemist" rel="nofollow noopener noreferrer">датасетом Alchemist</a>. Он состоит из 3 350 пар «картинка-текст» и имеет лицензию Apache 2.0, пользуйтесь.<br><br>Alchemist сокращает дистанцию между крутыми потюненными закрытыми моделями и открытыми, для которых такой тюнинг недоступен. Ранее сообществу был доступен только пофильтрованный на эстетичность кусочек LAION и файнтюн-датасеты под узкий домен, например аниме или живопись. LAION часто не давал существенного прироста качества, а файнтюны под узкий домен ограничивали возможности генерации за его пределами.<br><br>Ниже мы подробно рассказываем, как получить датасет уровня Alchemist, имея лишь сырой набор интернет-данных. Отметим, что весь пайплайн — про картинки. Мы считаем, что так правильно: тексты потом лучше сгенерировать синтетические. <br><br>Итак, стартуя с датасета на 10 млрд примеров, мы выбрали картинки высокого разрешения без NSFW-контента и удалили те, что содержали вотермарки, имели низкое качество и были неэстетичны. Когда осталось примерно 300 млн изображений, дальнейшее выкручивание порогов фильтрации не помогало: модели недостаточно чувствительны, чтобы отделять хорошие картинки от великолепных. Выбирать руками лучшее из такого большого набора — тоже сомнительная затея. <br><br>На этом этапе мы предположили, что предобученная диффузионка может сама знать, какие картинки хорошие, а какие — не очень. Пробовали подходы из области <a href="https://arxiv.org/abs/2205.09329" rel="nofollow noopener noreferrer">dataset pruning</a>, например, пропускать картинки через модель и смотреть на значение лосса. Оказалось, что так отбираются только самые простые изображения — абстрактные иллюстрации, вроде обоев на рабочий стол. В них немного деталей и их легко моделировать, но на файнтюне от них мало толку.<br><br>В итоге нам пришлось придумать свой метод, суть которого в следующем. <br><br>1. Возьмём 1000 картинок из наших 300 млн и разметим на условно плохие (LQ) и хорошие (HQ). Хорошими будем считать те, у которых высокие эстетичность и техническое качество, умеренная наполненность контентом. <br>2. Смастерим общий промт, который будет содержать перечисление желаемых характеристик: “aesthetic”, “high quality” и т. д. <br>3. Дальше будем брать LQ- и HQ-картинки, зашумлять их до какого-то t, подавать в нашу предобученую диффузионку вместе с промтом и смотреть, что происходит со значениями в cross-attention. <br><br>Оказывается, что на основе нашей небольшой и грубой разметки можно выделить комбинации активаций в cross-attn и токенов, которые будут хорошо отделять изображения с нужными нам свойствами. Если просуммировать эти значения, получим скаляр, который и будет нашим скором качества изображения. Проскорив таким образом 300 млн картинок, мы выбрали топ-3350 — это картинки из нашего датасета. <br><br>Дальше осталось сделать тексты — исходные из интернета могут быть ошибочны, содержать лишнюю или упускать нужную информацию. Наше наблюдение: лучше всего работают умеренно подробные промты, похожие на те, которые пишет скорее увлечённый пользователь, чем профессиональный промпт-инженер. YandexVLM как раз умеет подстраиваться под нужный формат. С её помощью мы сгенерировали тексты для каждой картинки, получив датасет Alchemist.<br><br>Чтобы убедиться в обобщаемости датасета и метода, мы сделали и <a href="https://huggingface.co/collections/yandex/alchemist-6825f7a16cbcc71128ee525f" rel="nofollow noopener noreferrer">выложили файнтюны</a> SD 1.5, SD 2.1, SDXL-base 1.0, SD 3.5 Medium и Large. У всех файнтюнов растёт эстетичность и наполненность генераций, которую мы называем “image complexity”. Подробнее о методике и экспериментах <a href="https://arxiv.org/abs/2505.19297" rel="nofollow noopener noreferrer">читайте в препринте</a>. <br><br><em>Статью подготовили </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Валерий Старцев, Александр Устюжанин, Алексей Кириллов, Дмитрий Баранчук, Сергей Кастрюлин<br></em><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><br>___<br><em>Meta признана экстремистской организацией, а Facebook и Instagram запрещены на территории РФ</em><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/140_480.webp" srcset="../assets/media/thumbs/140_480.webp 480w, ../assets/media/140.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="140" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/141_480.webp" srcset="../assets/media/thumbs/141_480.webp 480w, ../assets/media/141.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="140" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/142_480.webp" srcset="../assets/media/thumbs/142_480.webp 480w, ../assets/media/142.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="140" data-image-index="2" /></div></div>
      <div class="actions">
        <span>12 893 просмотров · 65 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/140" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/140.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="139" data-search="the chosen one: consistent characters in text-to-image diffusion models сегодня разбираем статью, которая предлагает не самый практичный, но достаточно любопытный способ заставить генеративную модель выдавать одного и того же персонажа при разных промптах. например, это важно для сторителлинга и комиксов, где герой должен сохранять идентичность во всех сценах. основная идея статьи — добиться того, чтобы по одному текстовому промпту всегда генерировался один и тот же персонаж. при стандартной генерации «ёжика-альбиноса с фиолетовыми иголками» без подготовки получаются разные ёжики: похожие, но отличающиеся в деталях. обычно задачу решают через dreambooth или текстовую инверсию на одной картинке, но это ведёт к жесткому переобучению и потере вариативности окружения. авторы предлагают другой путь. они не используют исходное изображение и работают только с текстом. сначала генерируют 128 картинок по одному промпту (sdxl), затем извлекают эмбеддинги через dinov2 и выполняют кластеризацию. выбирают самый крупный и плотный кластер — там образ героя выглядит максимально однородно. на этом подмножестве проводят fine-tune модели с помощью lora и текстовой инверсии, после чего повторяют цикл генерации, кластеризации и обучения ещё четыре–пять раз. процедура занимает около 24 минут на одной gpu. так удаётся зафиксировать ключевые черты персонажа — цвет кожи, форму глаз, аксессуары и даже позу, хотя фон при этом остаётся неизменным. при смене промпта обучение придётся повторить: метод жёстко привязан к тексту. сравнение с базовыми методами: - vanilla textual inversion — образы слишком разнородны; - dreambooth full fine-tuning — модель переобучается на фон и перестаёт менять окружение; - текстовая инверсия через lora: недообучается, даёт слабую консистентность. в итоге этот метод («sauce») позволяет получить баланс между соответствием промту и стабильностью образа. auto-метрика clip-score и оценки на amazon mturk подтвердили, что согласованность растёт без серьёзных потерь в точности при сохранении разнообразия фонов и поз. абляционный анализ показывает, что без кластеризации модели не сохраняют образ. одна итерация обучения даёт малозаметный эффект, а при реинициализации весов каждую итерацию результаты ухудшаются. метод совместим с другими техниками: при генерации истории из четырёх промптов герой остаётся постоянным; с controlnet можно задать новую позу, сохранив лицо, а сочетание с dreambooth и lora улучшает детализацию. основные ограничения связаны с тем, что кластер может захватить фон или часто встречающиеся детали — котик может «прилипнуть» к листикам, а позы и окружение мешают выделить только лицо героя. авторы предлагают предоставить пользователю выбор из нескольких кластеров. в перспективе авторы хотят расширить подход для работы с реальными фотографиями: сначала получить текстовое описание через captioning, затем применить тот же цикл генерации, кластеризации и дообучения. немного технических деталей: 128 изображений, 500 шагов обучения с adamw, порог плотности кластера — 0,8 от медианной дистанции с адаптивным подбором на первой итерации. в заключение можно подметить, что метод хоть и интересный, но на практике требует много времени и ресурсов, а результат всё же далёк от идеала. но сама идея итеративной кластеризации и дообучения модели заслуживает внимания. разбор подготовил ❣ григорий лившиц cv time the chosen one: consistent characters in text-to-image diffusion models сегодня разбираем статью , которая предлагает не самый практичный, но достаточно любопытный способ заставить генеративную модель выдавать одного и того же персонажа при разных промптах. например, это важно для сторителлинга и комиксов, где герой должен сохранять идентичность во всех сценах. основная идея статьи — добиться того, чтобы по одному текстовому промпту всегда генерировался один и тот же персонаж. при стандартной генерации «ёжика-альбиноса с фиолетовыми иголками» без подготовки получаются разные ёжики: похожие, но отличающиеся в деталях. обычно задачу решают через dreambooth или текстовую инверсию на одной картинке, но это ведёт к жесткому переобучению и потере вариативности окружения. авторы предлагают другой путь. они не используют исходное изображение и работают только с текстом. сначала генерируют 128 картинок по одному промпту (sdxl), затем извлекают эмбеддинги через dinov2 и выполняют кластеризацию. выбирают самый крупный и плотный кластер — там образ героя выглядит максимально однородно. на этом подмножестве проводят fine-tune модели с помощью lora и текстовой инверсии, после чего повторяют цикл генерации, кластеризации и обучения ещё четыре–пять раз. процедура занимает около 24 минут на одной gpu. так удаётся зафиксировать ключевые черты персонажа — цвет кожи, форму глаз, аксессуары и даже позу, хотя фон при этом остаётся неизменным. при смене промпта обучение придётся повторить: метод жёстко привязан к тексту. сравнение с базовыми методами: - vanilla textual inversion — образы слишком разнородны; - dreambooth full fine-tuning — модель переобучается на фон и перестаёт менять окружение; - текстовая инверсия через lora: недообучается, даёт слабую консистентность. в итоге этот метод («sauce») позволяет получить баланс между соответствием промту и стабильностью образа. auto-метрика clip-score и оценки на amazon mturk подтвердили, что согласованность растёт без серьёзных потерь в точности при сохранении разнообразия фонов и поз. абляционный анализ показывает, что без кластеризации модели не сохраняют образ. одна итерация обучения даёт малозаметный эффект, а при реинициализации весов каждую итерацию результаты ухудшаются. метод совместим с другими техниками: при генерации истории из четырёх промптов герой остаётся постоянным; с controlnet можно задать новую позу, сохранив лицо, а сочетание с dreambooth и lora улучшает детализацию. основные ограничения связаны с тем, что кластер может захватить фон или часто встречающиеся детали — котик может «прилипнуть» к листикам, а позы и окружение мешают выделить только лицо героя. авторы предлагают предоставить пользователю выбор из нескольких кластеров. в перспективе авторы хотят расширить подход для работы с реальными фотографиями: сначала получить текстовое описание через captioning, затем применить тот же цикл генерации, кластеризации и дообучения. немного технических деталей: 128 изображений, 500 шагов обучения с adamw, порог плотности кластера — 0,8 от медианной дистанции с адаптивным подбором на первой итерации. в заключение можно подметить, что метод хоть и интересный, но на практике требует много времени и ресурсов, а результат всё же далёк от идеала. но сама идея итеративной кластеризации и дообучения модели заслуживает внимания. разбор подготовил ❣ григорий лившиц cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-05-20T08:04:31+00:00" href="./posts/139.html">2025-05-20 08:04 UTC</a></div>
      </div>
      <div class="post-body"><strong>The Chosen One: Consistent Characters in Text-to-Image Diffusion Models</strong><br><br>Сегодня разбираем <a href="https://arxiv.org/abs/2311.10093" rel="nofollow noopener noreferrer">статью</a>, которая предлагает не самый практичный, но достаточно любопытный способ заставить генеративную модель выдавать одного и того же персонажа при разных промптах. Например, это важно для сторителлинга и комиксов, где герой должен сохранять идентичность во всех сценах.<br><br>Основная идея статьи — добиться того, чтобы по одному текстовому промпту всегда генерировался один и тот же персонаж. При стандартной генерации «ёжика-альбиноса с фиолетовыми иголками» без подготовки получаются разные ёжики:  похожие, но отличающиеся в деталях. Обычно задачу решают через DreamBooth или текстовую инверсию на одной картинке, но это ведёт к жесткому переобучению и потере вариативности окружения.<br><br>Авторы предлагают другой путь. Они не используют исходное изображение и работают только с текстом. Сначала генерируют 128 картинок по одному промпту (SDXL), затем извлекают эмбеддинги через DINOv2 и выполняют кластеризацию. Выбирают самый крупный и плотный кластер — там образ героя выглядит максимально однородно. На этом подмножестве проводят fine-tune модели с помощью LoRA и текстовой инверсии, после чего повторяют цикл генерации, кластеризации и обучения ещё четыре–пять раз. Процедура занимает около 24 минут на одной GPU.<br><br>Так удаётся зафиксировать ключевые черты персонажа — цвет кожи, форму глаз, аксессуары и даже позу, хотя фон при этом остаётся неизменным. При смене промпта обучение придётся повторить: метод жёстко привязан к тексту.<br><br>Сравнение с базовыми методами:<br><br>- Vanilla Textual Inversion — образы слишком разнородны;<br>- DreamBooth full fine-tuning — модель переобучается на фон и перестаёт менять окружение;<br>- текстовая инверсия через LoRA: недообучается, даёт слабую консистентность.<br><br>В итоге этот метод («Sauce») позволяет получить баланс между соответствием промту и стабильностью образа. Auto-метрика CLIP-Score и оценки на Amazon MTurk подтвердили, что согласованность растёт без серьёзных потерь в точности при сохранении разнообразия фонов и поз.<br><br>Абляционный анализ показывает, что без кластеризации модели не сохраняют образ. Одна итерация обучения даёт малозаметный эффект, а при реинициализации весов каждую итерацию результаты ухудшаются. <br><br>Метод совместим с другими техниками: при генерации истории из четырёх промптов герой остаётся постоянным; с ControlNet можно задать новую позу, сохранив лицо, а сочетание с DreamBooth и LoRA улучшает детализацию.<br><br>Основные ограничения связаны с тем, что кластер может захватить фон или часто встречающиеся детали — котик может «прилипнуть» к листикам, а позы и окружение мешают выделить только лицо героя. Авторы предлагают предоставить пользователю выбор из нескольких кластеров.<br><br>В перспективе авторы хотят расширить подход для работы с реальными фотографиями: сначала получить текстовое описание через captioning, затем применить тот же цикл генерации, кластеризации и дообучения.<br><br>Немного технических деталей: 128 изображений, 500 шагов обучения с AdamW, порог плотности кластера — 0,8 от медианной дистанции с адаптивным подбором на первой итерации.<br><br>В заключение можно подметить, что метод хоть и интересный, но на практике требует много времени и ресурсов, а результат всё же далёк от идеала. Но сама идея итеративной кластеризации и дообучения модели заслуживает внимания.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Григорий Лившиц</em><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/139_480.webp" srcset="../assets/media/thumbs/139_480.webp 480w, ../assets/media/139.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="139" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 195 просмотров · 22 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/139" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/139.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="137" data-search="hunyuanvideo: a systematic framework for large video generative models сегодня разбираем статью от команды tencent о hunyuanvideo — большой генеративной модели для видео. работа во многом напоминает moviegen, но есть некоторые важные отличия. а главное — веса модели выложены в открытый доступ, что редкость для видеомоделей. обучение начинается с картинок 256×256, потом разрешение повышают до 512×512. при этом 256×256 всё ещё поддерживается — чтобы не терять навык генерации на этом уровне. сначала учат только на изображениях, потом добавляют видео. генерация стартует с нормального распределения, стандартного для диффузионок. но вместо линейно-квадратичного расписания шагов из moviegen, здесь применяется «сдвинутое» специальным образом расписание. авторы говорят, что такой сдвиг даёт лучшее качество, чем квадратичное расписание, особенно при уменьшении количества шагов инференса. видео для обучения берут из датасета webvid. чтобы сбалансировать данные, авторы находят 10 000 центроид и сэмплируют из них так, чтобы равномерно распределить количество примеров между центроидами. если в одну центроиду попадает слишком много данных, часть отбрасывают. так датасет получается разнообразнее. у модели несколько видов параллелизма: тензорный (делят слои и ff-блоки между gpu), контекстный (делят токены между процессами) и параллелизм по данным. это помогает обрабатывать длинные последовательности, возникающие при генерации в высоком разрешении. также модель поддерживает cfg и guidance distillation — учитель и ученик, как обычно. ученик учится повторять учителя по результатам генерации. для переписывания промптов используют hunyuan large language model — особенно если исходный текст слишком технический. есть отдельная аудиомодель, которая по сгенерённому видео создаёт музыку. она учится на спектрограммах и работает в духе audiogen. ещё есть возможности персонализации: можно подать референс-картинку и получить видео. модель справляется с аватарами, движущимися портретами и анимацией объектов. авторы собрали свой бенчмарк из 1533 промптов и сравнились с пятью сильными бейзлайнами. публикуют не всё: выкладывают 600 промптов. смотрят на соответствие тексту, движение, визуальное качество и общее впечатление. их модель лидирует, но не с гигантским отрывом. оценки flops — без подробностей, так что сравнивать с другими моделями сложно. разбор подготовил ❣ денис кузнеделев cv time hunyuanvideo: a systematic framework for large video generative models сегодня разбираем статью от команды tencent о hunyuanvideo — большой генеративной модели для видео. работа во многом напоминает moviegen , но есть некоторые важные отличия. а главное — веса модели выложены в открытый доступ, что редкость для видеомоделей. обучение начинается с картинок 256×256, потом разрешение повышают до 512×512. при этом 256×256 всё ещё поддерживается — чтобы не терять навык генерации на этом уровне. сначала учат только на изображениях, потом добавляют видео. генерация стартует с нормального распределения, стандартного для диффузионок. но вместо линейно-квадратичного расписания шагов из moviegen, здесь применяется «сдвинутое» специальным образом расписание. авторы говорят, что такой сдвиг даёт лучшее качество, чем квадратичное расписание, особенно при уменьшении количества шагов инференса. видео для обучения берут из датасета webvid. чтобы сбалансировать данные, авторы находят 10 000 центроид и сэмплируют из них так, чтобы равномерно распределить количество примеров между центроидами. если в одну центроиду попадает слишком много данных, часть отбрасывают. так датасет получается разнообразнее. у модели несколько видов параллелизма: тензорный (делят слои и ff-блоки между gpu), контекстный (делят токены между процессами) и параллелизм по данным. это помогает обрабатывать длинные последовательности, возникающие при генерации в высоком разрешении. также модель поддерживает cfg и guidance distillation — учитель и ученик, как обычно. ученик учится повторять учителя по результатам генерации. для переписывания промптов используют hunyuan large language model — особенно если исходный текст слишком технический. есть отдельная аудиомодель, которая по сгенерённому видео создаёт музыку. она учится на спектрограммах и работает в духе audiogen. ещё есть возможности персонализации: можно подать референс-картинку и получить видео. модель справляется с аватарами, движущимися портретами и анимацией объектов. авторы собрали свой бенчмарк из 1533 промптов и сравнились с пятью сильными бейзлайнами. публикуют не всё: выкладывают 600 промптов. смотрят на соответствие тексту, движение, визуальное качество и общее впечатление. их модель лидирует, но не с гигантским отрывом. оценки flops — без подробностей, так что сравнивать с другими моделями сложно. разбор подготовил ❣ денис кузнеделев cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-05-12T08:05:59+00:00" href="./posts/137.html">2025-05-12 08:05 UTC</a></div>
      </div>
      <div class="post-body"><strong>HunyuanVideo: A Systematic Framework For Large Video Generative Models</strong><br><strong><br></strong>Сегодня разбираем <a href="https://arxiv.org/abs/2412.03603" rel="nofollow noopener noreferrer">статью</a> от команды Tencent о HunyuanVideo — большой генеративной модели для видео. Работа во многом напоминает <a href="https://arxiv.org/abs/2410.13720" rel="nofollow noopener noreferrer">MovieGen</a>, но есть некоторые важные отличия. А главное — веса модели выложены в открытый доступ, что редкость для видеомоделей.<br><br>Обучение начинается с картинок 256×256, потом разрешение повышают до 512×512. При этом 256×256 всё ещё поддерживается — чтобы не терять навык генерации на этом уровне. Сначала учат только на изображениях, потом добавляют видео. <br><br>Генерация стартует с нормального распределения, стандартного для диффузионок. Но вместо линейно-квадратичного расписания шагов из MovieGen, здесь применяется «сдвинутое» специальным образом расписание. Авторы говорят, что такой сдвиг даёт лучшее качество, чем квадратичное расписание, особенно при уменьшении количества шагов инференса.<br><br>Видео для обучения берут из датасета WebVid. Чтобы сбалансировать данные, авторы находят 10 000 центроид и сэмплируют из них так, чтобы равномерно распределить количество примеров между центроидами. Если в одну центроиду попадает слишком много данных, часть отбрасывают. Так датасет получается разнообразнее.<br><br>У модели несколько видов параллелизма: тензорный (делят слои и FF-блоки между GPU), контекстный (делят токены между процессами) и параллелизм по данным. Это помогает обрабатывать длинные последовательности, возникающие при генерации в высоком разрешении.<br><br>Также модель поддерживает CFG и guidance distillation — учитель и ученик, как обычно. Ученик учится повторять учителя по результатам генерации. Для переписывания промптов используют Hunyuan Large Language Model — особенно если исходный текст слишком технический.<br><br>Есть отдельная аудиомодель, которая по сгенерённому видео создаёт музыку. Она учится на спектрограммах и работает в духе AudioGen.<br><br>Ещё есть возможности персонализации: можно подать референс-картинку и получить видео. Модель справляется с аватарами, движущимися портретами и анимацией объектов.<br><br>Авторы собрали свой бенчмарк из 1533 промптов и сравнились с пятью сильными бейзлайнами. Публикуют не всё: выкладывают 600 промптов. Смотрят на соответствие тексту, движение, визуальное качество и общее впечатление. Их модель лидирует, но не с гигантским отрывом. Оценки FLOPs — без подробностей, так что сравнивать с другими моделями сложно.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Денис Кузнеделев</em><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/137_480.webp" srcset="../assets/media/thumbs/137_480.webp 480w, ../assets/media/137.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="137" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/138_480.webp" srcset="../assets/media/thumbs/138_480.webp 480w, ../assets/media/138.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="137" data-image-index="1" /></div></div>
      <div class="actions">
        <span>2 245 просмотров · 18 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/137" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/137.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="129" data-search="ещё немного этих мягких французских постеров с iclr наши инженеры и исследователи продолжают делиться своими находками на тему cv — а мы несём их вам, чтобы обеспечить полезным чтением в короткую праздничную неделю. solving video inverse problems using image diffusion models авторы предлагают разбить генерацию видео с помощью диффузионных моделей на этапы: сначала покадровая генерация, затем синхронизация кадров по времени. говорят, что получается быстрее и с хорошим качеством. deep random features for scalable interpolation of spatiotemporal data работа напоминает nerf, но для remote sensing данных. орбитальные спутники не дают плотную картинку по пространству и времени, поэтому авторы предлагают научиться генерации по координатам «пространство-время», которые измерил бы спутник в этот момент. century: a framework and dataset for evaluating ethical contextualisation of sensitive images исследователи из deepmind предлагают новый бенчмарк для оценки понимания моделями разных исторических событий, стратифицированных по типам связанных сущностей (люди, места и прочее) и по типу входных данных. no training, no problem: rethinking classifier-free guidance for diffusion models пара годных хаков для улучшения classifier-free guidance (cfg): - unconditional-эмбеддинги можно заменить на рандомные текстовые токены; - можно делать negative guidance на рандомные таймстемпы. rare-to-frequent: unlocking compositional generation power of diffusion models on rare concepts with llm guidance в этой работе помогают диффузионной модели лучше генерировать редкие концепты. для этого с помощью llm находят похожий, но более частый концепт и во время генерации используют информацию от обоих: редкого и частого. how much is a noisy image worth? data scaling laws for ambient diffusion авторы переформулируют лосс для зашумлённых изображений в диффузии, чтобы не отбрасывать данные и использовать их для обучения. сейчас они готовят продолжение работы с разбором гиперпараметров. a decade’s battle on dataset bias: are we there yet? забавный факт: имея классификатор с 7 тысячами параметров, можно с высокой точностью определить, к какому датасету принадлежит фотография. размер базы — более 3 миллиардов изображений. hd-painter: high-resolution and prompt-faithful text-guided image inpainting with diffusion models работа от picsart с улучшением инпеинтинга. решают проблему того, что диффузионка сильнее опирается на картинку, чем на промпт. для этого «перевешивают» аттеншн-мапы в селф-аттеншн по аттеншн-мапам из кросс-аттеншна. говорят, работает очень робастно. работы отобрали и прокомментировали ❣ пётр вытовтов, алексей спасёнов, сергей овчаренко, александр шишеня, евгений ляпустин, иван балашов cv time #yaiclr ещё немного этих мягких французских постеров с iclr наши инженеры и исследователи продолжают делиться своими находками на тему cv — а мы несём их вам, чтобы обеспечить полезным чтением в короткую праздничную неделю. solving video inverse problems using image diffusion models авторы предлагают разбить генерацию видео с помощью диффузионных моделей на этапы: сначала покадровая генерация, затем синхронизация кадров по времени. говорят, что получается быстрее и с хорошим качеством. deep random features for scalable interpolation of spatiotemporal data работа напоминает nerf, но для remote sensing данных. орбитальные спутники не дают плотную картинку по пространству и времени, поэтому авторы предлагают научиться генерации по координатам «пространство-время», которые измерил бы спутник в этот момент. century: a framework and dataset for evaluating ethical contextualisation of sensitive images исследователи из deepmind предлагают новый бенчмарк для оценки понимания моделями разных исторических событий, стратифицированных по типам связанных сущностей (люди, места и прочее) и по типу входных данных. no training, no problem: rethinking classifier-free guidance for diffusion models пара годных хаков для улучшения classifier-free guidance (cfg): - unconditional-эмбеддинги можно заменить на рандомные текстовые токены; - можно делать negative guidance на рандомные таймстемпы. rare-to-frequent: unlocking compositional generation power of diffusion models on rare concepts with llm guidance в этой работе помогают диффузионной модели лучше генерировать редкие концепты. для этого с помощью llm находят похожий, но более частый концепт и во время генерации используют информацию от обоих: редкого и частого. how much is a noisy image worth? data scaling laws for ambient diffusion авторы переформулируют лосс для зашумлённых изображений в диффузии, чтобы не отбрасывать данные и использовать их для обучения. сейчас они готовят продолжение работы с разбором гиперпараметров. a decade’s battle on dataset bias: are we there yet? забавный факт: имея классификатор с 7 тысячами параметров, можно с высокой точностью определить, к какому датасету принадлежит фотография. размер базы — более 3 миллиардов изображений. hd-painter: high-resolution and prompt-faithful text-guided image inpainting with diffusion models работа от picsart с улучшением инпеинтинга. решают проблему того, что диффузионка сильнее опирается на картинку, чем на промпт. для этого «перевешивают» аттеншн-мапы в селф-аттеншн по аттеншн-мапам из кросс-аттеншна. говорят, работает очень робастно. работы отобрали и прокомментировали ❣ пётр вытовтов, алексей спасёнов, сергей овчаренко, александр шишеня, евгений ляпустин, иван балашов cv time #yaiclr">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-28T15:46:04+00:00" href="./posts/129.html">2025-04-28 15:46 UTC</a></div>
      </div>
      <div class="post-body"><strong>Ещё немного этих <del>мягких французских</del> постеров с ICLR</strong><br><br>Наши инженеры и исследователи продолжают делиться своими находками на тему CV — а мы несём их вам, чтобы обеспечить полезным чтением в короткую праздничную неделю.<br><br><a href="https://arxiv.org/abs/2409.02574" rel="nofollow noopener noreferrer"><strong>Solving Video Inverse Problems Using Image Diffusion Models</strong></a><br><br>Авторы предлагают разбить генерацию видео с помощью диффузионных моделей на этапы: сначала покадровая генерация, затем синхронизация кадров по времени. Говорят, что получается быстрее и с хорошим качеством.<br><br><a href="https://arxiv.org/abs/2412.11350" rel="nofollow noopener noreferrer"><strong>Deep Random Features for Scalable Interpolation of Spatiotemporal Data</strong></a><br><br>Работа напоминает NeRF, но для remote sensing данных. Орбитальные спутники не дают плотную картинку по пространству и времени, поэтому авторы предлагают научиться генерации по координатам «пространство-время», которые измерил бы спутник в этот момент.<br><br><a href="https://openreview.net/forum?id=1KLBvrYz3V" rel="nofollow noopener noreferrer"><strong>Century: A Framework and Dataset for Evaluating Ethical Contextualisation of Sensitive Images</strong></a><br><br>Исследователи из DeepMind предлагают новый бенчмарк для оценки понимания моделями разных исторических событий, стратифицированных по типам связанных сущностей (люди, места и прочее) и по типу входных данных.<br><br><a href="https://arxiv.org/abs/2407.02687" rel="nofollow noopener noreferrer"><strong>No Training, No Problem: Rethinking Classifier-Free Guidance for Diffusion Models</strong></a><br><br>Пара годных хаков для улучшения Classifier-Free Guidance (CFG): <br>- unconditional-эмбеддинги можно заменить на рандомные текстовые токены;<br>- можно делать negative guidance на рандомные таймстемпы.<br><br><a href="https://arxiv.org/abs/2410.22376" rel="nofollow noopener noreferrer"><strong>Rare-to-Frequent: Unlocking Compositional Generation Power of Diffusion Models on Rare Concepts with LLM Guidance</strong></a><br><br>В этой работе помогают диффузионной модели лучше генерировать редкие концепты. Для этого с помощью LLM находят похожий, но более частый концепт и во время генерации используют информацию от обоих: редкого и частого.<br><br><a href="https://arxiv.org/abs/2411.02780" rel="nofollow noopener noreferrer"><strong>How much is a noisy image worth? Data Scaling Laws for Ambient Diffusion</strong></a><br><br>Авторы переформулируют лосс для зашумлённых изображений в диффузии, чтобы не отбрасывать данные и использовать их для обучения. Сейчас они готовят продолжение работы с разбором гиперпараметров.<br><strong><br></strong><a href="https://arxiv.org/abs/2403.08632" rel="nofollow noopener noreferrer"><strong>A Decade’s Battle on Dataset Bias: Are We There Yet?</strong></a><br><br>Забавный факт: имея классификатор с 7 тысячами параметров, можно с высокой точностью определить, к какому датасету принадлежит фотография. Размер базы — более 3 миллиардов изображений.<br><br><a href="https://arxiv.org/abs/2312.14091" rel="nofollow noopener noreferrer"><strong>HD-Painter: High-Resolution and Prompt-Faithful Text-Guided Image Inpainting with Diffusion Models</strong></a><br><br>Работа от PicsArt с улучшением инпеинтинга. Решают проблему того, что диффузионка сильнее опирается на картинку, чем на промпт. Для этого «перевешивают» аттеншн-мапы в селф-аттеншн по аттеншн-мапам из кросс-аттеншна. Говорят, работает очень робастно.<br><br><em>Работы отобрали и прокомментировали </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Пётр Вытовтов, Алексей Спасёнов, Сергей Овчаренко, Александр Шишеня, Евгений Ляпустин, Иван Балашов</em><br><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time </a><br><br>#YaICLR<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/129_480.webp" srcset="../assets/media/thumbs/129_480.webp 480w, ../assets/media/129.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="129" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/130_480.webp" srcset="../assets/media/thumbs/130_480.webp 480w, ../assets/media/130.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="129" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/131_480.webp" srcset="../assets/media/thumbs/131_480.webp 480w, ../assets/media/131.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="129" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/132_480.webp" srcset="../assets/media/thumbs/132_480.webp 480w, ../assets/media/132.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="129" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/133_480.webp" srcset="../assets/media/thumbs/133_480.webp 480w, ../assets/media/133.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="129" data-image-index="4" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/134_480.webp" srcset="../assets/media/thumbs/134_480.webp 480w, ../assets/media/134.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="129" data-image-index="5" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/135_480.webp" srcset="../assets/media/thumbs/135_480.webp 480w, ../assets/media/135.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="129" data-image-index="6" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/136_480.webp" srcset="../assets/media/thumbs/136_480.webp 480w, ../assets/media/136.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="129" data-image-index="7" /></div></div>
      <div class="actions">
        <span>1 711 просмотров · 16 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/129" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/129.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="128" data-search="iclr 2025 выходит на финишную прямую! мы внимательно следили за работами на конференции и собрали в одном посте все наши обзоры: - приветственный пост от ребят из cv-команды - подборка интересных работ. часть 1 - репортаж с первого invited talk - немного атмосферных фото и видео - подборка интересных работ. часть 2 - подборка интересных работ. часть 3 оставайтесь с нами, впереди более подробные разборы. а на видео — ещё немного сингапура. больше разборов, интересных постеров, фото и видео с iclr вы найдёте в наших других каналах: @recsyschannel, @mlunderhood, @stuffynlp, @speechinfo. cv time #yaiclr iclr 2025 выходит на финишную прямую! мы внимательно следили за работами на конференции и собрали в одном посте все наши обзоры: - приветственный пост от ребят из cv-команды - подборка интересных работ. часть 1 - репортаж с первого invited talk - немного атмосферных фото и видео - подборка интересных работ. часть 2 - подборка интересных работ. часть 3 оставайтесь с нами, впереди более подробные разборы. а на видео — ещё немного сингапура. больше разборов, интересных постеров, фото и видео с iclr вы найдёте в наших других каналах: @recsyschannel , @mlunderhood , @stuffynlp , @speechinfo . cv time #yaiclr">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-27T08:05:47+00:00" href="./posts/128.html">2025-04-27 08:05 UTC</a></div>
      </div>
      <div class="post-body"><strong>ICLR 2025 выходит на финишную прямую!<br></strong><br>Мы внимательно следили за работами на конференции и собрали в одном посте все наши обзоры:<br><br>- <a href="https://t.me/timeforcv/95" rel="nofollow noopener noreferrer">Приветственный пост от ребят из CV-команды<br>-</a> <a href="https://t.me/timeforcv/98?single" rel="nofollow noopener noreferrer">Подборка интересных работ. Часть 1</a><br>- <a href="https://t.me/timeforcv/106" rel="nofollow noopener noreferrer">Репортаж с первого Invited Talk </a><br>- <a href="https://t.me/timeforcv/111" rel="nofollow noopener noreferrer">Немного атмосферных фото и видео</a><br>- <a href="https://t.me/timeforcv/116" rel="nofollow noopener noreferrer">Подборка интересных работ. Часть 2</a><br>- <a href="https://t.me/timeforcv/123" rel="nofollow noopener noreferrer">Подборка интересных работ. Часть 3</a><br><br>Оставайтесь с нами, впереди более подробные разборы. А на видео — ещё немного Сингапура.<br><br><em>Больше разборов, интересных постеров, фото и видео с ICLR вы найдёте в наших других каналах: </em><em>@RecSysChannel</em><em>, </em><em>@MLunderhood</em><em>, </em><em>@stuffyNLP</em><em>, </em><em>@speechinfo</em><em>.</em><br><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a> <br><br>#YaICLR<div class="media"><video controls preload="metadata" src="../assets/media/128_IMG_6543__1_.mp4"></video></div></div>
      <div class="actions">
        <span>1 744 просмотров · 21 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/128" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/128.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="123" data-search="подборка интересных постеров и статей с iclr 2025. часть 3 mia-bench: towards better instruction following evaluation of multimodal llms неплохой бенчмарк на следование инструкциям, но уже достаточно простой для топ-моделей. автор говорит, что команда старалась сделать его не субъективным, и утверждает, что на небольшом семпле llm работает с точностью выше 90%. physics-informed diffusion models авторы говорят, что раз pinn&#x27;ы до сих пор нормально не работают, можно попробовать добавить физические ограничения в диффузионки. на простых примерах выглядит хорошо (но и pinn&#x27;ы были неплохими), а как будет на сложных — пока непонятно. do vision-language models represent space and how? evaluating spatial frame of reference under ambiguities статья об изучении пространственных bias’ов в vlm. оказывается, они плохо отвечают на вопросы про расположение с чьей-то перспективы (например, если рассматривать расположение относительно камеры или другого объекта в кадре). при этом в разных языках такое описание взаимного расположения объектов может строиться по-разному. и vlm, конечно же, смещены в сторону того, как это работает в английском, даже если они мультилингвальные (что потенциально ведет к проблемам с языками с другой системой описаний). param∆ for direct weight mixing: post-train large language model at zero cost при обновлении бейзлайна llm (например, с v1 на v2, если у них не изменилась архитектура) можно не переобучать его под задачу, а вычесть веса старого бейзлайна (v1), добавить веса нового (v2) и радоваться жизни с таким «бесплатным» обучением. работает хуже дообучения на новом бейзлайне, но лучше, чем отсутствие дообучения. авторы экспериментировали только с llama 3, llama 3.1 и полным файнтьюном модели под задачу. multimodal unsupervised domain generalization by retrieving across the modality gap улучшают ann через уточнённые эмбеддинги объектов на основе аугментации текстов, описывающих интересующие классы. центроиды картинок смещаются к их усреднённым положениям относительно эмбеддингов аугментированных запросов. работы отобрали и прокомментировали ❣ екатерина глазкова, ирина барская, пётр вытовтов, алексей спасёнов cv time #yaiclr подборка интересных постеров и статей с iclr 2025. часть 3 mia-bench: towards better instruction following evaluation of multimodal llms неплохой бенчмарк на следование инструкциям, но уже достаточно простой для топ-моделей. автор говорит, что команда старалась сделать его не субъективным, и утверждает, что на небольшом семпле llm работает с точностью выше 90%. physics-informed diffusion models авторы говорят, что раз pinn&amp;#x27;ы до сих пор нормально не работают, можно попробовать добавить физические ограничения в диффузионки. на простых примерах выглядит хорошо (но и pinn&amp;#x27;ы были неплохими), а как будет на сложных — пока непонятно. do vision-language models represent space and how? evaluating spatial frame of reference under ambiguities статья об изучении пространственных bias’ов в vlm. оказывается, они плохо отвечают на вопросы про расположение с чьей-то перспективы (например, если рассматривать расположение относительно камеры или другого объекта в кадре). при этом в разных языках такое описание взаимного расположения объектов может строиться по-разному. и vlm, конечно же, смещены в сторону того, как это работает в английском, даже если они мультилингвальные (что потенциально ведет к проблемам с языками с другой системой описаний). param∆ for direct weight mixing: post-train large language model at zero cost при обновлении бейзлайна llm (например, с v1 на v2, если у них не изменилась архитектура) можно не переобучать его под задачу, а вычесть веса старого бейзлайна (v1), добавить веса нового (v2) и радоваться жизни с таким «бесплатным» обучением. работает хуже дообучения на новом бейзлайне, но лучше, чем отсутствие дообучения. авторы экспериментировали только с llama 3, llama 3.1 и полным файнтьюном модели под задачу. multimodal unsupervised domain generalization by retrieving across the modality gap улучшают ann через уточнённые эмбеддинги объектов на основе аугментации текстов, описывающих интересующие классы. центроиды картинок смещаются к их усреднённым положениям относительно эмбеддингов аугментированных запросов. работы отобрали и прокомментировали ❣ екатерина глазкова, ирина барская, пётр вытовтов, алексей спасёнов cv time #yaiclr">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-26T13:06:15+00:00" href="./posts/123.html">2025-04-26 13:06 UTC</a></div>
      </div>
      <div class="post-body"><strong>Подборка интересных постеров и статей с ICLR 2025. Часть 3</strong><br><br><a href="https://arxiv.org/abs/2407.01509" rel="nofollow noopener noreferrer"><strong>MIA-Bench: Towards Better Instruction Following Evaluation of Multimodal LLMs</strong></a><br><br>Неплохой бенчмарк на следование инструкциям, но уже достаточно простой для топ-моделей. Автор говорит, что команда старалась сделать его не субъективным, и утверждает, что на небольшом семпле LLM работает с точностью выше 90%. <br><br><a href="https://arxiv.org/abs/2403.14404" rel="nofollow noopener noreferrer"><strong>Physics-Informed Diffusion Models</strong></a><br><br>Авторы говорят, что раз PINN&#x27;ы до сих пор нормально не работают, можно попробовать добавить физические ограничения в диффузионки. На простых примерах выглядит хорошо (но и PINN&#x27;ы были неплохими), а как будет на сложных — пока непонятно.<br><br><a href="https://arxiv.org/abs/2410.17385" rel="nofollow noopener noreferrer"><strong>Do Vision-Language Models Represent Space and How? Evaluating Spatial Frame of Reference Under Ambiguities</strong></a><br><br>Статья об изучении пространственных bias’ов в VLM. Оказывается, они плохо отвечают на вопросы про расположение с чьей-то перспективы (например, если рассматривать расположение относительно камеры или другого объекта в кадре). При этом в разных языках такое описание взаимного расположения объектов может строиться по-разному. И VLM, конечно же, смещены в сторону того, как это работает в английском, даже если они мультилингвальные (что потенциально ведет к проблемам с языками с другой системой описаний).<br><br><a href="https://openreview.net/forum?id=vqbd2OQnGp" rel="nofollow noopener noreferrer"><strong>Param∆ for Direct Weight Mixing: Post-Train Large Language Model at Zero Cost</strong></a><br><br>При обновлении бейзлайна LLM (например, с v1 на v2, если у них не изменилась архитектура) можно не переобучать его под задачу, а вычесть веса старого бейзлайна (v1), добавить веса нового (v2) и радоваться жизни с таким «бесплатным» обучением. Работает хуже дообучения на новом бейзлайне, но лучше, чем отсутствие дообучения. Авторы экспериментировали только с Llama 3, Llama 3.1 и полным файнтьюном модели под задачу.<br><br><a href="https://arxiv.org/abs/2402.04416" rel="nofollow noopener noreferrer"><strong>Multimodal Unsupervised Domain Generalization by Retrieving Across the Modality Gap</strong></a><br><br>Улучшают ANN через уточнённые эмбеддинги объектов на основе аугментации текстов, описывающих интересующие классы. Центроиды картинок смещаются к их усреднённым положениям относительно эмбеддингов аугментированных запросов.<br><br><em>Работы отобрали и прокомментировали </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Екатерина Глазкова, Ирина Барская, Пётр Вытовтов, Алексей Спасёнов</em><br><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a> <br><br>#YaICLR<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/123_480.webp" srcset="../assets/media/thumbs/123_480.webp 480w, ../assets/media/123.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="123" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/124_480.webp" srcset="../assets/media/thumbs/124_480.webp 480w, ../assets/media/124.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="123" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/125_480.webp" srcset="../assets/media/thumbs/125_480.webp 480w, ../assets/media/125.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="123" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/126_480.webp" srcset="../assets/media/thumbs/126_480.webp 480w, ../assets/media/126.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="123" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/127_480.webp" srcset="../assets/media/thumbs/127_480.webp 480w, ../assets/media/127.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="123" data-image-index="4" /></div></div>
      <div class="actions">
        <span>1 542 просмотров · 14 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/123" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/123.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="116" data-search="подборка интересных постеров и статей с iclr 2025. часть 2 iclr пока не думает заканчиваться — посему несём вам новую (большую) порцию избранных работ. во многих случаях нашим инженерам удалось поговорить с авторами, поэтому идеи буквально из первых рук. can we talk models into seeing the world differently любопытная работа, изучающая робастность vlm к текстурным bias&#x27;ам: получается лучше, чем при обучении на imagenet, но пока хуже, чем у людей. непонятно, дело в данных, в другом методе обучения или в том, что большинство картиночных энкодеров — clip-like. a simple approach to unifying diffusion-based conditional generation параллельно учат две диффузионки, связанные кросс-аттеншнами: одну — для картинок, другую — для карты глубины. таймстемпы семплируются независимо. на инференсе можно генерировать любую из модальностей, независимо или одновременно. модель без дообучения обобщается на большее число веток, позволяя выполнять редактирование изображений. less is more: masking elements in image condition features avoids content leakages in style transfer diffusion models disenvisioner: disentangled and enriched visual prompt for customized image generation пара работ по улучшению стилизации и персонализации. идеи довольно похожие: в первой — в пространстве clip&#x27;a понимают, какие картиночные фичи соответствуют концепту, и маскируют их. во второй — учат адаптер с двумя токенами (релевантные и нерелевантные эмбеды) — выкидывая вторые на инференсе. diffusion models are real time game engines doom запустили на диффузионках. демо впечатляет тем, что модель запоминает локации. кажется, что это большой прогресс. duoduo clip: efficient 3d understanding with multi-view images незатейливо вставляют и дообучают в clip multi-view attention слои, чтобы получить multi-view-модель. её эмбеддинги можно использовать взаимозаменяемо с clip-эмбедами в поиске по базам данных с ростом качества. работы отобрали и прокомментировали ❣ александр шишеня, сергей овчаренко, иван балашов, расим ахунзянов cv time #yaiclr подборка интересных постеров и статей с iclr 2025. часть 2 iclr пока не думает заканчиваться — посему несём вам новую (большую) порцию избранных работ. во многих случаях нашим инженерам удалось поговорить с авторами, поэтому идеи буквально из первых рук. can we talk models into seeing the world differently любопытная работа, изучающая робастность vlm к текстурным bias&amp;#x27;ам: получается лучше, чем при обучении на imagenet, но пока хуже, чем у людей. непонятно, дело в данных, в другом методе обучения или в том, что большинство картиночных энкодеров — clip-like. a simple approach to unifying diffusion-based conditional generation параллельно учат две диффузионки, связанные кросс-аттеншнами: одну — для картинок, другую — для карты глубины. таймстемпы семплируются независимо. на инференсе можно генерировать любую из модальностей, независимо или одновременно. модель без дообучения обобщается на большее число веток, позволяя выполнять редактирование изображений. less is more: masking elements in image condition features avoids content leakages in style transfer diffusion models disenvisioner: disentangled and enriched visual prompt for customized image generation пара работ по улучшению стилизации и персонализации. идеи довольно похожие: в первой — в пространстве clip&amp;#x27;a понимают, какие картиночные фичи соответствуют концепту, и маскируют их. во второй — учат адаптер с двумя токенами (релевантные и нерелевантные эмбеды) — выкидывая вторые на инференсе. diffusion models are real time game engines doom запустили на диффузионках. демо впечатляет тем, что модель запоминает локации. кажется, что это большой прогресс. duoduo clip: efficient 3d understanding with multi-view images незатейливо вставляют и дообучают в clip multi-view attention слои, чтобы получить multi-view-модель. её эмбеддинги можно использовать взаимозаменяемо с clip-эмбедами в поиске по базам данных с ростом качества. работы отобрали и прокомментировали ❣ александр шишеня, сергей овчаренко, иван балашов, расим ахунзянов cv time #yaiclr">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-26T08:02:36+00:00" href="./posts/116.html">2025-04-26 08:02 UTC</a></div>
      </div>
      <div class="post-body"><strong>Подборка интересных постеров и статей с ICLR 2025. Часть 2</strong><br><br>ICLR пока не думает заканчиваться — посему несём вам новую (большую) порцию избранных работ. Во многих случаях нашим инженерам удалось поговорить с авторами, поэтому идеи буквально из первых рук.<br><br><a href="https://arxiv.org/abs/2403.09193" rel="nofollow noopener noreferrer"><strong>Can We Talk Models Into Seeing the World Differently</strong></a><br><br>Любопытная работа, изучающая робастность VLM к текстурным bias&#x27;ам: получается лучше, чем при обучении на Imagenet, но пока хуже, чем у людей. Непонятно, дело в данных, в другом методе обучения или в том, что большинство картиночных энкодеров — CLIP-like.<br><br><a href="https://arxiv.org/abs/2410.11439" rel="nofollow noopener noreferrer"><strong>A Simple Approach to Unifying Diffusion-based Conditional Generation</strong></a><br><br>Параллельно учат две диффузионки, связанные кросс-аттеншнами: одну — для картинок, другую — для карты глубины. Таймстемпы семплируются независимо. На инференсе можно генерировать любую из модальностей, независимо или одновременно. Модель без дообучения обобщается на большее число веток, позволяя выполнять редактирование изображений.<br><br><a href="https://arxiv.org/abs/2502.07466" rel="nofollow noopener noreferrer"><strong>Less is More: Masking Elements in Image Condition Features Avoids Content Leakages in Style Transfer Diffusion Models</strong></a><br><br><a href="https://arxiv.org/abs/2410.02067" rel="nofollow noopener noreferrer"><strong>DisEnvisioner: Disentangled and Enriched Visual Prompt for Customized Image Generation</strong></a><br><br>Пара работ по улучшению стилизации и персонализации. Идеи довольно похожие: в первой — в пространстве CLIP&#x27;a понимают, какие картиночные фичи соответствуют концепту, и маскируют их. Во второй — учат адаптер с двумя токенами (релевантные и нерелевантные эмбеды) — выкидывая вторые на инференсе.<br><br><a href="https://arxiv.org/abs/2408.14837" rel="nofollow noopener noreferrer"><strong>Diffusion Models Are Real Time Game Engines</strong></a><br><br>Doom запустили на диффузионках. <a href="https://gamengen.github.io/" rel="nofollow noopener noreferrer">Демо впечатляет </a>тем, что модель запоминает локации. Кажется, что это большой прогресс.<br><br><a href="https://3dlg-hcvc.github.io/DuoduoCLIP/" rel="nofollow noopener noreferrer"><strong>Duoduo CLIP: Efficient 3D Understanding with Multi-View Images</strong></a><br><br>Незатейливо вставляют и дообучают в CLIP multi-view attention слои, чтобы получить multi-view-модель. Её эмбеддинги можно использовать взаимозаменяемо с CLIP-эмбедами в поиске по базам данных с ростом качества.<br><br><em>Работы отобрали и прокомментировали </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em>  Александр Шишеня, Сергей Овчаренко, Иван Балашов, Расим Ахунзянов<br></em><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a> <br><br>#YaICLR<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/116_480.webp" srcset="../assets/media/thumbs/116_480.webp 480w, ../assets/media/116.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="116" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/117_480.webp" srcset="../assets/media/thumbs/117_480.webp 480w, ../assets/media/117.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="116" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/118_480.webp" srcset="../assets/media/thumbs/118_480.webp 480w, ../assets/media/118.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="116" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/119_480.webp" srcset="../assets/media/thumbs/119_480.webp 480w, ../assets/media/119.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="116" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/120_480.webp" srcset="../assets/media/thumbs/120_480.webp 480w, ../assets/media/120.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="116" data-image-index="4" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/121_480.webp" srcset="../assets/media/thumbs/121_480.webp 480w, ../assets/media/121.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="116" data-image-index="5" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/122_480.webp" srcset="../assets/media/thumbs/122_480.webp 480w, ../assets/media/122.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="116" data-image-index="6" /></div></div>
      <div class="actions">
        <span>1 320 просмотров · 16 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/116" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/116.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="111" data-search="трушный конф-виб iclr 2025 - наша cv-команда в сборе; - авторы статьи «adam: a method for stochastic optimization», получившей на iclr 2025 test-of-time award; - фото избушки, в которой была написана одна из статей; - и, конечно же, роботы, куда без них. cv time #yaiclr трушный конф-виб iclr 2025 - наша cv-команда в сборе; - авторы статьи « adam: a method for stochastic optimization », получившей на iclr 2025 test-of-time award; - фото избушки, в которой была написана одна из статей; - и, конечно же, роботы, куда без них. cv time #yaiclr">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-25T10:45:40+00:00" href="./posts/111.html">2025-04-25 10:45 UTC</a></div>
      </div>
      <div class="post-body"><strong>Трушный конф-виб</strong> <strong>ICLR 2025<br></strong><br>- наша CV-команда в сборе; <br>- авторы статьи «<a href="https://arxiv.org/abs/1412.6980" rel="nofollow noopener noreferrer">Adam:</a> <a href="https://arxiv.org/abs/1412.6980" rel="nofollow noopener noreferrer">A Method for Stochastic Optimization</a>», <a href="https://blog.iclr.cc/2025/04/14/announcing-the-test-of-time-award-winners-from-iclr-2015/" rel="nofollow noopener noreferrer">получившей</a> на ICLR 2025 Test-of-Time Award; <br>- фото избушки, в которой была написана одна из статей; <br>- и, конечно же, роботы, куда без них. <br><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><br><br>#YaICLR<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/111_480.webp" srcset="../assets/media/thumbs/111_480.webp 480w, ../assets/media/111.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="111" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/112_480.webp" srcset="../assets/media/thumbs/112_480.webp 480w, ../assets/media/112.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="111" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/113_480.webp" srcset="../assets/media/thumbs/113_480.webp 480w, ../assets/media/113.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="111" data-image-index="2" /><video controls preload="metadata" src="../assets/media/114_______.mp4"></video></div></div>
      <div class="actions">
        <span>1 492 просмотров · 26 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/111" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/111.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="106" data-search="building safe and robust ai systems — первый invited talk хоть название и указывает на ai safety, поначалу речь шла совсем о другом. докладчик вспоминал, какими разнообразными и смелыми были идеи в исследованиях прошлых лет — и констатировал некоторый застой в академическом ресёрче. подробнее об идеях из старых статей: — input convex neural networks: построение выпуклой нейросети, которую можно использовать как energy-based модель. — optimization as a layer: слой сети внутри себя решает задачу оптимизации. — deep equilibrium models: по сути, rnn, но в итоге это направление заглохло, потому что на практике вычислительно сложные модели с малым количеством параметров уступают перепараметризованным. — fighting adversarial samples: подходы так и не прижились, потому что все методы заметно ухудшают качество, при этом не гарантируют полное избавление от adversarial samples. просим прощения — фото слайдов немного не в фокусе. но так даже вайбовее и в духе старых статей. послушал презентацию и записал тезисы ❣ александр шишеня cv time #yaiclr building safe and robust ai systems — первый invited talk хоть название и указывает на ai safety, поначалу речь шла совсем о другом. докладчик вспоминал, какими разнообразными и смелыми были идеи в исследованиях прошлых лет — и констатировал некоторый застой в академическом ресёрче. подробнее об идеях из старых статей: — input convex neural networks : построение выпуклой нейросети, которую можно использовать как energy-based модель. — optimization as a layer : слой сети внутри себя решает задачу оптимизации. — deep equilibrium models : по сути, rnn, но в итоге это направление заглохло, потому что на практике вычислительно сложные модели с малым количеством параметров уступают перепараметризованным. — fighting adversarial samples : подходы так и не прижились, потому что все методы заметно ухудшают качество, при этом не гарантируют полное избавление от adversarial samples. просим прощения — фото слайдов немного не в фокусе. но так даже вайбовее и в духе старых статей. послушал презентацию и записал тезисы ❣ александр шишеня cv time #yaiclr">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-24T14:32:47+00:00" href="./posts/106.html">2025-04-24 14:32 UTC</a></div>
      </div>
      <div class="post-body"><a href="https://iclr.cc/virtual/2025/invited-talk/36782" rel="nofollow noopener noreferrer"><strong>Building Safe and Robust AI Systems</strong></a><strong> — первый Invited Talk </strong><br><br>Хоть название и указывает на AI Safety, поначалу речь шла совсем о другом. Докладчик вспоминал, какими разнообразными и смелыми были идеи в исследованиях прошлых лет — и констатировал некоторый застой в академическом ресёрче. Подробнее об идеях из старых статей:<br><br>— <strong>Input Convex Neural Networks</strong>: построение выпуклой нейросети, которую можно использовать как energy-based модель.<br><br>— <strong>Optimization as a Layer</strong>: слой сети внутри себя решает задачу оптимизации.<br><br>— <strong>Deep Equilibrium Models</strong>: по сути, RNN, но в итоге это направление заглохло, потому что на практике вычислительно сложные модели с малым количеством параметров уступают перепараметризованным.<br><br>— <strong>Fighting Adversarial Samples</strong>: подходы так и не прижились, потому что все методы заметно ухудшают качество, при этом не гарантируют полное избавление от adversarial samples.<br><br>Просим прощения — фото слайдов немного не в фокусе. Но так даже вайбовее и в духе старых статей.<br><br><em>Послушал презентацию и записал тезисы</em> <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Александр Шишеня</em><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><br><br>#YaICLR<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/106_480.webp" srcset="../assets/media/thumbs/106_480.webp 480w, ../assets/media/106.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="106" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/107_480.webp" srcset="../assets/media/thumbs/107_480.webp 480w, ../assets/media/107.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="106" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/108_480.webp" srcset="../assets/media/thumbs/108_480.webp 480w, ../assets/media/108.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="106" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/109_480.webp" srcset="../assets/media/thumbs/109_480.webp 480w, ../assets/media/109.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="106" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/110_480.webp" srcset="../assets/media/thumbs/110_480.webp 480w, ../assets/media/110.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="106" data-image-index="4" /></div></div>
      <div class="actions">
        <span>1 506 просмотров · 13 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/106" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/106.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="98" data-search="подборка интересных постеров и статей с iclr 2025. часть 1 наши корреспонденты уже принесли обзоры работ первого дня конференции, 24 апреля. пишите в комментариях, какие идеи зацепили и о чём ещё хотите узнать. eagle: exploring the design space for multi-modal llms with mixture of encoders авторы из nvidia исследуют, как использование нескольких предобученных vision-энкодеров помогает моделям лучше справляться со сложными визуальными задачами, такими как ocr и анализ документов. каждый энкодер обучали отдельно с зафриженной 7b моделью vicuna. на заключительных этапах объединяли все экодеры и обучали финальную модель. heavy tailed diffusion models у распределения картинок тяжёлый хвост, у нормального — лёгкий. есть теоретическое обоснование, что из-за липшицевости нейросети диффузионка не может генерировать из распределения с лёгким хвостом распределение с тяжёлым. frecas: efficient higher-resolution image generation via frequency-aware cascaded sampling генерируют картинки 2048 на основе модели, предобученной для 1024. на каждой стадии используется одна и та же модель, на ood-разрешениях подменяются аттеншн-мапы. forte: finding outliers using representation typicality estimation сделали фреймворк для автоматического нахождения аутлайеров в данных. достаточно дать данные и несколько референсных точек, и можно будет получить оценку аутлайерности. может быть полезно, например, для нахождения фото в карточках организации, которые к организации не имеют отношения. painting with words: elevating detailed image captioning with benchmark and alignment learning валидируют image captions, разбивая на крошечные утверждения и валидируя gpt (точность 85+), потом собирают два реворда на полноту и точность — пыщ-пыщ — профит. captured by captions: on memorization and its mitigation in clip models разработали метрику для выявления неправильно аннотированных текстов для обучения clip&#x27;a. утверждают, что хороший буст качества даёт группировка мисанноипованных (неправильно аннотированных) картинок в один батч. f³set: towards analyzing fast, frequent, and fine-grained events from videos находят события на сложных видео с блюром и быстрыми изменениями, например спортивных трансляциях. архитектура: детектор ивента, классификатор ивента, блендим и в gru. может быть полезно, чтобы найти красивый кадр на превью пользовательских видео. weathergfm: learning a weather generalist foundation model via in-context learning предлагают учить одну модельку под все погодные задачи. и при этом задавать визуальным промптом, что именно модели необходимо сделать: наукаст, сегментацию спутника и прочее. на первый взгляд, выглядит прикольно, но из погодных моделей сравниваются только с ifs и climax. high-dynamic radar sequence prediction for weather nowcasting using spatiotemporal coherent gaussian representation идея статьи в том, чтобы представить 3d-информацию об осадках с помощью набора гауссиан, а потом их преобразовывать, чтобы получить наукастовый прогноз. потом из новых гауссиан восстанавливается финальное поле осадков. правда extreme-ивенты авторы не рассматривают. интересные постеры увидели ❣ александр шишеня, пётр вытовтов, иван балашов, сергей овчаренко, денис асонов cv time #yaiclr подборка интересных постеров и статей с iclr 2025. часть 1 наши корреспонденты уже принесли обзоры работ первого дня конференции, 24 апреля. пишите в комментариях, какие идеи зацепили и о чём ещё хотите узнать. eagle: exploring the design space for multi-modal llms with mixture of encoders авторы из nvidia исследуют, как использование нескольких предобученных vision-энкодеров помогает моделям лучше справляться со сложными визуальными задачами, такими как ocr и анализ документов. каждый энкодер обучали отдельно с зафриженной 7b моделью vicuna. на заключительных этапах объединяли все экодеры и обучали финальную модель. heavy tailed diffusion models у распределения картинок тяжёлый хвост, у нормального — лёгкий. есть теоретическое обоснование, что из-за липшицевости нейросети диффузионка не может генерировать из распределения с лёгким хвостом распределение с тяжёлым. frecas: efficient higher-resolution image generation via frequency-aware cascaded sampling генерируют картинки 2048 на основе модели, предобученной для 1024. на каждой стадии используется одна и та же модель, на ood-разрешениях подменяются аттеншн-мапы. forte: finding outliers using representation typicality estimation сделали фреймворк для автоматического нахождения аутлайеров в данных. достаточно дать данные и несколько референсных точек, и можно будет получить оценку аутлайерности. может быть полезно, например, для нахождения фото в карточках организации, которые к организации не имеют отношения. painting with words: elevating detailed image captioning with benchmark and alignment learning валидируют image captions, разбивая на крошечные утверждения и валидируя gpt (точность 85+), потом собирают два реворда на полноту и точность — пыщ-пыщ — профит. captured by captions: on memorization and its mitigation in clip models разработали метрику для выявления неправильно аннотированных текстов для обучения clip&amp;#x27;a. утверждают, что хороший буст качества даёт группировка мисанноипованных (неправильно аннотированных) картинок в один батч. f³set: towards analyzing fast, frequent, and fine-grained events from videos находят события на сложных видео с блюром и быстрыми изменениями, например спортивных трансляциях. архитектура: детектор ивента, классификатор ивента, блендим и в gru. может быть полезно, чтобы найти красивый кадр на превью пользовательских видео. weathergfm: learning a weather generalist foundation model via in-context learning предлагают учить одну модельку под все погодные задачи. и при этом задавать визуальным промптом, что именно модели необходимо сделать: наукаст, сегментацию спутника и прочее. на первый взгляд, выглядит прикольно, но из погодных моделей сравниваются только с ifs и climax. high-dynamic radar sequence prediction for weather nowcasting using spatiotemporal coherent gaussian representation идея статьи в том, чтобы представить 3d-информацию об осадках с помощью набора гауссиан, а потом их преобразовывать, чтобы получить наукастовый прогноз. потом из новых гауссиан восстанавливается финальное поле осадков. правда extreme-ивенты авторы не рассматривают. интересные постеры увидели ❣ александр шишеня, пётр вытовтов, иван балашов, сергей овчаренко, денис асонов cv time #yaiclr">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-24T08:36:45+00:00" href="./posts/98.html">2025-04-24 08:36 UTC</a></div>
      </div>
      <div class="post-body"><strong>Подборка интересных постеров и статей с ICLR 2025. Часть 1<br></strong><br>Наши корреспонденты уже принесли обзоры работ первого дня конференции, 24 апреля. Пишите в комментариях, какие идеи зацепили и о чём ещё хотите узнать.<br><br><a href="https://arxiv.org/abs/2408.15998" rel="nofollow noopener noreferrer"><strong>Eagle: Exploring The Design Space for Multi-modal LLMs with Mixture of Encoders</strong></a><br>Авторы из NVIDIA исследуют, как использование нескольких предобученных vision-энкодеров помогает моделям лучше справляться со сложными визуальными задачами, такими как OCR и анализ документов. Каждый энкодер обучали отдельно с зафриженной 7B моделью Vicuna. На заключительных этапах объединяли все экодеры и обучали финальную модель.<br><br><a href="https://arxiv.org/abs/2410.14171" rel="nofollow noopener noreferrer"><strong>Heavy Tailed Diffusion Models</strong></a><br>У распределения картинок тяжёлый хвост, у нормального — лёгкий. Есть теоретическое обоснование, что из-за липшицевости нейросети диффузионка не может генерировать из распределения с лёгким хвостом распределение с тяжёлым.<br><br><a href="https://arxiv.org/abs/2410.18410" rel="nofollow noopener noreferrer"><strong>FreCaS: Efficient Higher-Resolution Image Generation via Frequency-aware Cascaded Sampling</strong></a><br>Генерируют картинки 2048 на основе модели, предобученной для 1024. На каждой стадии используется одна и та же модель, на ood-разрешениях подменяются аттеншн-мапы.<br><br><a href="https://arxiv.org/abs/2410.01322" rel="nofollow noopener noreferrer"><strong>FORTE: Finding Outliers using Representation Typicality Estimation</strong></a><br>Сделали фреймворк для автоматического нахождения аутлайеров в данных. Достаточно дать данные и несколько референсных точек, и можно будет получить оценку аутлайерности. Может быть полезно, например, для нахождения фото в карточках организации, которые к организации не имеют отношения.<br><br><a href="http://arxiv.org/abs/2503.07906" rel="nofollow noopener noreferrer"><strong>Painting With Words: Elevating Detailed Image Captioning with Benchmark and Alignment Learning</strong></a><br>Валидируют image captions, разбивая на крошечные утверждения и валидируя GPT (точность 85+), потом собирают два реворда на полноту и точность — пыщ-пыщ  — профит.<br><br><a href="https://arxiv.org/abs/2502.07830" rel="nofollow noopener noreferrer"><strong>Captured by Captions: On Memorization and its Mitigation in CLIP Models</strong></a><br>Разработали метрику для выявления неправильно аннотированных текстов для обучения clip&#x27;a. Утверждают, что хороший буст качества даёт группировка мисанноипованных (неправильно аннотированных) картинок в один батч.<br><br><a href="https://arxiv.org/abs/2504.08222" rel="nofollow noopener noreferrer"><strong>F³Set: Towards Analyzing Fast, Frequent, and Fine-grained Events from Videos</strong></a><br>Находят события на сложных видео с блюром и быстрыми изменениями, например спортивных трансляциях. Архитектура: детектор ивента, классификатор ивента, блендим и в GRU. Может быть полезно, чтобы найти красивый кадр на превью пользовательских видео.<br><br><a href="https://arxiv.org/abs/2411.05420" rel="nofollow noopener noreferrer"><strong>WeatherGFM: Learning A Weather Generalist Foundation Model via In-context Learning</strong></a><br>Предлагают учить одну модельку под все погодные задачи. И при этом задавать визуальным промптом, что именно модели необходимо сделать: наукаст, сегментацию спутника и прочее. На первый взгляд, выглядит прикольно, но из погодных моделей сравниваются только с IFS и ClimaX.<br><br><a href="https://arxiv.org/abs/2502.14895" rel="nofollow noopener noreferrer"><strong>High-Dynamic Radar Sequence Prediction for Weather Nowcasting Using Spatiotemporal Coherent Gaussian Representation</strong></a><br>Идея статьи в том, чтобы представить 3D-информацию об осадках с помощью набора гауссиан, а потом их преобразовывать, чтобы получить наукастовый прогноз. Потом из новых гауссиан восстанавливается финальное поле осадков. Правда extreme-ивенты авторы не рассматривают.<br><br><em>Интересные постеры увидели </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Александр Шишеня, Пётр Вытовтов, Иван Балашов, Сергей Овчаренко, Денис Асонов</em><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><br><br>#YaICLR<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/98_480.webp" srcset="../assets/media/thumbs/98_480.webp 480w, ../assets/media/98.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="98" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/99_480.webp" srcset="../assets/media/thumbs/99_480.webp 480w, ../assets/media/99.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="98" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/100_480.webp" srcset="../assets/media/thumbs/100_480.webp 480w, ../assets/media/100.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="98" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/101_480.webp" srcset="../assets/media/thumbs/101_480.webp 480w, ../assets/media/101.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="98" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/102_480.webp" srcset="../assets/media/thumbs/102_480.webp 480w, ../assets/media/102.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="98" data-image-index="4" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/103_480.webp" srcset="../assets/media/thumbs/103_480.webp 480w, ../assets/media/103.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="98" data-image-index="5" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/104_480.webp" srcset="../assets/media/thumbs/104_480.webp 480w, ../assets/media/104.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="98" data-image-index="6" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/105_480.webp" srcset="../assets/media/thumbs/105_480.webp 480w, ../assets/media/105.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="98" data-image-index="7" /></div></div>
      <div class="actions">
        <span>2 083 просмотров · 21 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/98" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/98.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="95" data-search="чемодан — аэропорт — иии… нас встречает бананово-лимонный сингапур! а уже завтра стартует iclr`25. чемодан — аэропорт — иии… нас встречает бананово-лимонный сингапур! а уже завтра стартует iclr`25.">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-23T12:19:04+00:00" href="./posts/95.html">2025-04-23 12:19 UTC</a></div>
      </div>
      <div class="post-body">Чемодан — аэропорт — иии… Нас встречает бананово-лимонный Сингапур! А уже завтра стартует ICLR`25.<div class="media"><video controls preload="metadata" src="../assets/media/95_2025-04-23_14.57.04.mp4"></video><img class="media-img" loading="lazy" src="../assets/media/thumbs/96_480.webp" srcset="../assets/media/thumbs/96_480.webp 480w, ../assets/media/96.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="95" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/97_480.webp" srcset="../assets/media/thumbs/97_480.webp 480w, ../assets/media/97.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="95" data-image-index="1" /></div></div>
      <div class="actions">
        <span>1 526 просмотров · 49 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/95" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/95.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="94" data-search="lost and found: overcoming detector failures in online multi-object tracking сегодня разбираем статью с eccv’24, в которой предложили метод улучшения онлайн-трекинга busca. это надстройка над любыми современными трекерами в парадигме tracking-by-detection (tbd), помогающая не терять треки при пропуске объектов детектором. в подходах tbd объекты сначала детектируют, затем соотносят с существующими треками. проблема в том, что даже лучшие детекторы периодически теряют объекты, особенно при их низкой видимости. из-за этого треки прерываются. busca решает эту проблему, продолжая треки даже без детекций. архитектура busca в основе метода — decision transformer, работающий полностью в онлайн-режиме (без изменения прошлых результатов или доступа к будущим кадрам). после стандартного сопоставления детекций с треками, несматченные треки обрабатываются busca, которая анализирует: — candidate proposal b: прогнозируемую позицию объекта с помощью фильтра калмана; — contextual proposals c: q=4 ближайших объекта из успешно отслеживаемых треков на текущем кадре (для учёта окружения); — learned tokens l: специальные токены [halluc.] для обнаружения искажённых треков и [miss.], когда объект покинул сцену или все предложения неподходящие. обработка токенов различается: - трековые наблюдения (несколько последних наблюдений из несматченного трека), candidate и contextual proposals проходят через resnet-50 для извлечения 512-мерных визуальных признаков; - learned tokens и разделительный токен [sep] инициализируются случайно и обучаются совместно с остальной архитектурой. все токены получают специальное пространственно-временное кодирование (ste), отражающее относительные время, размеры и расстояние до последнего наблюдения трека. затем decision transformer обрабатывает все токены вместе. полученные представления предложений проходят через mlp-слой для генерации их вероятностей. если выбирается candidate b — трек продолжается с обновлёнными координатами, в остальных случаях — ставится на паузу. эксперименты busca особенно эффективна для объектов с низкой видимостью и увеличивает среднюю длину треков. метод обучался только на синтетических данных (motsynth) и работает со скоростью около 45 мс на кадр на nvidia rtx gpu. busca протестирована на пяти трекерах (bytetrack, strongsort, ghost, transcenter, centertrack) и улучшила все ключевые метрики на mot16, mot17 и mot20. разбор подготовила ❣ мария поклонская cv time lost and found: overcoming detector failures in online multi-object tracking сегодня разбираем статью с eccv’24, в которой предложили метод улучшения онлайн-трекинга busca. это надстройка над любыми современными трекерами в парадигме tracking-by-detection (tbd), помогающая не терять треки при пропуске объектов детектором. в подходах tbd объекты сначала детектируют, затем соотносят с существующими треками. проблема в том, что даже лучшие детекторы периодически теряют объекты, особенно при их низкой видимости. из-за этого треки прерываются. busca решает эту проблему, продолжая треки даже без детекций. архитектура busca в основе метода — decision transformer, работающий полностью в онлайн-режиме (без изменения прошлых результатов или доступа к будущим кадрам). после стандартного сопоставления детекций с треками, несматченные треки обрабатываются busca, которая анализирует: — candidate proposal b : прогнозируемую позицию объекта с помощью фильтра калмана; — contextual proposals c : q=4 ближайших объекта из успешно отслеживаемых треков на текущем кадре (для учёта окружения); — learned tokens l : специальные токены [halluc.] для обнаружения искажённых треков и [miss.], когда объект покинул сцену или все предложения неподходящие. обработка токенов различается: - трековые наблюдения (несколько последних наблюдений из несматченного трека), candidate и contextual proposals проходят через resnet-50 для извлечения 512-мерных визуальных признаков; - learned tokens и разделительный токен [sep] инициализируются случайно и обучаются совместно с остальной архитектурой. все токены получают специальное пространственно-временное кодирование (ste), отражающее относительные время, размеры и расстояние до последнего наблюдения трека. затем decision transformer обрабатывает все токены вместе. полученные представления предложений проходят через mlp-слой для генерации их вероятностей. если выбирается candidate b — трек продолжается с обновлёнными координатами, в остальных случаях — ставится на паузу. эксперименты busca особенно эффективна для объектов с низкой видимостью и увеличивает среднюю длину треков. метод обучался только на синтетических данных (motsynth) и работает со скоростью около 45 мс на кадр на nvidia rtx gpu. busca протестирована на пяти трекерах (bytetrack, strongsort, ghost, transcenter, centertrack) и улучшила все ключевые метрики на mot16, mot17 и mot20. разбор подготовила ❣ мария поклонская cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-22T07:31:47+00:00" href="./posts/94.html">2025-04-22 07:31 UTC</a></div>
      </div>
      <div class="post-body"><strong>Lost and Found: Overcoming Detector Failures in Online Multi-Object Tracking</strong><br><br>Сегодня разбираем <a href="https://arxiv.org/html/2407.10151v2" rel="nofollow noopener noreferrer">статью</a> с ECCV’24, в которой предложили метод улучшения онлайн-трекинга BUSCA. Это надстройка над любыми современными трекерами в парадигме Tracking-by-Detection (TbD), помогающая не терять треки при пропуске объектов детектором.<br><br>В подходах TbD объекты сначала детектируют, затем соотносят с существующими треками. Проблема в том, что даже лучшие детекторы периодически теряют объекты, особенно при их низкой видимости. Из-за этого треки прерываются. BUSCA решает эту проблему, продолжая треки даже без детекций.<br><br><strong>Архитектура BUSCA</strong><br><br>В основе метода — Decision Transformer, работающий полностью в онлайн-режиме (без изменения прошлых результатов или доступа к будущим кадрам).<br><br>После стандартного сопоставления детекций с треками, несматченные треки обрабатываются BUSCA, которая анализирует:<br><br>— <strong>Candidate proposal B</strong>: прогнозируемую позицию объекта с помощью фильтра Калмана;<br><br>— <strong>Contextual proposals C</strong>: Q=4 ближайших объекта из успешно отслеживаемых треков на текущем кадре (для учёта окружения);<br><br>— <strong>Learned tokens L</strong>: специальные токены [Halluc.] для обнаружения искажённых треков и [Miss.], когда объект покинул сцену или все предложения неподходящие.<br><br>Обработка токенов различается:<br><br>- Трековые наблюдения (несколько последних наблюдений из несматченного трека), Candidate и Contextual proposals проходят через ResNet-50 для извлечения 512-мерных визуальных признаков;<br>- Learned tokens и разделительный токен [SEP] инициализируются случайно и обучаются совместно с остальной архитектурой.<br><br>Все токены получают специальное пространственно-временное кодирование (STE), отражающее относительные время, размеры и расстояние до последнего наблюдения трека.<br><br>Затем Decision Transformer обрабатывает все токены вместе. Полученные представления предложений проходят через MLP-слой для генерации их вероятностей. Если выбирается candidate B — трек продолжается с обновлёнными координатами, в остальных случаях — ставится на паузу.<br><br><strong>Эксперименты</strong><br><br>BUSCA особенно эффективна для объектов с низкой видимостью и увеличивает среднюю длину треков. Метод обучался только на синтетических данных (MOTSynth) и работает со скоростью около 45 мс на кадр на NVIDIA RTX GPU.<br><br>BUSCA протестирована на пяти трекерах (ByteTrack, StrongSORT, GHOST, TransCenter, CenterTrack) и улучшила все ключевые метрики на MOT16, MOT17 и MOT20.<br><br><em>Разбор подготовила </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Мария Поклонская</em> <br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/94_480.webp" srcset="../assets/media/thumbs/94_480.webp 480w, ../assets/media/94.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="94" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 942 просмотров · 19 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/94" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/94.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="89" data-search="личный опыт инженеров яндекса — дмитрий баранчук продолжаем рассказывать об ml-инженерах и исследователях в яндексе. сегодня о своих задачах, успехах и профессиональных ожиданиях рассказал руководитель команды исследователей по генеративному cv в yandex research. работы, о которых дима говорит в карточках: — switti: designing scale-wise transformers for text-to-image synthesis; — invertible consistency distillation for text-guided image editing in around 7 steps. больше карточек — по хештэгу #yamlpeople. cv time личный опыт инженеров яндекса — дмитрий баранчук продолжаем рассказывать об ml-инженерах и исследователях в яндексе. сегодня о своих задачах, успехах и профессиональных ожиданиях рассказал руководитель команды исследователей по генеративному cv в yandex research. работы, о которых дима говорит в карточках: — switti: designing scale-wise transformers for text-to-image synthesis ; — invertible consistency distillation for text-guided image editing in around 7 steps . больше карточек — по хештэгу #yamlpeople. cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-17T07:35:50+00:00" href="./posts/89.html">2025-04-17 07:35 UTC</a></div>
      </div>
      <div class="post-body"><strong>Личный опыт инженеров Яндекса — Дмитрий Баранчук<br></strong><br>Продолжаем рассказывать об ML-инженерах и исследователях в Яндексе. Сегодня о своих задачах, успехах и профессиональных ожиданиях рассказал руководитель команды исследователей по генеративному CV в Yandex Research.<br><br>Работы, о которых Дима говорит в карточках:<br><br>— <a href="https://arxiv.org/pdf/2412.01819" rel="nofollow noopener noreferrer">Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis</a>;<br>—<a href="https://arxiv.org/abs/2406.14539" rel="nofollow noopener noreferrer"> Invertible consistency distillation for text-guided image editing in around 7 steps</a>.<br><br>Больше карточек — по хештэгу #YaMLpeople.<br><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/89_480.webp" srcset="../assets/media/thumbs/89_480.webp 480w, ../assets/media/89.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="89" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/90_480.webp" srcset="../assets/media/thumbs/90_480.webp 480w, ../assets/media/90.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="89" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/91_480.webp" srcset="../assets/media/thumbs/91_480.webp 480w, ../assets/media/91.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="89" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/92_480.webp" srcset="../assets/media/thumbs/92_480.webp 480w, ../assets/media/92.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="89" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/93_480.webp" srcset="../assets/media/thumbs/93_480.webp 480w, ../assets/media/93.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="89" data-image-index="4" /></div></div>
      <div class="actions">
        <span>1 876 просмотров · 29 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/89" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/89.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="88" data-search="partedit: fine-grained image editing using pre-trained diffusion models сегодня разбираем статью о редактировании изображений. авторы показывают, как с помощью предобученной диффузионной модели sdxl можно детально изменять заданные сегменты на изображении. предложенный метод требует обучения токенов частей картинки на датасете, где для каждой картинки и целевого токена есть маска, соответствующая области, которую нужно изменить. в промпт подаются два токена, например: &lt;head&gt; (токен головы) и &lt;bg&gt; (бэкграунд для головы). причём токен бэкграунда (&lt;bg&gt;) уникален для каждого целевого токена. дальше токены обучаются так, чтобы маски аттеншна в sdxl давали маски сегментации нужной части картинки. при этом сама диффузионная модель остается замороженной, обучаются только токены. авторы отдельно изучают, в каком диапазоне (от 0 до 50, где 0 — почти чистое изображение, а 50 — максимально зашумлённое) лучше брать усреднение. при обучении на таймстемпах [50, 40] маски получаются некачественные. в диапазоне [30, 20] результат лучше. при [10, 0] — чуть хуже, чем на среднем. когда эмбеддинги выучены, происходит редактирование. в случае с синтетическими изображениями используются три ветки: 1. генерация исходного синтетического изображения. например, по промпту &quot;a closeup of a man&quot; в качестве промпта подаётся, например, &quot;a closeup of a man&quot;, где мы хотим заменить голову человека на голову робота. sdxl генерирует изображение и при этом сохраняется траектория — промежуточные латенты для каждого таймстемпа. 2. средняя часть архитектуры работает как сегментатор: в sdxl подаются только два выученных эмбеддинга: части, которую нужно отредактировать и бэкграунда. в нашем примере это будет &lt;head&gt; и &lt;bg&gt;. для каждого таймстемпа собираются все маски аттеншна со всех слоёв (предварительно приводятся к одному размеру и агрегируются). затем применяется алгоритм otsu, который вычисляет локальный порог: если значение больше порога — это единица, если меньше — ноль, а в промежутке от 1/2k до 3/2k маска просто сохраняется. 3. генерация отредактированного изображения: через sdxl прогоняются выученные эмбеддинги того, что нужно заменить (например, чтобы получить &quot;a closeup of a man with robotic &lt;head&gt;&quot;). на этапе редактирования фичемапы с исходного и редактирующего прогона блендятся с помощью предобработанных масок аттеншна из второй ветки. в конце прогона получается отредактированное изображение: нужная часть изменена, остальное — как в оригинале. решение применимо не только к синтетическим, но и к реальным изображениям: с помощью методов инверсии ledit++ и ef-ddpm получают латенты, а с помощью blip2 — промпт-описание исходной картинки. в сравнениях с другими подходами в качестве метрики используется alpha-clip: по маске определяется область редактирования, а с помощью clip считается, насколько результат соответствует заданному промпту. как водится — по всем метрикам результаты превосходят конкурентов. разбор подготовил ❣ александр шишеня cv time partedit: fine-grained image editing using pre-trained diffusion models сегодня разбираем статью о редактировании изображений. авторы показывают, как с помощью предобученной диффузионной модели sdxl можно детально изменять заданные сегменты на изображении. предложенный метод требует обучения токенов частей картинки на датасете, где для каждой картинки и целевого токена есть маска, соответствующая области, которую нужно изменить. в промпт подаются два токена, например: &amp;lt;head&amp;gt; (токен головы) и &amp;lt;bg&amp;gt; (бэкграунд для головы). причём токен бэкграунда (&amp;lt;bg&amp;gt;) уникален для каждого целевого токена. дальше токены обучаются так, чтобы маски аттеншна в sdxl давали маски сегментации нужной части картинки. при этом сама диффузионная модель остается замороженной, обучаются только токены. авторы отдельно изучают, в каком диапазоне (от 0 до 50, где 0 — почти чистое изображение, а 50 — максимально зашумлённое) лучше брать усреднение. при обучении на таймстемпах [50, 40] маски получаются некачественные. в диапазоне [30, 20] результат лучше. при [10, 0] — чуть хуже, чем на среднем. когда эмбеддинги выучены, происходит редактирование. в случае с синтетическими изображениями используются три ветки: 1. генерация исходного синтетического изображения. например, по промпту &amp;quot;a closeup of a man&amp;quot; в качестве промпта подаётся, например, &amp;quot;a closeup of a man&amp;quot;, где мы хотим заменить голову человека на голову робота. sdxl генерирует изображение и при этом сохраняется траектория — промежуточные латенты для каждого таймстемпа. 2. средняя часть архитектуры работает как сегментатор: в sdxl подаются только два выученных эмбеддинга: части, которую нужно отредактировать и бэкграунда. в нашем примере это будет &amp;lt;head&amp;gt; и &amp;lt;bg&amp;gt;. для каждого таймстемпа собираются все маски аттеншна со всех слоёв (предварительно приводятся к одному размеру и агрегируются). затем применяется алгоритм otsu, который вычисляет локальный порог: если значение больше порога — это единица, если меньше — ноль, а в промежутке от 1/2k до 3/2k маска просто сохраняется. 3. генерация отредактированного изображения: через sdxl прогоняются выученные эмбеддинги того, что нужно заменить (например, чтобы получить &amp;quot;a closeup of a man with robotic &amp;lt;head&amp;gt;&amp;quot;). на этапе редактирования фичемапы с исходного и редактирующего прогона блендятся с помощью предобработанных масок аттеншна из второй ветки. в конце прогона получается отредактированное изображение: нужная часть изменена, остальное — как в оригинале. решение применимо не только к синтетическим, но и к реальным изображениям: с помощью методов инверсии ledit++ и ef-ddpm получают латенты, а с помощью blip2 — промпт-описание исходной картинки. в сравнениях с другими подходами в качестве метрики используется alpha-clip: по маске определяется область редактирования, а с помощью clip считается, насколько результат соответствует заданному промпту. как водится — по всем метрикам результаты превосходят конкурентов. разбор подготовил ❣ александр шишеня cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-10T08:02:01+00:00" href="./posts/88.html">2025-04-10 08:02 UTC</a></div>
      </div>
      <div class="post-body"><strong>PartEdit: Fine-Grained Image Editing using Pre-Trained Diffusion Models</strong><br><br>Сегодня разбираем <a href="https://arxiv.org/abs/2502.04050" rel="nofollow noopener noreferrer">статью</a> о редактировании изображений. Авторы показывают, как с помощью предобученной диффузионной модели SDXL можно детально изменять заданные сегменты на изображении.<br><br>Предложенный метод требует обучения токенов частей картинки на датасете, где для каждой картинки и целевого токена есть маска, соответствующая области, которую нужно изменить. В промпт подаются два токена, например: &lt;head&gt; (токен головы) и &lt;BG&gt; (бэкграунд для головы). Причём токен бэкграунда (&lt;BG&gt;) уникален для каждого целевого токена.<br><br>Дальше токены обучаются так, чтобы маски аттеншна в SDXL давали маски сегментации нужной части картинки. При этом сама диффузионная модель остается замороженной, обучаются только токены.<br><br>Авторы отдельно изучают, в каком диапазоне (от 0 до 50, где 0 — почти чистое изображение, а 50 — максимально зашумлённое) лучше брать усреднение. При обучении на таймстемпах [50, 40] маски получаются некачественные. В диапазоне [30, 20] результат лучше. При [10, 0] — чуть хуже, чем на среднем. <br><br>Когда эмбеддинги выучены, происходит редактирование. В случае с синтетическими изображениями используются три ветки: <br><br><strong>1. Генерация исходного синтетического изображения.</strong> Например, по промпту &quot;A closeup of a man&quot; в качестве промпта подаётся, например, &quot;A closeup of a man&quot;, где мы хотим заменить голову человека на голову робота. SDXL генерирует изображение и при этом сохраняется траектория — промежуточные латенты для каждого таймстемпа.<br><br><strong>2. Средняя часть архитектуры работает как сегментатор: </strong>в SDXL подаются только два выученных эмбеддинга: части, которую нужно отредактировать и бэкграунда. В нашем примере это будет &lt;head&gt; и &lt;BG&gt;.<br><br>Для каждого таймстемпа собираются все маски аттеншна со всех слоёв (предварительно приводятся к одному размеру и агрегируются). Затем применяется алгоритм OTSU, который вычисляет локальный порог: если значение больше порога — это единица, если меньше — ноль, а в промежутке от 1/2K до 3/2K маска просто сохраняется. <br><br><strong>3. Генерация отредактированного изображения:</strong> через SDXL прогоняются выученные эмбеддинги того, что нужно заменить (например, чтобы получить &quot;A closeup of a man with robotic &lt;head&gt;&quot;).<br><br>На этапе редактирования фичемапы с исходного и редактирующего прогона блендятся с помощью предобработанных масок аттеншна из второй ветки. В конце прогона получается отредактированное изображение: нужная часть изменена, остальное — как в оригинале.<br><br>Решение применимо не только к синтетическим, но и к реальным изображениям: с помощью методов инверсии LEDIT++ и EF-DDPM получают латенты, а с помощью BLIP2 — промпт-описание исходной картинки.<br><br>В сравнениях с другими подходами в качестве метрики используется Alpha-CLIP: по маске определяется область редактирования, а с помощью CLIP считается, насколько результат соответствует заданному промпту. Как водится — по всем метрикам результаты превосходят конкурентов.<br><br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Александр Шишеня</em><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/88_480.webp" srcset="../assets/media/thumbs/88_480.webp 480w, ../assets/media/88.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="88" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 009 просмотров · 15 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/88" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/88.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="87" data-search="тематическая подборка статей: генерация подобрали свежие статьи о генеративных моделях. в этот раз — обсуждают, как улучшить токенизацию для диффузионных моделей, дистиллировать cfg и оптимизировать обучение генератора. а ещё есть работа о том, как интерпретировать внимание в diffusion transformers и использовать его для сегментации. автоэнкодеры reconstruction vs. generation: taming optimization dilemma in latent diffusion models авторы говорят, что увеличение числа каналов в автоэнкодере улучшает реконструкцию (что логично), но делает задачу для генератора более сложной, приводя к ухудшению генераций. предлагают дополнительным лоссом предсказывать признаки от бэкбона (dino/mae/etc) — это делает фичи автоэнкодера более «простыми» для генератора и улучшает его сходимость. masked autoencoders are effective tokenizers for diffusion models развитие предыдущей работы: связали улучшение качества представления автоэнкодера с уменьшением числа мод в mixture of gaussian модели, и переделали архитектуру автоэнкодера в mae-трансформер. эдитинг realedit: reddit edits as a large-scale empirical dataset for image transformations в статье предлагают парсить reddit для сбора датасета по эдитингу картинок: брать треды, где пользователи просят отфотошопить их картинки. отбирают посты до 2021 года, чтобы в них не было применения ai. ускорение dice: distilling classifier-free guidance into text embeddings авторы говорят, что можно дистиллировать classifier-free guidance (cfg), включая negative prompt, в небольшую нейронку поверх текстовых эмбеддов. visual generation without guidance в статье предлагают алгоритм обучения генератора, для которого потом не нужно делать cfg. заявляют, что это работает лучше, чем дистилляция. rl calibrated multi-preference optimization for aligning diffusion models исследователи из google предлагают метод, который, по их утверждению, лучше, чем direct preference optimization (dpo), благодаря аккуратному выбору пар для обучения и более хитрой функции потерь. diffusion model as a noise-aware latent reward model for step-level preference optimization предлагают делать rl непосредственно в латентном пространстве — для этого нужна reward-модель, способная в нём работать. говорят, что идеально подходит предобученная диффузионная модель, которую можно дообучить на предсказание reward’а. утверждают, что это упрощает пайплайн обучения и улучшает финальное качество. другое conceptattention: diffusion transformers learn highly interpretable features авторы говорят, что можно использовать предобученную диффузионную модель для получения sota сегментационных масок в zero-shot-режиме. для этого делают надстройку над аттеншн-слоями в dit&#x27;е. подборку подготовил ❣ артём конев cv time тематическая подборка статей: генерация подобрали свежие статьи о генеративных моделях. в этот раз — обсуждают, как улучшить токенизацию для диффузионных моделей, дистиллировать cfg и оптимизировать обучение генератора. а ещё есть работа о том, как интерпретировать внимание в diffusion transformers и использовать его для сегментации. автоэнкодеры reconstruction vs. generation: taming optimization dilemma in latent diffusion models авторы говорят, что увеличение числа каналов в автоэнкодере улучшает реконструкцию (что логично), но делает задачу для генератора более сложной, приводя к ухудшению генераций. предлагают дополнительным лоссом предсказывать признаки от бэкбона (dino/mae/etc) — это делает фичи автоэнкодера более «простыми» для генератора и улучшает его сходимость. masked autoencoders are effective tokenizers for diffusion models развитие предыдущей работы: связали улучшение качества представления автоэнкодера с уменьшением числа мод в mixture of gaussian модели, и переделали архитектуру автоэнкодера в mae-трансформер. эдитинг realedit: reddit edits as a large-scale empirical dataset for image transformations в статье предлагают парсить reddit для сбора датасета по эдитингу картинок: брать треды, где пользователи просят отфотошопить их картинки. отбирают посты до 2021 года, чтобы в них не было применения ai. ускорение dice: distilling classifier-free guidance into text embeddings авторы говорят, что можно дистиллировать classifier-free guidance (cfg), включая negative prompt, в небольшую нейронку поверх текстовых эмбеддов. visual generation without guidance в статье предлагают алгоритм обучения генератора, для которого потом не нужно делать cfg. заявляют, что это работает лучше, чем дистилляция. rl calibrated multi-preference optimization for aligning diffusion models исследователи из google предлагают метод, который, по их утверждению, лучше, чем direct preference optimization (dpo), благодаря аккуратному выбору пар для обучения и более хитрой функции потерь. diffusion model as a noise-aware latent reward model for step-level preference optimization предлагают делать rl непосредственно в латентном пространстве — для этого нужна reward-модель, способная в нём работать. говорят, что идеально подходит предобученная диффузионная модель, которую можно дообучить на предсказание reward’а. утверждают, что это упрощает пайплайн обучения и улучшает финальное качество. другое conceptattention: diffusion transformers learn highly interpretable features авторы говорят, что можно использовать предобученную диффузионную модель для получения sota сегментационных масок в zero-shot-режиме. для этого делают надстройку над аттеншн-слоями в dit&amp;#x27;е. подборку подготовил ❣ артём конев cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-02T07:38:09+00:00" href="./posts/87.html">2025-04-02 07:38 UTC</a></div>
      </div>
      <div class="post-body"><strong>Тематическая подборка статей: генерация</strong><br><br>Подобрали свежие статьи о генеративных моделях. В этот раз — обсуждают, как улучшить токенизацию для диффузионных моделей, дистиллировать CFG и оптимизировать обучение генератора. А ещё есть работа о том, как интерпретировать внимание в Diffusion Transformers и использовать его для сегментации.<br><br><strong>Автоэнкодеры</strong><br> <br><a href="https://arxiv.org/pdf/2501.01423v1" rel="nofollow noopener noreferrer"><strong>Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models</strong></a><br>Авторы говорят, что увеличение числа каналов в автоэнкодере улучшает реконструкцию (что логично), но делает задачу для генератора более сложной, приводя к ухудшению генераций. Предлагают дополнительным лоссом предсказывать признаки от бэкбона (dino/mae/etc) — это делает фичи автоэнкодера более «простыми» для генератора и улучшает его сходимость.<br><br><a href="https://arxiv.org/abs/2502.03444" rel="nofollow noopener noreferrer"><strong>Masked Autoencoders Are Effective Tokenizers for Diffusion Models</strong></a><br>Развитие предыдущей работы: связали улучшение качества представления автоэнкодера с уменьшением числа мод в mixture of gaussian модели, и переделали архитектуру автоэнкодера в MAE-трансформер.<br> <br><strong>Эдитинг</strong><br> <br><a href="https://arxiv.org/abs/2502.03629" rel="nofollow noopener noreferrer"><strong>REALEDIT: Reddit Edits As a Large-scale Empirical Dataset for Image Transformations</strong></a><br>В статье предлагают парсить Reddit для сбора датасета по эдитингу картинок: брать треды, где пользователи просят отфотошопить их картинки. Отбирают посты до 2021 года, чтобы в них не было применения AI.<br> <br><strong>Ускорение</strong><br> <br><a href="https://arxiv.org/abs/2502.03726" rel="nofollow noopener noreferrer"><strong>DICE: Distilling Classifier-Free Guidance into Text Embeddings</strong></a><br>Авторы говорят, что можно дистиллировать Classifier-Free Guidance (CFG), включая negative prompt, в небольшую нейронку поверх текстовых эмбеддов.<br> <br><a href="https://arxiv.org/abs/2501.15420" rel="nofollow noopener noreferrer"><strong>Visual Generation Without Guidance</strong></a><br>В статье предлагают алгоритм обучения генератора, для которого потом не нужно делать CFG. Заявляют, что это работает лучше, чем дистилляция.<br> <br><strong>RL</strong><br> <br><a href="https://arxiv.org/abs/2502.02588" rel="nofollow noopener noreferrer"><strong>Calibrated Multi-Preference Optimization for Aligning Diffusion Models</strong></a><br>Исследователи из Google предлагают метод, который, по их утверждению, лучше, чем Direct Preference Optimization (DPO), благодаря аккуратному выбору пар для обучения и более хитрой функции потерь.<br><br><a href="https://arxiv.org/abs/2502.01051" rel="nofollow noopener noreferrer"><strong>Diffusion Model as a Noise-Aware Latent Reward Model for Step-Level Preference Optimization</strong></a><br>Предлагают делать RL непосредственно в латентном пространстве — для этого нужна reward-модель, способная в нём работать. Говорят, что идеально подходит предобученная диффузионная модель, которую можно дообучить на предсказание reward’а. Утверждают, что это упрощает пайплайн обучения и улучшает финальное качество.<br><br><strong>Другое</strong><br><br><a href="https://arxiv.org/html/2502.04320v1" rel="nofollow noopener noreferrer"><strong>ConceptAttention: Diffusion Transformers Learn Highly Interpretable Features</strong></a><br>Авторы говорят, что можно использовать предобученную диффузионную модель для получения SOTA сегментационных масок в zero-shot-режиме. Для этого делают надстройку над аттеншн-слоями в DiT&#x27;е.<br><br><em>Подборку подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Артём Конев<br></em><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a></div>
      <div class="actions">
        <span>5 186 просмотров · 22 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/87" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/87.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    </div>
    
    <div class="pager static-pager" style="justify-content:center">
      <div class="page-links">
        <a class="nav-link" href="index.html">←</a>
        <a class="page-link" href="index.html">1</a> <a class="page-link current" href="page-2.html">2</a> <a class="page-link" href="page-3.html">3</a> <a class="page-link" href="page-4.html">4</a>
        <a class="nav-link" href="page-3.html">→</a>
      </div>
    </div>
    
  </main>

  <footer class="footer">
    <div class="container">
      <div class="footer-inner">
        <span>based on <a href="https://github.com/ml-brand/tg-to-gh-pages" target="_blank" rel="noopener">tg-to-gh-pages</a> (created by <a href="https://github.com/ml-brand" target="_blank" rel="noopener">ML Brand</a>)</span>
        <a id="repoLink" href="https://github.com/ml-brand/tg-to-gh-pages" target="_blank" rel="noopener">Do the same with your channel.</a>
        <span class="footer-links">
          static copy ·
          <a href="../feed.xml" target="_blank" rel="noopener">RSS</a> ·
          <a href="../atom.xml" target="_blank" rel="noopener">Atom</a>
        </span>
      </div>
    </div>
  </footer>

  <script>
    window.__STATIC_POSTS = [{"id": 175, "media": [{"kind": "photo", "path": "../assets/media/175.jpg", "thumb": "../assets/media/thumbs/175_480.webp", "size": 68443, "mime": "image/jpeg", "name": null}]}, {"id": 173, "media": [{"kind": "photo", "path": "../assets/media/173.jpg", "thumb": "../assets/media/thumbs/173_480.webp", "size": 86931, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/174.jpg", "thumb": "../assets/media/thumbs/174_480.webp", "size": 106920, "mime": "image/jpeg", "name": null}]}, {"id": 172, "media": []}, {"id": 170, "media": [{"kind": "photo", "path": "../assets/media/170.jpg", "thumb": "../assets/media/thumbs/170_480.webp", "size": 140705, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/171.jpg", "thumb": "../assets/media/thumbs/171_480.webp", "size": 138561, "mime": "image/jpeg", "name": null}]}, {"id": 169, "media": [{"kind": "photo", "path": "../assets/media/169.jpg", "thumb": "../assets/media/thumbs/169_480.webp", "size": 54475, "mime": "image/jpeg", "name": null}]}, {"id": 168, "media": [{"kind": "photo", "path": "../assets/media/168.jpg", "thumb": "../assets/media/thumbs/168_480.webp", "size": 93902, "mime": "image/jpeg", "name": null}]}, {"id": 167, "media": []}, {"id": 157, "media": [{"kind": "photo", "path": "../assets/media/157.jpg", "thumb": "../assets/media/thumbs/157_480.webp", "size": 124193, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/158.jpg", "thumb": "../assets/media/thumbs/158_480.webp", "size": 169603, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/159.jpg", "thumb": "../assets/media/thumbs/159_480.webp", "size": 131935, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/160.jpg", "thumb": "../assets/media/thumbs/160_480.webp", "size": 160511, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/161.jpg", "thumb": "../assets/media/thumbs/161_480.webp", "size": 146900, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/162.jpg", "thumb": "../assets/media/thumbs/162_480.webp", "size": 109526, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/163.jpg", "thumb": "../assets/media/thumbs/163_480.webp", "size": 159603, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/164.jpg", "thumb": "../assets/media/thumbs/164_480.webp", "size": 167166, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/165.jpg", "thumb": "../assets/media/thumbs/165_480.webp", "size": 167167, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/166.jpg", "thumb": "../assets/media/thumbs/166_480.webp", "size": 164881, "mime": "image/jpeg", "name": null}]}, {"id": 150, "media": [{"kind": "photo", "path": "../assets/media/150.jpg", "thumb": "../assets/media/thumbs/150_480.webp", "size": 67018, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/151.jpg", "thumb": "../assets/media/thumbs/151_480.webp", "size": 130761, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/152.jpg", "thumb": "../assets/media/thumbs/152_480.webp", "size": 179150, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/153.jpg", "thumb": "../assets/media/thumbs/153_480.webp", "size": 215157, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/154.jpg", "thumb": "../assets/media/thumbs/154_480.webp", "size": 172019, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/155.jpg", "thumb": "../assets/media/thumbs/155_480.webp", "size": 177377, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/156.jpg", "thumb": "../assets/media/thumbs/156_480.webp", "size": 168092, "mime": "image/jpeg", "name": null}]}, {"id": 149, "media": []}, {"id": 148, "media": [{"kind": "photo", "path": "../assets/media/148.jpg", "thumb": "../assets/media/thumbs/148_480.webp", "size": 51465, "mime": "image/jpeg", "name": null}]}, {"id": 147, "media": [{"kind": "photo", "path": "../assets/media/147.jpg", "thumb": "../assets/media/thumbs/147_480.webp", "size": 77806, "mime": "image/jpeg", "name": null}]}, {"id": 146, "media": [{"kind": "photo", "path": "../assets/media/146.jpg", "thumb": "../assets/media/thumbs/146_480.webp", "size": 61349, "mime": "image/jpeg", "name": null}]}, {"id": 144, "media": [{"kind": "photo", "path": "../assets/media/144.jpg", "thumb": "../assets/media/thumbs/144_480.webp", "size": 77468, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/145.jpg", "thumb": "../assets/media/thumbs/145_480.webp", "size": 79024, "mime": "image/jpeg", "name": null}]}, {"id": 143, "media": [{"kind": "photo", "path": "../assets/media/143.jpg", "thumb": "../assets/media/thumbs/143_480.webp", "size": 49252, "mime": "image/jpeg", "name": null}]}, {"id": 140, "media": [{"kind": "photo", "path": "../assets/media/140.jpg", "thumb": "../assets/media/thumbs/140_480.webp", "size": 148331, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/141.jpg", "thumb": "../assets/media/thumbs/141_480.webp", "size": 38903, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/142.jpg", "thumb": "../assets/media/thumbs/142_480.webp", "size": 112076, "mime": "image/jpeg", "name": null}]}, {"id": 139, "media": [{"kind": "photo", "path": "../assets/media/139.jpg", "thumb": "../assets/media/thumbs/139_480.webp", "size": 82932, "mime": "image/jpeg", "name": null}]}, {"id": 137, "media": [{"kind": "photo", "path": "../assets/media/137.jpg", "thumb": "../assets/media/thumbs/137_480.webp", "size": 63839, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/138.jpg", "thumb": "../assets/media/thumbs/138_480.webp", "size": 56094, "mime": "image/jpeg", "name": null}]}, {"id": 129, "media": [{"kind": "photo", "path": "../assets/media/129.jpg", "thumb": "../assets/media/thumbs/129_480.webp", "size": 203207, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/130.jpg", "thumb": "../assets/media/thumbs/130_480.webp", "size": 144454, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/131.jpg", "thumb": "../assets/media/thumbs/131_480.webp", "size": 169518, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/132.jpg", "thumb": "../assets/media/thumbs/132_480.webp", "size": 192973, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/133.jpg", "thumb": "../assets/media/thumbs/133_480.webp", "size": 223009, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/134.jpg", "thumb": "../assets/media/thumbs/134_480.webp", "size": 129527, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/135.jpg", "thumb": "../assets/media/thumbs/135_480.webp", "size": 170684, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/136.jpg", "thumb": "../assets/media/thumbs/136_480.webp", "size": 155238, "mime": "image/jpeg", "name": null}]}, {"id": 128, "media": [{"kind": "video", "path": "../assets/media/128_IMG_6543__1_.mp4", "thumb": null, "size": 2365086, "mime": "video/mp4", "name": "IMG_6543__1_.mp4"}]}, {"id": 123, "media": [{"kind": "photo", "path": "../assets/media/123.jpg", "thumb": "../assets/media/thumbs/123_480.webp", "size": 151285, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/124.jpg", "thumb": "../assets/media/thumbs/124_480.webp", "size": 140953, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/125.jpg", "thumb": "../assets/media/thumbs/125_480.webp", "size": 262511, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/126.jpg", "thumb": "../assets/media/thumbs/126_480.webp", "size": 186835, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/127.jpg", "thumb": "../assets/media/thumbs/127_480.webp", "size": 169949, "mime": "image/jpeg", "name": null}]}, {"id": 116, "media": [{"kind": "photo", "path": "../assets/media/116.jpg", "thumb": "../assets/media/thumbs/116_480.webp", "size": 211122, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/117.jpg", "thumb": "../assets/media/thumbs/117_480.webp", "size": 168205, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/118.jpg", "thumb": "../assets/media/thumbs/118_480.webp", "size": 186760, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/119.jpg", "thumb": "../assets/media/thumbs/119_480.webp", "size": 189048, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/120.jpg", "thumb": "../assets/media/thumbs/120_480.webp", "size": 190553, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/121.jpg", "thumb": "../assets/media/thumbs/121_480.webp", "size": 253389, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/122.jpg", "thumb": "../assets/media/thumbs/122_480.webp", "size": 277351, "mime": "image/jpeg", "name": null}]}, {"id": 111, "media": [{"kind": "photo", "path": "../assets/media/111.jpg", "thumb": "../assets/media/thumbs/111_480.webp", "size": 169117, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/112.jpg", "thumb": "../assets/media/thumbs/112_480.webp", "size": 101344, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/113.jpg", "thumb": "../assets/media/thumbs/113_480.webp", "size": 182320, "mime": "image/jpeg", "name": null}, {"kind": "video", "path": "../assets/media/114_______.mp4", "thumb": null, "size": 2281156, "mime": "video/mp4", "name": "______.mp4"}]}, {"id": 106, "media": [{"kind": "photo", "path": "../assets/media/106.jpg", "thumb": "../assets/media/thumbs/106_480.webp", "size": 109153, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/107.jpg", "thumb": "../assets/media/thumbs/107_480.webp", "size": 108629, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/108.jpg", "thumb": "../assets/media/thumbs/108_480.webp", "size": 97428, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/109.jpg", "thumb": "../assets/media/thumbs/109_480.webp", "size": 86860, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/110.jpg", "thumb": "../assets/media/thumbs/110_480.webp", "size": 92920, "mime": "image/jpeg", "name": null}]}, {"id": 98, "media": [{"kind": "photo", "path": "../assets/media/98.jpg", "thumb": "../assets/media/thumbs/98_480.webp", "size": 259291, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/99.jpg", "thumb": "../assets/media/thumbs/99_480.webp", "size": 230205, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/100.jpg", "thumb": "../assets/media/thumbs/100_480.webp", "size": 216109, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/101.jpg", "thumb": "../assets/media/thumbs/101_480.webp", "size": 230251, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/102.jpg", "thumb": "../assets/media/thumbs/102_480.webp", "size": 165366, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/103.jpg", "thumb": "../assets/media/thumbs/103_480.webp", "size": 201589, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/104.jpg", "thumb": "../assets/media/thumbs/104_480.webp", "size": 229857, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/105.jpg", "thumb": "../assets/media/thumbs/105_480.webp", "size": 204624, "mime": "image/jpeg", "name": null}]}, {"id": 95, "media": [{"kind": "video", "path": "../assets/media/95_2025-04-23_14.57.04.mp4", "thumb": null, "size": 639953, "mime": "video/mp4", "name": "2025-04-23_14.57.04.mp4"}, {"kind": "photo", "path": "../assets/media/96.jpg", "thumb": "../assets/media/thumbs/96_480.webp", "size": 200603, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/97.jpg", "thumb": "../assets/media/thumbs/97_480.webp", "size": 211336, "mime": "image/jpeg", "name": null}]}, {"id": 94, "media": [{"kind": "photo", "path": "../assets/media/94.jpg", "thumb": "../assets/media/thumbs/94_480.webp", "size": 95006, "mime": "image/jpeg", "name": null}]}, {"id": 89, "media": [{"kind": "photo", "path": "../assets/media/89.jpg", "thumb": "../assets/media/thumbs/89_480.webp", "size": 87212, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/90.jpg", "thumb": "../assets/media/thumbs/90_480.webp", "size": 86392, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/91.jpg", "thumb": "../assets/media/thumbs/91_480.webp", "size": 171425, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/92.jpg", "thumb": "../assets/media/thumbs/92_480.webp", "size": 106341, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/93.jpg", "thumb": "../assets/media/thumbs/93_480.webp", "size": 91064, "mime": "image/jpeg", "name": null}]}, {"id": 88, "media": [{"kind": "photo", "path": "../assets/media/88.jpg", "thumb": "../assets/media/thumbs/88_480.webp", "size": 77077, "mime": "image/jpeg", "name": null}]}, {"id": 87, "media": []}];
    window.__STATIC_META = {"title": "CV Time", "username": "timeforcv", "channel": "timeforcv", "last_sync_utc": "2026-02-10T16:45:42Z", "posts_count": 107, "last_seen_message_id": 239, "stats": {"new": 122, "updated": 20, "media_downloaded": 122}, "avatar": "assets/channel_avatar.jpg", "meta_schema_version": "1.0.0", "posts_schema_version": "1.0.0"};
  </script>
  <script src="../common.js"></script>
  <script src="../static.js"></script>
</body>
</html>
