<!doctype html>
<html lang="ru">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>CV Time — статическая версия (стр. 4/4)</title>
  <meta name="description" content="Статическая версия зеркала Telegram-канала" />
  <link rel="icon" href="../favicon.ico?v=2026-02-10T07%3A45%3A40Z" sizes="any" />
  <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32.png?v=2026-02-10T07%3A45%3A40Z" />
  <link rel="apple-touch-icon" href="../apple-touch-icon.png?v=2026-02-10T07%3A45%3A40Z" />

  <link rel="stylesheet" href="../style.css" />
  <script src="../metrika.js"></script>
</head>
<body>
  <header class="header">
    <div class="container">
      <div class="title-grid">
        <a class="grid-avatar" href="#" target="_blank" rel="noopener">
          <img id="channelAvatar" class="channel-avatar" src="../assets/channel_avatar.jpg" alt="Аватар канала"  />
        </a>
        <div class="grid-main">
          <div class="title-head">
            <div class="title-left">
              <a class="badge-chip" id="siteTitleWrap" href="#" target="_blank" rel="noopener"><h1 id="siteTitle">CV Time</h1></a>
            </div>
            <div class="hero-actions">
              <a id="subscribeBtn" class="subscribe-btn" href="https://t.me/+JoULEedmHyE5MmYy" target="_blank" rel="noopener" >Подписаться</a>
              <a class="icon-btn" href="../" aria-label="Перейти к динамической версии">↺</a>
              <button id="themeToggle" class="icon-btn" type="button" aria-label="Переключить тему"></button>
            </div>
          </div>
        </div>
        <div class="controls"></div>
      </div>
    </div>
  </header>

  
  <div id="promoBanner" class="promo-banner" hidden>
    <div class="container promo-inner">
      <span class="promo-text"><a href="https://t.me/addlist/5NH3RoVejEI1MGEy">Подпишись на все наши ML каналы. Они классные, отвечаем!</a></span>
      <button id="promoClose" class="promo-close" type="button" aria-label="Скрыть плашку">×</button>
    </div>
  </div>
  

  <main class="container">
    
    <div class="pager static-pager" style="justify-content:center">
      <div class="page-links">
        <a class="nav-link" href="page-3.html">←</a>
        <a class="page-link" href="index.html">1</a> <a class="page-link" href="page-2.html">2</a> <a class="page-link" href="page-3.html">3</a> <a class="page-link current" href="page-4.html">4</a>
        <a class="nav-link disabled" href="#">→</a>
      </div>
    </div>
    
    <div id="posts" class="posts">
      
    <article class="post" data-post-id="20" data-search="making llama see and draw with seed tokenizer llama — семейство больших языковых моделей от meta ai, которые до недавнего времени понимали только текстовые запросы и не умели обрабатывать изображения. популярный подход к тому, чтобы загрузить в такую модель картинку — закодировать изображение в вектор специальным энкодером, привести адаптером в нужную размерность и подать результат на вход нейросети так же, как и текст. а llama вернёт ответ на естественном языке. например, посчитает для вас яблоки на загруженном фото. так можно решить задачу распознавания, но объединить еë с чем-то другим, например, с генерацией изображений уже не получится. авторы сегодняшней статьи попробовали это исправить: добавить к обычной llama токенайзер seed, чтобы она могла не только распознавать, что изображено на картинке, но и генерировать что-то новое на основе входных данных. суть похода — на схемах. сначала авторы в несколько стадий обучают картиночные токены на vq-кодбуках — так токен сразу получает причинное свойство и его остаëтся только векторизировать. интересно, что сначала обучается именно seed tokenizer, а после того, как он будет готов, языковую vlm-модель отдельно обучают предсказывать следующие токены, в том числе новые картиночные. а дальше детокенайз — через кодбук дискретные коды превращают обратно в вещественные вектора и подают в диффузионную нейросеть. комбинация llama + seed напоминает по архитектуре emu. но если вы внимательно прочитаете статью, о которой я рассказываю, то легко заметите отличительную особенность: множество красивых картинок с результатами, но очень плохие vlm-метрики. мне удалось пообщаться с авторами, и они честно ответили, что сейчас метрики — не их сильная сторона. но они продолжают улучшать токенайзер и уже подали на конференцию новую версию — seed-x. им удалось избавиться от кодбуков: обучают диффузионную нейросеть теперь на визуальных эмбеддингах. разбор подготовил ❣ андрей чернов cv time making llama see and draw with seed tokenizer llama — семейство больших языковых моделей от meta ai, которые до недавнего времени понимали только текстовые запросы и не умели обрабатывать изображения. популярный подход к тому, чтобы загрузить в такую модель картинку — закодировать изображение в вектор специальным энкодером, привести адаптером в нужную размерность и подать результат на вход нейросети так же, как и текст. а llama вернёт ответ на естественном языке. например, посчитает для вас яблоки на загруженном фото. так можно решить задачу распознавания, но объединить еë с чем-то другим, например, с генерацией изображений уже не получится. авторы сегодняшней статьи попробовали это исправить: добавить к обычной llama токенайзер seed, чтобы она могла не только распознавать, что изображено на картинке, но и генерировать что-то новое на основе входных данных. суть похода — на схемах. сначала авторы в несколько стадий обучают картиночные токены на vq-кодбуках — так токен сразу получает причинное свойство и его остаëтся только векторизировать. интересно, что сначала обучается именно seed tokenizer, а после того, как он будет готов, языковую vlm-модель отдельно обучают предсказывать следующие токены, в том числе новые картиночные. а дальше детокенайз — через кодбук дискретные коды превращают обратно в вещественные вектора и подают в диффузионную нейросеть. комбинация llama + seed напоминает по архитектуре emu . но если вы внимательно прочитаете статью, о которой я рассказываю, то легко заметите отличительную особенность: множество красивых картинок с результатами, но очень плохие vlm-метрики. мне удалось пообщаться с авторами, и они честно ответили, что сейчас метрики — не их сильная сторона. но они продолжают улучшать токенайзер и уже подали на конференцию новую версию — seed-x. им удалось избавиться от кодбуков: обучают диффузионную нейросеть теперь на визуальных эмбеддингах. разбор подготовил ❣ андрей чернов cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-10-08T13:08:14+00:00" href="./posts/20.html">2024-10-08 13:08 UTC</a></div>
      </div>
      <div class="post-body"><strong>Making LLaMA SEE and Draw with SEED Tokenizer</strong><br><br>LLaMA — семейство больших языковых моделей от Meta AI, которые до недавнего времени понимали только текстовые запросы и не умели обрабатывать изображения.<br><br>Популярный подход к тому, чтобы загрузить в такую модель картинку — закодировать изображение в вектор специальным энкодером, привести адаптером в нужную размерность и подать результат на вход нейросети так же, как и текст. А LLaMA вернёт ответ на естественном языке. Например, посчитает для вас яблоки на загруженном фото. <br><br>Так можно решить задачу распознавания, но объединить еë с чем-то другим, например, с генерацией изображений уже не получится. <br><br>Авторы <a href="https://arxiv.org/abs/2310.01218" rel="nofollow noopener noreferrer">сегодняшней статьи</a> попробовали это исправить: добавить к обычной LLaMA токенайзер SEED, чтобы она могла не только распознавать, что изображено на картинке, но и генерировать что-то новое на основе входных данных. <br><br>Суть похода — на схемах. Сначала авторы в несколько стадий обучают картиночные токены на VQ-кодбуках — так токен сразу получает причинное свойство и его остаëтся только векторизировать. Интересно, что сначала обучается именно SEED tokenizer, а после того, как он будет готов, языковую VLM-модель отдельно обучают предсказывать следующие токены, в том числе новые картиночные.<br><br>А дальше детокенайз — через кодбук дискретные коды превращают обратно в вещественные вектора и подают в диффузионную нейросеть. <br><br>Комбинация LLaMA + SEED напоминает по архитектуре <a href="https://arxiv.org/pdf/2307.05222" rel="nofollow noopener noreferrer">EMU</a>. Но если вы внимательно прочитаете статью, о которой я рассказываю, то легко заметите отличительную особенность: множество красивых картинок с результатами, но очень плохие VLM-метрики. Мне удалось пообщаться с авторами, и они честно ответили, что сейчас метрики — не их сильная сторона. Но они продолжают улучшать токенайзер и уже подали на конференцию новую версию — SEED-X. Им удалось избавиться от кодбуков: обучают диффузионную нейросеть теперь на визуальных эмбеддингах. <br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Андрей Чернов</em><br><a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/20_480.webp" srcset="../assets/media/thumbs/20_480.webp 480w, ../assets/media/20.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="20" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/21_480.webp" srcset="../assets/media/thumbs/21_480.webp 480w, ../assets/media/21.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="20" data-image-index="1" /></div></div>
      <div class="actions">
        <span>1 823 просмотров · 40 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/20" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/20.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="19" data-search="eccv 2024 — всё! это было круто, познавательно и насыщенно. спасибо, вам, что читали, и спасибо нашим экспертам, которые рассказывали о самых интересных статьях, докладах и воркшопах. их вы можете найти на канале по хэштегу #yaeccv. но это ещё не всё — совсем скоро мы вернёмся с впечатлениями от конференции из первых уст. ну и, конечно, с новыми разборами статей о cv. оставайтесь с нами! eccv 2024 — всё! это было круто, познавательно и насыщенно. спасибо, вам, что читали, и спасибо нашим экспертам, которые рассказывали о самых интересных статьях, докладах и воркшопах. их вы можете найти на канале по хэштегу #yaeccv. но это ещё не всё — совсем скоро мы вернёмся с впечатлениями от конференции из первых уст. ну и, конечно, с новыми разборами статей о cv. оставайтесь с нами!">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-10-04T16:02:10+00:00" href="./posts/19.html">2024-10-04 16:02 UTC</a></div>
      </div>
      <div class="post-body"><strong>ECCV 2024 — всё!</strong><br><br>Это было круто, познавательно и насыщенно. Спасибо, вам, что читали, и спасибо нашим экспертам, которые рассказывали о самых интересных статьях, докладах и воркшопах. Их вы можете найти на канале по хэштегу #YaECCV.<br><br>Но это ещё не всё — совсем скоро мы вернёмся с впечатлениями от конференции из первых уст. Ну и, конечно, с новыми разборами статей о CV. Оставайтесь с нами!</div>
      <div class="actions">
        <span>1 580 просмотров · 17 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/19" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/19.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="18" data-search="">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-10-04T16:02:06+00:00" href="./posts/18.html">2024-10-04 16:02 UTC</a></div>
      </div>
      <div class="post-body"><p class="muted">[без текста]</p><div class="media"><video controls preload="metadata" src="../assets/media/18.mp4"></video></div></div>
      <div class="actions">
        <span>1 486 просмотров · 8 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/18" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/18.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="17" data-search="новая порция материалов с eccv 2024 в последний день конференции принесли ещё несколько статей, которые точно заслуживают внимания. turboedit: text-based image editing using few-step diffusion models статья о редактировании реальных изображении при помощи text2image дифузионных моделей. в основе работы лежат два наблюдения: 1. при равных сидах редактирование длинных текстовых промптов заметно меньше влияет на изменение общей композиции генерации, в отличие от манипуляций с короткими промптами. это объясняется меньшей магнитудой изменения в cross-attention-слоях. 2. одношаговые генеративные модели вроде sdxl turbo не сталкиваются с трудностями в оптимизационной задаче инверсии, а также позволяет проводить манипуляции с attention-картами для редактирования изображения. совмещение этих идей даёт оптимизационный процесс, который учит инвертирующую модель. с её помощью получается начальный шум, для которого запускается процедура расшумления исходной моделью с редактированным промптом, чтобы получить редактированную генерацию. для улучшения реконструкции предлагается два подхода. вместо одношаговой модели обучать многошаговую refiner-модель в стиле restyle. либо можно маскировать attention-карты для локализации изменений. edict: exact diffusion inversion via coupled transformations авторы предлагают новый семплер для редактирования картинок на основе текстовой инверсии. суть в том, что для для интегрирования используют результаты предыдущего и следующего шага. при этом не добавляют вычислительного оверхеда, потому что результаты и так получаются естественным образом. в сравнении с ddim-инверсией такой подход даёт почти идеальное восстановление. be yourself: bounded attention for multi-subject text-to-image generation работа о multi-subject grounded генерации. поднимается всем известная проблема «запутанности» семантически похожих концептов, происходящей в аttention-блоках. авторы предлагают использовать пространственную информацию карт внимания не только для маскирования «соседних» конкурирующих токенов, но и для guidance во время инференса модели. а кроме того — смещать диффузионную траекторию по направлению, максимизирующему концентрацию attention в заданном bounding box для соответствующего объекта в промпте. reground: improving textual and spatial grounding at no cost статья, в основе которой архитектурный анализ сети. в качестве базовой авторы рассматривают очень популярную в своё время модель gligen — она позволяет добавлять дополнительное условие на пространственное расположение объектов на генерации посредством bounding box. исследователи обратили внимание на последовательный характер внедрённого в сеть блока gated self-attention, который отвечает за grounding-токены. подобный архитектурный выбор нарушает ожидаемое распределение входа в cross-attention-модуль и тем самым нарушает текстовую составляющую условной генерации. простая перестановка с последовательного соединения на параллельное решает проблему и позволяет найти компромисс для соответствия обоим условиям. это также улучшает и все существующие работы, использующие gligen в качестве составляющей метода. recon: training-free acceleration for text-to-image synthesis with retrieval of concept prompt trajectories в статье рассматривается метод ускорения генераций с прицелом на продакшн и способом, основанном на кешировании некоторых x_t генераций отдельных концептов. суть в том, чтобы брать комплексные длинные промпты, разбивать их на концепты, отфильтровывать не визуальные, а потом делать частичную генерацию до шага t и помещать результат в базу. для генерации картинки по полному промпту, нужные частичные генерации складывают, а остаток траектории — генерируют отдельно. авторы завяляют, что ускорение составляет в среднем 30% без сильной потери в качестве. поделились любопытным ❣ александр устюжанин и сергей кастрюлин #yaeccv cv time новая порция материалов с eccv 2024 в последний день конференции принесли ещё несколько статей, которые точно заслуживают внимания. turboedit: text-based image editing using few-step diffusion models статья о редактировании реальных изображении при помощи text2image дифузионных моделей. в основе работы лежат два наблюдения: 1. при равных сидах редактирование длинных текстовых промптов заметно меньше влияет на изменение общей композиции генерации, в отличие от манипуляций с короткими промптами. это объясняется меньшей магнитудой изменения в cross-attention-слоях. 2. одношаговые генеративные модели вроде sdxl turbo не сталкиваются с трудностями в оптимизационной задаче инверсии, а также позволяет проводить манипуляции с attention-картами для редактирования изображения. совмещение этих идей даёт оптимизационный процесс, который учит инвертирующую модель. с её помощью получается начальный шум, для которого запускается процедура расшумления исходной моделью с редактированным промптом, чтобы получить редактированную генерацию. для улучшения реконструкции предлагается два подхода. вместо одношаговой модели обучать многошаговую refiner-модель в стиле restyle. либо можно маскировать attention-карты для локализации изменений. edict: exact diffusion inversion via coupled transformations авторы предлагают новый семплер для редактирования картинок на основе текстовой инверсии. суть в том, что для для интегрирования используют результаты предыдущего и следующего шага. при этом не добавляют вычислительного оверхеда, потому что результаты и так получаются естественным образом. в сравнении с ddim-инверсией такой подход даёт почти идеальное восстановление. be yourself: bounded attention for multi-subject text-to-image generation работа о multi-subject grounded генерации. поднимается всем известная проблема «запутанности» семантически похожих концептов, происходящей в аttention-блоках. авторы предлагают использовать пространственную информацию карт внимания не только для маскирования «соседних» конкурирующих токенов, но и для guidance во время инференса модели. а кроме того — смещать диффузионную траекторию по направлению, максимизирующему концентрацию attention в заданном bounding box для соответствующего объекта в промпте. reground: improving textual and spatial grounding at no cost статья, в основе которой архитектурный анализ сети. в качестве базовой авторы рассматривают очень популярную в своё время модель gligen — она позволяет добавлять дополнительное условие на пространственное расположение объектов на генерации посредством bounding box. исследователи обратили внимание на последовательный характер внедрённого в сеть блока gated self-attention, который отвечает за grounding-токены. подобный архитектурный выбор нарушает ожидаемое распределение входа в cross-attention-модуль и тем самым нарушает текстовую составляющую условной генерации. простая перестановка с последовательного соединения на параллельное решает проблему и позволяет найти компромисс для соответствия обоим условиям. это также улучшает и все существующие работы, использующие gligen в качестве составляющей метода. recon: training-free acceleration for text-to-image synthesis with retrieval of concept prompt trajectories в статье рассматривается метод ускорения генераций с прицелом на продакшн и способом, основанном на кешировании некоторых x_t генераций отдельных концептов. суть в том, чтобы брать комплексные длинные промпты, разбивать их на концепты, отфильтровывать не визуальные, а потом делать частичную генерацию до шага t и помещать результат в базу. для генерации картинки по полному промпту, нужные частичные генерации складывают, а остаток траектории — генерируют отдельно. авторы завяляют, что ускорение составляет в среднем 30% без сильной потери в качестве. поделились любопытным ❣ александр устюжанин и сергей кастрюлин #yaeccv cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-10-04T11:12:44+00:00" href="./posts/17.html">2024-10-04 11:12 UTC</a></div>
      </div>
      <div class="post-body"><strong>Новая порция материалов с ECCV</strong> <strong>2024</strong><br><br>В последний день конференции принесли ещё несколько статей, которые точно заслуживают внимания. <br><br><a href="https://arxiv.org/abs/2408.00735" rel="nofollow noopener noreferrer"><strong>TurboEdit: Text-Based Image Editing Using Few-Step Diffusion Models</strong></a><br><br>Статья о редактировании реальных изображении при помощи text2image дифузионных моделей. В основе работы лежат два наблюдения: <br><br>1. При равных сидах редактирование длинных текстовых промптов заметно меньше влияет на изменение общей композиции генерации, в отличие от манипуляций с короткими промптами. Это объясняется меньшей магнитудой изменения в cross-attention-слоях.<br><br>2. Одношаговые генеративные модели вроде SDXL Turbo не сталкиваются с трудностями в оптимизационной задаче инверсии, а также позволяет проводить манипуляции с attention-картами для редактирования изображения. <br><br>Совмещение этих идей даёт оптимизационный процесс, который учит инвертирующую модель. С её помощью получается начальный шум, для которого запускается процедура расшумления исходной моделью с редактированным промптом, чтобы получить редактированную генерацию.<br><br>Для улучшения реконструкции предлагается два подхода. Вместо одношаговой модели обучать многошаговую refiner-модель в стиле ReStyle. Либо можно маскировать attention-карты для локализации изменений.<br><br><a href="https://arxiv.org/abs/2211.12446" rel="nofollow noopener noreferrer"><strong>EDICT: Exact Diffusion Inversion via Coupled Transformations</strong></a><br><br>Авторы предлагают новый семплер для редактирования картинок на основе текстовой инверсии. Суть в том, что для для интегрирования используют результаты предыдущего и следующего шага. При этом не добавляют вычислительного оверхеда, потому что результаты и так получаются естественным образом. В сравнении с DDIM-инверсией такой подход даёт почти идеальное восстановление. <br><br><a href="https://arxiv.org/abs/2403.16990" rel="nofollow noopener noreferrer"><strong>Be Yourself: Bounded Attention for Multi-Subject Text-to-Image Generation</strong></a><br><br>Работа о multi-subject grounded генерации. Поднимается всем известная проблема «запутанности» семантически похожих концептов, происходящей в аttention-блоках. Авторы предлагают использовать пространственную информацию карт внимания не только для маскирования «соседних» конкурирующих токенов, но и для guidance во время инференса модели. А кроме того — смещать диффузионную траекторию по направлению, максимизирующему концентрацию attention в заданном bounding box для соответствующего объекта в промпте.<br><br><a href="https://arxiv.org/abs/2403.13589" rel="nofollow noopener noreferrer"><strong>ReGround: Improving Textual and Spatial Grounding at No Cost</strong></a><br><br>Статья, в основе которой архитектурный анализ сети. В качестве базовой авторы рассматривают очень популярную в своё время модель GLIGEN — она позволяет добавлять дополнительное условие на пространственное расположение объектов на генерации посредством bounding box. <br><br>Исследователи обратили внимание на последовательный характер внедрённого в сеть блока gated self-attention, который отвечает за grounding-токены. Подобный архитектурный выбор нарушает ожидаемое распределение входа в cross-attention-модуль и тем самым нарушает текстовую составляющую условной генерации. <br><br>Простая перестановка с последовательного соединения на параллельное решает проблему и позволяет найти компромисс для соответствия обоим условиям. Это также улучшает и все существующие работы, использующие GLIGEN в качестве составляющей метода.<br><br><a href="https://research.adobe.com/publication/recon-training-free-acceleration-for-text-to-image-synthesis-with-retrieval-of-concept-prompt-trajectories/" rel="nofollow noopener noreferrer"><strong>ReCON: Training-Free Acceleration for Text-to-Image Synthesis with Retrieval of Concept Prompt Trajectories</strong></a><br><br>В статье рассматривается метод ускорения генераций с прицелом на продакшн и способом, основанном на кешировании некоторых x_t генераций отдельных концептов. Суть в том, чтобы брать комплексные длинные промпты, разбивать их на концепты, отфильтровывать не визуальные, а потом делать частичную генерацию до шага t и помещать результат в базу. <br><br>Для генерации картинки по полному промпту, нужные частичные генерации складывают, а остаток траектории — генерируют отдельно. Авторы завяляют, что ускорение составляет в среднем 30% без сильной потери в качестве.<br><br><em>Поделились любопытным </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Александр Устюжанин и Сергей Кастрюлин</em><br><br>#YaECCV<br><br><a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer">CV Time</a></div>
      <div class="actions">
        <span>1 590 просмотров · 12 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/17" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/17.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="16" data-search="⚡️прямое включение с eccv 2024 наш специальный корреспондент дарья виноградова заметила роботов, похожих на собак, и постеры со статьями. наступил последний день конференции, но нам ещё есть, что рассказать! ⚡️прямое включение с eccv 2024 наш специальный корреспондент дарья виноградова заметила роботов, похожих на собак, и постеры со статьями. наступил последний день конференции, но нам ещё есть, что рассказать!">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-10-04T08:47:12+00:00" href="./posts/16.html">2024-10-04 08:47 UTC</a></div>
      </div>
      <div class="post-body"><strong>⚡️Прямое включение с ECCV 2024</strong><br><br>Наш специальный корреспондент Дарья Виноградова заметила роботов, похожих на собак, и постеры со статьями. Наступил последний день конференции, но нам ещё есть, что рассказать!</div>
      <div class="actions">
        <span>1 321 просмотров · 19 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/16" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/16.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="15" data-search="">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-10-04T08:46:18+00:00" href="./posts/15.html">2024-10-04 08:46 UTC</a></div>
      </div>
      <div class="post-body"><p class="muted">[без текста]</p><div class="media"><video controls preload="metadata" src="../assets/media/15.mp4"></video></div></div>
      <div class="actions">
        <span>1 473 просмотров · 26 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/15" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/15.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="13" data-search="blueberry оказалась flux 1.1. pro помните загадочную blueberry, которая лидировала на text2image-арене? на eccv выяснилось, что это была flux 1.1 pro от black forest labs — об этом сообщил сооснователь компании алекс зауэр. более того, по его словам, сегодня нас ждёт ещё один анонс, связанный с этой моделью. cv time #yaeccv blueberry оказалась flux 1.1. pro помните загадочную blueberry, которая лидировала на text2image-арене ? на eccv выяснилось, что это была flux 1.1 pro от black forest labs — об этом сообщил сооснователь компании алекс зауэр. более того, по его словам, сегодня нас ждёт ещё один анонс, связанный с этой моделью. cv time #yaeccv">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-10-03T13:47:34+00:00" href="./posts/13.html">2024-10-03 13:47 UTC</a></div>
      </div>
      <div class="post-body"><strong>Blueberry оказалась Flux 1.1. Pro</strong><br><br>Помните загадочную Blueberry, которая лидировала на <a href="https://huggingface.co/spaces/ArtificialAnalysis/Text-to-Image-Leaderboard" rel="nofollow noopener noreferrer">text2image-арене</a>? На ECCV выяснилось, что это была Flux 1.1 Pro от Black Forest Labs — об этом сообщил сооснователь компании Алекс Зауэр. Более того, по его словам, сегодня нас ждёт ещё один анонс, связанный с этой моделью.<br><br><a href="https://t.me/+OuN-SY3nIjdlYjli" rel="nofollow noopener noreferrer">CV Time</a><br><br>#YaECCV</div>
      <div class="actions">
        <span>9 413 просмотров · 11 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/13" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/13.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="12" data-search="доклады с eccv 2024? их есть у нас! продолжаем рассказывать о самых интересных статьях, докладах и воркшопах с неё. вот, что принесли сегодня. stable video 3d stability ai предлагает модель image-to-3d. они используют image-to-video-модель stable video и файнтюнят её на задачу генерации видео с вращением виртуальной камеры вокруг заданного на изображении объекта. подобно вчерашнему controlnet light подходу, здесь добавляют в сам unet сферические параметры камеры в качестве кондишена, а также clip embedding входной картинки. далее модель обучают на регулярной сетке азимутов и постоянном значении элевации, а только на следующем этапе переходят на непрерывную параметризацию с произвольными значениями. праеры из stable video позволяют получать консистентные novel views. чтобы приблизиться к получению 3d-мэша, авторы предлагают двухэтапный пайплайн: сначала обучить нерф на задачу реконструкции (без sds) поверх выходов зафайнтьюненной sv под орбитальную съемку на регулярных позах камеры. затем, уже на втором этапе, используется masked sds на непрерывных позах. причём маскирование происходит по не наблюдаемым с регулярных ракурсов частях мэша. это важно, чтобы не произошла деградация (блюр) наблюдаемых частей. авторы также говорят о проблемах baked-in lighting. чтобы их решить, простую illumination-модель обучают на задизентанглинг diffuse color и освещения. авторы сравнивают multiview-генерации с zero 1-to-3 и разносят их в одну калитку. long-clip: unlocking the long-text capability of clip сначала авторы определяют, что эффективная длина последовательности в клипе составляет порядка 20 токенов. этого очень мало для некоторых приложений — например, для ретривала или определения схожести картинки с длинными текстами. а ещё клипы часто используются в качестве текстовых энкодеров для text-conditional генеративных моделей, где такая длина последовательности тоже не достаточна. авторы статьи пробуют дообучить модель на более длинных последовательностях, но главный минус такого подхода — сложности с выделением важного. модель начинает воспринимать все атрибуты как равно значимые и реагирует на мельчайшие изменения в каждом из них. чтобы решить эту проблему, авторы предлагают двухэтапный тюнинг: 1. тюнинг на длинных кепшенах (fine-grained tuning); 2. извлечение главных компонентов изображений и текстов с помощью pca и алайнмент их между собой обычным контрастив-лоссом (coarse-grained tuning). в результате модель выдает лучшие показатели и в оценке соответствия длинных текстов изображению, и в качестве текстового энкодера для text2image-диффузии. videoagent: a memory-augmented multimodal agent for video understanding авторы предлагают использовать мультимодальный агент для анализа длинных видео. они наделяют его памятью нескольких типов. во-первых, это текстовые описания каждого 2-секундного клипа (здесь используют модель lavila). во-вторых, — описания на уровне эмбеддингов: самого клипа (тут берут viclip) и полученного текстового кэпшна (text-embedding-3-large от openai). и память о конкретных затреканных объектах: их эмбеддинги для реидентификации (из clip) и моменты появления в видео (отслеживаются bytetrack) складываются в sql-базу используя такую память, агент может: — описывать 2-секундные фрагменты видео; — искать клип по текстовому запросу с описание происходящего — используются текстовые и видео-фичи клипов, чтобы определить сходство с текстовым запросом; — отвечать на вопрос по видео — выделяется наиболее релевантный фрагмент и запускается video-llava; — рассказывать о качествах конкретных объектов — например, их количестве. здесь происходит поиск по фичам в трекинговой базе и отправка соответствующего sql-запроса. агент сам выбирает наиболее подходящее действие с помощью дополнительной llm. система выглядит тяжёлой, учитывая то, сколько моделей для неё нужно. однако она позволяет побить на известных qa-видео-датасетах крутые модели вроде videollava, lavila и internvideo. разборы подготовили ❣ александр устюжанин, сергей кастрюлин и дарья виноградова cv time #yaeccv доклады с eccv 2024? их есть у нас! продолжаем рассказывать о самых интересных статьях, докладах и воркшопах с неё. вот, что принесли сегодня. stable video 3d stability ai предлагает модель image-to-3d. они используют image-to-video-модель stable video и файнтюнят её на задачу генерации видео с вращением виртуальной камеры вокруг заданного на изображении объекта. подобно вчерашнему controlnet light подходу, здесь добавляют в сам unet сферические параметры камеры в качестве кондишена, а также clip embedding входной картинки. далее модель обучают на регулярной сетке азимутов и постоянном значении элевации, а только на следующем этапе переходят на непрерывную параметризацию с произвольными значениями. праеры из stable video позволяют получать консистентные novel views. чтобы приблизиться к получению 3d-мэша, авторы предлагают двухэтапный пайплайн: сначала обучить нерф на задачу реконструкции (без sds) поверх выходов зафайнтьюненной sv под орбитальную съемку на регулярных позах камеры. затем, уже на втором этапе, используется masked sds на непрерывных позах. причём маскирование происходит по не наблюдаемым с регулярных ракурсов частях мэша. это важно, чтобы не произошла деградация (блюр) наблюдаемых частей. авторы также говорят о проблемах baked-in lighting. чтобы их решить, простую illumination-модель обучают на задизентанглинг diffuse color и освещения. авторы сравнивают multiview-генерации с zero 1-to-3 и разносят их в одну калитку. long-clip: unlocking the long-text capability of clip сначала авторы определяют, что эффективная длина последовательности в клипе составляет порядка 20 токенов. этого очень мало для некоторых приложений — например, для ретривала или определения схожести картинки с длинными текстами. а ещё клипы часто используются в качестве текстовых энкодеров для text-conditional генеративных моделей, где такая длина последовательности тоже не достаточна. авторы статьи пробуют дообучить модель на более длинных последовательностях, но главный минус такого подхода — сложности с выделением важного. модель начинает воспринимать все атрибуты как равно значимые и реагирует на мельчайшие изменения в каждом из них. чтобы решить эту проблему, авторы предлагают двухэтапный тюнинг: 1. тюнинг на длинных кепшенах (fine-grained tuning); 2. извлечение главных компонентов изображений и текстов с помощью pca и алайнмент их между собой обычным контрастив-лоссом (coarse-grained tuning). в результате модель выдает лучшие показатели и в оценке соответствия длинных текстов изображению, и в качестве текстового энкодера для text2image-диффузии. videoagent: a memory-augmented multimodal agent for video understanding авторы предлагают использовать мультимодальный агент для анализа длинных видео. они наделяют его памятью нескольких типов. во-первых, это текстовые описания каждого 2-секундного клипа (здесь используют модель lavila). во-вторых, — описания на уровне эмбеддингов: самого клипа (тут берут viclip) и полученного текстового кэпшна (text-embedding-3-large от openai). и память о конкретных затреканных объектах: их эмбеддинги для реидентификации (из clip) и моменты появления в видео (отслеживаются bytetrack) складываются в sql-базу используя такую память, агент может: — описывать 2-секундные фрагменты видео; — искать клип по текстовому запросу с описание происходящего — используются текстовые и видео-фичи клипов, чтобы определить сходство с текстовым запросом; — отвечать на вопрос по видео — выделяется наиболее релевантный фрагмент и запускается video-llava; — рассказывать о качествах конкретных объектов — например, их количестве. здесь происходит поиск по фичам в трекинговой базе и отправка соответствующего sql-запроса. агент сам выбирает наиболее подходящее действие с помощью дополнительной llm. система выглядит тяжёлой, учитывая то, сколько моделей для неё нужно. однако она позволяет побить на известных qa-видео-датасетах крутые модели вроде videollava, lavila и internvideo. разборы подготовили ❣ александр устюжанин, сергей кастрюлин и дарья виноградова cv time #yaeccv">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-10-03T11:07:02+00:00" href="./posts/12.html">2024-10-03 11:07 UTC</a></div>
      </div>
      <div class="post-body"><strong>Доклады с ECCV 2024? Их есть у нас! </strong><br><br>Продолжаем рассказывать о самых интересных статьях, докладах и воркшопах с неё. Вот, что принесли сегодня. <br><br><strong>Stable Video 3D</strong><br><br>Stability AI предлагает модель image-to-3d. Они используют image-to-video-модель Stable Video и файнтюнят её на задачу генерации видео с вращением виртуальной камеры вокруг заданного на изображении объекта. Подобно вчерашнему ControlNet Light подходу, здесь добавляют в сам unet сферические параметры камеры в качестве кондишена, а также clip embedding входной картинки. Далее модель обучают на регулярной сетке азимутов и постоянном значении элевации, а только на следующем этапе переходят на непрерывную параметризацию с произвольными значениями. Праеры из Stable Video позволяют получать консистентные novel views. <br><br>Чтобы приблизиться к получению 3D-мэша, авторы предлагают двухэтапный пайплайн: сначала обучить нерф на задачу реконструкции (без SDS) поверх выходов зафайнтьюненной SV под орбитальную съемку на регулярных позах камеры. Затем, уже на втором этапе, используется Masked SDS на непрерывных позах. Причём маскирование происходит по не наблюдаемым с регулярных ракурсов частях мэша. Это важно, чтобы не произошла деградация (блюр) наблюдаемых частей.<br><br>Авторы также говорят о проблемах baked-in lighting. Чтобы их решить, простую illumination-модель обучают на задизентанглинг diffuse color и освещения. Авторы сравнивают multiview-генерации с Zero 1-to-3 и разносят их в одну калитку.<br><br><a href="https://arxiv.org/abs/2403.15378" rel="nofollow noopener noreferrer"><strong>Long-CLIP: Unlocking the Long-Text Capability of CLIP</strong></a><br><br>Сначала авторы определяют, что эффективная длина последовательности в клипе составляет порядка 20 токенов. Этого очень мало для некоторых приложений — например, для ретривала или определения схожести картинки с длинными текстами. А ещё клипы часто используются в качестве текстовых энкодеров для text-conditional генеративных моделей, где такая длина последовательности тоже не достаточна. <br><br>Авторы статьи пробуют дообучить модель на более длинных последовательностях, но главный минус такого подхода — сложности с выделением важного. Модель начинает воспринимать все атрибуты как равно значимые и реагирует на мельчайшие изменения в каждом из них. Чтобы решить эту проблему, авторы предлагают двухэтапный тюнинг:<br><br>1. тюнинг на длинных кепшенах (fine-grained tuning);<br>2. извлечение главных компонентов изображений и текстов с помощью PCA и алайнмент их между собой обычным контрастив-лоссом (coarse-grained tuning). <br><br>В результате модель выдает лучшие показатели и в оценке соответствия длинных текстов изображению, и в качестве текстового энкодера для text2image-диффузии.<br><br><a href="https://arxiv.org/abs/2403.15378" rel="nofollow noopener noreferrer"><strong>VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding</strong></a><br><br>Авторы  предлагают использовать мультимодальный агент для анализа длинных видео. Они наделяют его памятью нескольких типов.<br> <br>Во-первых, это текстовые описания каждого 2-секундного клипа (здесь используют модель LaViLa). Во-вторых, — описания на уровне эмбеддингов: самого клипа (тут берут ViCLIP) и полученного текстового кэпшна (text-embedding-3-large от OpenAI). И память о конкретных затреканных объектах: их эмбеддинги для реидентификации (из CLIP) и моменты появления в видео (отслеживаются ByteTrack) складываются в SQL-базу<br><br>Используя такую память, агент может:<br><br>— описывать 2-секундные фрагменты видео;<br>— искать клип по текстовому запросу с описание происходящего — используются текстовые и видео-фичи клипов, чтобы определить сходство с текстовым запросом; <br>—  отвечать на вопрос по видео — выделяется наиболее релевантный фрагмент и запускается Video-LLaVA;<br>—  рассказывать о качествах конкретных объектов — например, их количестве. Здесь происходит поиск по фичам в трекинговой базе и отправка соответствующего SQL-запроса.<br><br>Агент сам выбирает наиболее подходящее действие с помощью дополнительной LLM. Система выглядит тяжёлой, учитывая то, сколько моделей для неё нужно. Однако она позволяет побить на известных QA-видео-датасетах крутые модели вроде VideoLLaVA, LaViLa и InternVideo.<br><br><em>Разборы подготовили </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Александр Устюжанин, Сергей Кастрюлин и Дарья Виноградова</em><br><br><a href="https://t.me/+OuN-SY3nIjdlYjli" rel="nofollow noopener noreferrer">CV Time</a><br><br>#YaECCV</div>
      <div class="actions">
        <span>1 367 просмотров · 13 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/12" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/12.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="9" data-search="привет! возвращаемся на поля eccv в поисках интересностей. #yaeccv привет! возвращаемся на поля eccv в поисках интересностей. #yaeccv">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-10-03T08:56:37+00:00" href="./posts/9.html">2024-10-03 08:56 UTC</a></div>
      </div>
      <div class="post-body">Привет! Возвращаемся на поля ECCV в поисках интересностей. <br><br>#YaECCV<div class="media"><video controls preload="metadata" src="../assets/media/9_IMG_1264.MP4.mp4"></video></div></div>
      <div class="actions">
        <span>1 179 просмотров · 21 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/9" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/9.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="8" data-search="просто обаятельный робот, который узнаёт наших экспертов, вам в ленту. чтобы скрасить вечер среды. видео сняла ❣ дарья виноградова. #yaeccv просто обаятельный робот, который узнаёт наших экспертов, вам в ленту. чтобы скрасить вечер среды. видео сняла ❣ дарья виноградова. #yaeccv">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-10-02T14:49:53+00:00" href="./posts/8.html">2024-10-02 14:49 UTC</a></div>
      </div>
      <div class="post-body">Просто обаятельный робот, который узнаёт наших экспертов, вам в ленту. Чтобы скрасить вечер среды.<br><br><em>Видео сняла </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Дарья Виноградова.</em><br><br>#YaECCV<div class="media"><video controls preload="metadata" src="../assets/media/8______.mp4"></video></div></div>
      <div class="actions">
        <span>1 125 просмотров · 14 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/8" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/8.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="7" data-search="ещё интересное с eccv 2024 продолжаем освещать конференцию и подсвечивать самые занятные воркшопы и доклады. рассказ из воркшопа knowledge in generative models авторы задаются вопросом: как в сети кодируются знания о каком-либо визуальном образе? классический способ — сегментировать объект на изображении, а потом посмотреть активации нейронов, которые ведут к пикселям внутри маски. однако этот способ обнаруживает далеко не всю информацию, которой обладает модель. предлагается взять множество «картиночных» моделей: генеративную stylegan2, дискриминативные dino vit, resnet и так далее, — а затем посмотреть на одних картинках схожесть в послойных активациях. для этого генерируем картинку через stylegan2, а затем прогоняем через дискриминативные модели.все найденные пары — и есть искомые знания в модели. далее эти rosetta-нейроны можно использовать в инверсии и редактировании. но это для ганов. для диффузии эта штука не работает, потому что активации нейронов меняются со временем. поэтому в качестве постоянной компоненты предлагают использовать веса модели. следует файнтюнить модель на разные концепты, потом рассматривать полученные веса как точку в пространстве весов. здесь можно найти интересную линейную делимость по некоторым признакам, а также получается непрерывно сэмплить (близкие точки семантически похожи) картинки. investigating style similarity in diffusion models авторы стремятся понять, умеет ли модель воспроизводить стили художников из реально сделанных ими картин. классические ssl-методы вроде clip кодируют семантическую информацию и, соответственно, для такого анализа не подходят. поэтому авторы сначала обучают модельку для выделения стилистических эмбедов и даже выкладывают её. дальше они берут laion-aesthetics, выделяют оттуда сабсет на 12 миллионов пар с эстетическим скором более шести и парсят так, чтобы выделить информацию о стилях (на основе кепшенов). например, если в кепшене есть что-то вроде “in a style of van goth”, то они кладут семпл в класс винсент ван гог. разметка получается шумная, тем не менее полученный сабсет называют laion styles. следом авторы берут стили из этого датасета и смотрят, насколько стилевые эмбеды картинок в датасете похожи на стилевые эмбеды генераций. пробегая по большому количеству классов используют это как оценку умения моделей генерировать изображения и подражать разным стилям. flashtex: fast relightable mesh texturing with light controlnet работа посвящена генерации text-to-texture для трёх моделей. они фиксируют регулярные параметры освещения и позы камеры, а также три типа материалов. затем рендерят мэш входного объекта при вышеописанных условиях для каждого материала, подают в controlnet в качестве кондишена и учат текстурировать мэш в 2d. затем авторы представляют материал в виде multi-resolution hash grid. и проводят оптимизацию материала с классическими лоссами: реконструкция по выходам controlnet light для регулярных параметров света и камеры и sds — как у dreamfusion — для непрерывных параметров. для консистентности multiview генерации авторы подают в controlnet коллаж сразу с несколькими параметрами камеры. параметризация controlnet по свету позволяет отделить материал модели от освещения для генерации более реалистичной текстуры. рассказали об интересном и крутом ❣ александр устюжанин и сергей кастрюлин. cv time #yaeccv ещё интересное с eccv 2024 продолжаем освещать конференцию и подсвечивать самые занятные воркшопы и доклады. рассказ из воркшопа knowledge in generative models авторы задаются вопросом: как в сети кодируются знания о каком-либо визуальном образе? классический способ — сегментировать объект на изображении, а потом посмотреть активации нейронов, которые ведут к пикселям внутри маски. однако этот способ обнаруживает далеко не всю информацию, которой обладает модель. предлагается взять множество «картиночных» моделей: генеративную stylegan2, дискриминативные dino vit, resnet и так далее, — а затем посмотреть на одних картинках схожесть в послойных активациях. для этого генерируем картинку через stylegan2, а затем прогоняем через дискриминативные модели.все найденные пары — и есть искомые знания в модели. далее эти rosetta-нейроны можно использовать в инверсии и редактировании. но это для ганов. для диффузии эта штука не работает, потому что активации нейронов меняются со временем. поэтому в качестве постоянной компоненты предлагают использовать веса модели. следует файнтюнить модель на разные концепты, потом рассматривать полученные веса как точку в пространстве весов. здесь можно найти интересную линейную делимость по некоторым признакам, а также получается непрерывно сэмплить (близкие точки семантически похожи) картинки. investigating style similarity in diffusion models авторы стремятся понять, умеет ли модель воспроизводить стили художников из реально сделанных ими картин. классические ssl-методы вроде clip кодируют семантическую информацию и, соответственно, для такого анализа не подходят. поэтому авторы сначала обучают модельку для выделения стилистических эмбедов и даже выкладывают её. дальше они берут laion-aesthetics, выделяют оттуда сабсет на 12 миллионов пар с эстетическим скором более шести и парсят так, чтобы выделить информацию о стилях (на основе кепшенов). например, если в кепшене есть что-то вроде “in a style of van goth”, то они кладут семпл в класс винсент ван гог. разметка получается шумная, тем не менее полученный сабсет называют laion styles. следом авторы берут стили из этого датасета и смотрят, насколько стилевые эмбеды картинок в датасете похожи на стилевые эмбеды генераций. пробегая по большому количеству классов используют это как оценку умения моделей генерировать изображения и подражать разным стилям. flashtex: fast relightable mesh texturing with light controlnet работа посвящена генерации text-to-texture для трёх моделей. они фиксируют регулярные параметры освещения и позы камеры, а также три типа материалов. затем рендерят мэш входного объекта при вышеописанных условиях для каждого материала, подают в controlnet в качестве кондишена и учат текстурировать мэш в 2d. затем авторы представляют материал в виде multi-resolution hash grid. и проводят оптимизацию материала с классическими лоссами: реконструкция по выходам controlnet light для регулярных параметров света и камеры и sds — как у dreamfusion — для непрерывных параметров. для консистентности multiview генерации авторы подают в controlnet коллаж сразу с несколькими параметрами камеры. параметризация controlnet по свету позволяет отделить материал модели от освещения для генерации более реалистичной текстуры. рассказали об интересном и крутом ❣ александр устюжанин и сергей кастрюлин. cv time #yaeccv">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-10-02T13:33:40+00:00" href="./posts/7.html">2024-10-02 13:33 UTC</a></div>
      </div>
      <div class="post-body"><strong>Ещё интересное с ECCV 2024</strong><br><br>Продолжаем освещать конференцию и подсвечивать самые занятные воркшопы и доклады. <br><br><strong>Рассказ из воркшопа Knowledge in Generative Models</strong><br><br>Авторы задаются вопросом: как в сети кодируются знания о каком-либо визуальном образе? Классический способ — сегментировать объект на изображении, а потом посмотреть активации нейронов, которые ведут к пикселям внутри маски. Однако этот способ обнаруживает далеко не всю информацию, которой обладает модель. <br><br>Предлагается взять множество «картиночных» моделей: генеративную StyleGAN2, дискриминативные DINO VIT, ResNet и так далее, — а затем посмотреть на одних картинках схожесть в послойных активациях. Для этого генерируем картинку через StyleGAN2, а затем прогоняем через дискриминативные модели.Все найденные пары — и есть искомые знания в модели. Далее эти Rosetta-нейроны можно использовать в инверсии и редактировании. Но это для ганов.<br><br>Для диффузии эта штука не работает, потому что активации нейронов меняются со временем. Поэтому в качестве постоянной компоненты предлагают использовать веса модели. Следует файнтюнить модель на разные концепты, потом рассматривать полученные веса как точку в пространстве весов. Здесь можно найти интересную линейную делимость по некоторым признакам, а также получается непрерывно сэмплить (близкие точки семантически похожи) картинки.<br><br><a href="https://eccv.ecva.net/virtual/2024/poster/278" rel="nofollow noopener noreferrer"><strong>Investigating Style Similarity in Diffusion Models</strong></a><br><br>Авторы стремятся понять, умеет ли модель воспроизводить стили художников из реально сделанных ими картин. Классические SSL-методы вроде CLIP кодируют семантическую информацию и, соответственно, для такого анализа не подходят. Поэтому авторы сначала обучают модельку для выделения стилистических эмбедов и даже выкладывают её. <br><br>Дальше они берут LAION-aesthetics, выделяют оттуда сабсет на 12 миллионов пар с эстетическим скором более шести и парсят так, чтобы выделить информацию о стилях (на основе кепшенов). Например, Если в кепшене есть что-то вроде “in a style of van Goth”, то они кладут семпл в класс Винсент ван Гог. Разметка получается шумная, тем не менее полученный сабсет называют LAION styles. <br><br>Следом авторы берут стили из этого датасета и смотрят, насколько стилевые эмбеды картинок в датасете похожи на стилевые эмбеды генераций. Пробегая по большому количеству классов используют это как оценку умения моделей генерировать изображения и подражать разным стилям. <br><br><a href="https://arxiv.org/abs/2402.13251" rel="nofollow noopener noreferrer"><strong>FlashTex: Fast Relightable Mesh Texturing with Light ControlNet</strong></a><br><br>Работа посвящена генерации text-to-texture для трёх моделей. Они фиксируют регулярные параметры освещения и позы камеры, а также три типа материалов. Затем рендерят мэш входного объекта при вышеописанных условиях для каждого материала, подают в ControlNet в качестве кондишена и учат текстурировать мэш в 2D. Затем авторы представляют материал в виде multi-resolution hash grid. И проводят оптимизацию материала с классическими лоссами: реконструкция по выходам ControlNet Light для регулярных параметров света и камеры и SDS —  как у DreamFusion — для непрерывных параметров.<br><br>Для консистентности multiview генерации авторы подают в ControlNet коллаж сразу с несколькими параметрами камеры. Параметризация ControlNet по свету позволяет отделить материал модели от освещения для генерации более реалистичной текстуры. <br><br><em>Рассказали об интересном и крутом </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Александр Устюжанин и Сергей Кастрюлин.</em><br><br><a href="https://t.me/+zRuUtoQN_QNiOWMy" rel="nofollow noopener noreferrer">CV Time</a><br><br>#YaECCV</div>
      <div class="actions">
        <span>1 091 просмотров · 13 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/7" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/7.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="6" data-search="интересное с eccv 2024 29 сентября в милане стартовала конференция eccv 2024, которая собрала ведущих специалистов в области компьютерного зрения — в том числе, и от яндекса. мы не могли обойти такое громкое событие стороной, и будем освещать его с самых разных сторон. но прежде всего расскажем о самых интересных докладах первого дня мероприятия. к делу! synthetic to authentic: transferring realism to 3d face renderings for boosting face recognition авторы предлагают «механизм реалистичности», который позволяет сократить доменный сдвиг между реальными и синтетическими изображения людей. в итоге — более высокие результаты на исходной задаче по сравнению с обучением на синтетике и без преобразований. на некоторых бенчмарках использование такого механизма сравнимо даже с обучением на реальных данных. dpa-net: structured 3d abstraction from sparse views via differentiable primitive assembly современные text-to-3d и image-to-3d-модели не очень хорошо понимают физику — они рисуют неправильные тени и формы объектов. исследователи предлагают разные подходы к решению этой проблемы: например, использование диффузии и технологий рендера вроде nerf или 3d gaussian splatting. автор же представленной работы утверждает, что для понимания физики требуется строить сетку с 3d моделированием (противопоставление rendering и modeling) через примитивы. воркшоп knowledge as association дэвид бау из северо-восточного университета предполагает, что модели содержат в себе две сущности. первая — knowledge — отражает знания. они находятся в весах модели. вторая — memory — это то, что модель может вспомнить в процессе генерации при определённых входных данных. memory содержится в активациях. доклады бережно отбирали ❣ дарья виноградова, александр устюжанин и сергей кастрюлин. cv time #yaeccv интересное с eccv 2024 29 сентября в милане стартовала конференция eccv 2024, которая собрала ведущих специалистов в области компьютерного зрения — в том числе, и от яндекса. мы не могли обойти такое громкое событие стороной, и будем освещать его с самых разных сторон. но прежде всего расскажем о самых интересных докладах первого дня мероприятия. к делу! synthetic to authentic: transferring realism to 3d face renderings for boosting face recognition авторы предлагают «механизм реалистичности», который позволяет сократить доменный сдвиг между реальными и синтетическими изображения людей. в итоге — более высокие результаты на исходной задаче по сравнению с обучением на синтетике и без преобразований. на некоторых бенчмарках использование такого механизма сравнимо даже с обучением на реальных данных. dpa-net: structured 3d abstraction from sparse views via differentiable primitive assembly современные text-to-3d и image-to-3d-модели не очень хорошо понимают физику — они рисуют неправильные тени и формы объектов. исследователи предлагают разные подходы к решению этой проблемы: например, использование диффузии и технологий рендера вроде nerf или 3d gaussian splatting . автор же представленной работы утверждает, что для понимания физики требуется строить сетку с 3d моделированием (противопоставление rendering и modeling) через примитивы. воркшоп knowledge as association дэвид бау из северо-восточного университета предполагает, что модели содержат в себе две сущности. первая — knowledge — отражает знания. они находятся в весах модели. вторая — memory — это то, что модель может вспомнить в процессе генерации при определённых входных данных. memory содержится в активациях. доклады бережно отбирали ❣ дарья виноградова, александр устюжанин и сергей кастрюлин. cv time #yaeccv">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-10-01T12:39:33+00:00" href="./posts/6.html">2024-10-01 12:39 UTC</a></div>
      </div>
      <div class="post-body"><strong>Интересное с ECCV 2024</strong><br><br>29 сентября в Милане стартовала конференция ECCV 2024, которая собрала ведущих специалистов в области компьютерного зрения — в том числе, и от Яндекса. Мы не могли обойти такое громкое событие стороной, и будем освещать его с самых разных сторон. Но прежде всего расскажем о самых интересных докладах первого дня мероприятия. К делу!<br><br><a href="https://arxiv.org/abs/2407.07627" rel="nofollow noopener noreferrer"><strong>Synthetic to Authentic: Transferring Realism to 3D Face Renderings for Boosting Face Recognition</strong></a><br><br>Авторы предлагают «механизм реалистичности», который позволяет сократить доменный сдвиг между реальными и синтетическими изображения людей. В итоге — более высокие результаты на исходной задаче по сравнению с обучением на синтетике и без преобразований. На некоторых бенчмарках использование такого механизма сравнимо даже с обучением на реальных данных.<br><br><a href="https://arxiv.org/abs/2404.00875" rel="nofollow noopener noreferrer"><strong>DPA-Net: Structured 3D Abstraction from Sparse Views via Differentiable Primitive Assembly</strong></a><br><br>Современные text-to-3D и image-to-3D-модели не очень хорошо понимают физику — они рисуют неправильные тени и формы объектов. Исследователи предлагают разные подходы к решению этой проблемы: например, использование диффузии и технологий рендера вроде <a href="https://arxiv.org/abs/2211.10440" rel="nofollow noopener noreferrer">NeRF</a> или <a href="https://arxiv.org/abs/2309.16653" rel="nofollow noopener noreferrer">3D Gaussian Splatting</a>. Автор же представленной работы утверждает, что для понимания физики требуется строить сетку с 3D моделированием (противопоставление rendering и modeling) через примитивы.<br><br><strong>Воркшоп Knowledge as Association</strong><br><br>Дэвид Бау из Северо-Восточного университета предполагает, что модели содержат в себе две сущности. Первая — Knowledge — отражает знания. Они находятся в весах модели. Вторая — Memory — это то, что модель может вспомнить в процессе генерации при определённых входных данных. Memory содержится в активациях. <br><br><em>Доклады бережно отбирали </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Дарья Виноградова, Александр Устюжанин и Сергей Кастрюлин.<br></em><br><a href="https://t.me/+c50-0vl6aNcyMzI6" rel="nofollow noopener noreferrer">CV Time</a><br><br>#YaECCV<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/6_480.webp" srcset="../assets/media/thumbs/6_480.webp 480w, ../assets/media/6.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="6" data-image-index="0" /></div></div>
      <div class="actions">
        <span>3 642 просмотров · 18 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/6" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/6.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="5" data-search="paligemma — открытая и производительная vlm сегодняшняя статья о paligemma. это vlm, которая разработана на основе технологий семейства моделей pali и gemma. на вход paligemma может получать изображения или видео как последовательность кадров. модель обучается в четыре этапа: unimodal pretraining — на этом этапе pali и gemma обучаются отдельно на данных одного типа. multimodal pretraining — здесь вся модель обучается на большом — миллиард сэмплов — наборе мультимодальных задач, таких как объединение визуальных и текстовых данных. здесь важно, что ни одна часть модели не остаётся «замороженной» — все её компоненты обучаются вместе. resolution increase — происходит дополнительное обучение на данных с более высоким разрешением изображений (448 и 896 пикселей). трансфер на целевые задачи вроде описания изображений для coco, vqa для дистанционного зондирования и другие специализированные цели. этот этап также предполагает дообучение на новых данных, чтобы модель могла решать задачи, для которых она ранее не обучалась. важная особенность paigemma — использование подхода prefix-lm. ко всей входной последовательности — изображениям и префиксам — здесь применяется фулл (двунаправленный) аттеншн. это сделано для того, чтобы большее количество токенов могло активно участвовать в процессе «мышления» с самого начала, так как токены изображения могут обращаться к токенам префикса, которые представляют собой запрос. для суффиксов используют кэжуал аттеншн для в авторегресионной генерации. это позволяет генерировать ответы последовательно, предсказывая следующий токен на основе предыдущих. в задаче генерации описаний для изображений на датасете coco paligemma выдала результат 141,9 балла при разрешении 224 пикселей и улучшила показатели до 144,6 на 448. в задаче визуальных вопросов и ответов vqav2 модель продемонстрировала результат в 83,2 и 85,6 балла для разрешения в 224 и 448 пикселей соответственно. разбор подготовил ❣ александр шишеня cv time paligemma — открытая и производительная vlm сегодняшняя статья о paligemma. это vlm, которая разработана на основе технологий семейства моделей pali и gemma. на вход paligemma может получать изображения или видео как последовательность кадров. модель обучается в четыре этапа: unimodal pretraining — на этом этапе pali и gemma обучаются отдельно на данных одного типа. multimodal pretraining — здесь вся модель обучается на большом — миллиард сэмплов — наборе мультимодальных задач, таких как объединение визуальных и текстовых данных. здесь важно, что ни одна часть модели не остаётся «замороженной» — все её компоненты обучаются вместе. resolution increase — происходит дополнительное обучение на данных с более высоким разрешением изображений (448 и 896 пикселей). трансфер на целевые задачи вроде описания изображений для coco, vqa для дистанционного зондирования и другие специализированные цели. этот этап также предполагает дообучение на новых данных, чтобы модель могла решать задачи, для которых она ранее не обучалась. важная особенность paigemma — использование подхода prefix-lm. ко всей входной последовательности — изображениям и префиксам — здесь применяется фулл (двунаправленный) аттеншн. это сделано для того, чтобы большее количество токенов могло активно участвовать в процессе «мышления» с самого начала, так как токены изображения могут обращаться к токенам префикса, которые представляют собой запрос. для суффиксов используют кэжуал аттеншн для в авторегресионной генерации. это позволяет генерировать ответы последовательно, предсказывая следующий токен на основе предыдущих. в задаче генерации описаний для изображений на датасете coco paligemma выдала результат 141,9 балла при разрешении 224 пикселей и улучшила показатели до 144,6 на 448. в задаче визуальных вопросов и ответов vqav2 модель продемонстрировала результат в 83,2 и 85,6 балла для разрешения в 224 и 448 пикселей соответственно. разбор подготовил ❣ александр шишеня cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-09-27T11:40:20+00:00" href="./posts/5.html">2024-09-27 11:40 UTC</a></div>
      </div>
      <div class="post-body"><strong>PaliGemma — открытая и производительная VLM</strong><br><br><a href="https://arxiv.org/abs/2407.07726" rel="nofollow noopener noreferrer">Сегодняшняя статья</a> о PaliGemma. Это VLM, которая разработана на основе технологий семейства моделей PaLI и Gemma. На вход  PaliGemma может получать изображения или видео как последовательность кадров. <br><br>Модель обучается в четыре этапа:<br><br><strong>Unimodal pretraining</strong> — на этом этапе PaLI и Gemma обучаются отдельно на данных одного типа. <br><br><strong>Multimodal pretraining</strong> — здесь вся модель обучается на большом — миллиард сэмплов — наборе мультимодальных задач, таких как объединение визуальных и текстовых данных. Здесь важно, что ни одна часть модели не остаётся «замороженной» — все её компоненты обучаются вместе.<br><br><strong>Resolution increase </strong>— происходит дополнительное обучение на данных с более высоким разрешением изображений (448 и 896 пикселей).<br><br><strong>Трансфер на целевые задачи</strong> вроде описания изображений для COCO, VQA для дистанционного зондирования и другие специализированные цели. Этот этап также предполагает дообучение на новых данных, чтобы модель могла решать задачи, для которых она ранее не обучалась.<br><br>Важная особенность PaiGemma — использование подхода prefix-LM. Ко всей входной последовательности — изображениям и префиксам — здесь применяется фулл (двунаправленный) аттеншн. Это сделано для того, чтобы большее количество токенов могло активно участвовать в процессе «мышления» с самого начала, так как токены изображения могут обращаться к токенам префикса, которые представляют собой запрос.<br><br>Для суффиксов используют кэжуал аттеншн для в авторегресионной генерации. Это позволяет генерировать ответы последовательно, предсказывая следующий токен на основе предыдущих.<br><br>В задаче генерации описаний для изображений на датасете COCO PaliGemma выдала результат 141,9 балла при разрешении 224 пикселей и улучшила показатели до 144,6 на 448. В задаче визуальных вопросов и ответов VQAv2 модель продемонстрировала результат в 83,2 и 85,6 балла для разрешения в 224 и 448 пикселей соответственно. <br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Александр Шишеня<br></em><a href="https://t.me/+BVce85g2BehhNzZi" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/5_480.webp" srcset="../assets/media/thumbs/5_480.webp 480w, ../assets/media/5.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="5" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 444 просмотров · 18 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/5" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/5.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="4" data-search="one-step diffusion with distribution matching distillation с помощью диффузионных моделей можно генерировать изображения отличного качества по текстовому описанию, но без дистилляции они требуют десятки прогонов модели, что снижает их эффективность эффективными по сравнению с альтернативными парадигмами. авторы сегодняшней статьи разработали distribution matching distillation (dmd) — процедуру дистилляции диффузионной модели (учителя) в одношаговый генератор изображений (студента), который хорошо сохраняет качество картинок и их соответствие текстовому описанию. авторы предлагают учить одношаговый генератор изображений p_\theta(x) за счет минимизации обратной kl-дивергенции между распределением студента и распределением данных: kl(p_\theta(x) || p_{data}). напомним, что в таком kl-е мат-ожидание берется по распределению студента и следовательно при оценке методом монте-карло примеры сэмплируются из текущей версии модели-студента. тем не менее, посчитать обратный kl явно все еще не получится. однако главная идея работы заключается в том, что мы можем оценить производную обратного kl-я, используя прямой проход уже обученных диффузионных моделей: одна модель оценивает скор реального распределения (\nabla_x \log p_data(x)), а другая скор распределения студента (\nabla_x \log p_\theta(x)). отметим, что в самом начале у нас есть только одна модель, которая оценивает \nabla_x \log p_data(x), а вторую нужно учить на текущих генерациях студентов тем самым адаптируя её под изменяющееся распределение студента по ходу обучения. процедура напоминает обучение gan-a, но с другим обджективом. внимательный читатель обратит внимание, что диффузия обычно предсказывает скоры для зашумленных распределений, а не для чистых, как описано выше. но в данном методе это не баг, а фича, о которой лучше почитать в самой статье. важный нюанс при минимизации обратного kl — он склонен к потере мод распределения, что часто выражается в генерации однотипных картинок для одного текстового описания. чтобы побороть проблему, авторы предлагают добавить дополнительный дистилляционный лосс, который заставляет модель не забывать куски распределения учителя. дополнительный обджектив представляет собой регрессионный лосс, когда сначала собираются пары (шум — картинка), сгенерированные учителем, и затем по mse многошаговый выход учителя сближается с одношаговым студентом по тому же входному шуму. таким образом, итоговая процедура обучения состоит из двух лоссов: обратный kl, отвечающий за качество генерации студента, и регрессионный лосс, стабилизирующий процедуру обучения и не дающий потерять разнообразие. по словам авторов статьи, такой подход позволяет добиться лучших результатов, чем предыдущие методы (на момент декабря 2023 года), достигая 2.62 fid на imagenet 64x64 и 11.49 fid на zero-shot coco-30k, что сравнимо с результатами stable diffusion, но при этом dmd работает гораздо быстрее, генерируя изображения со скоростью до 20 fps. однако, стоит обратить внимание, что метрики для генерации картинок — ненадежный показатель качества, поэтому в идеале нужно проводить оценку на людях, чего авторы не сделали. модели и кода в открытом доступе тоже не было, поэтому о реальном качестве результатов остается только догадываться. в оправдание авторов: спустя полгода команда выпустила улучшенную версию метода, к которой приложили и код, и модели, и полноценную оценку качества генерации. почитать о новом методе можно здесь. разбор подготовил ❣ дмитрий баранчук cv time one-step diffusion with distribution matching distillation с помощью диффузионных моделей можно генерировать изображения отличного качества по текстовому описанию, но без дистилляции они требуют десятки прогонов модели, что снижает их эффективность эффективными по сравнению с альтернативными парадигмами. авторы сегодняшней статьи разработали distribution matching distillation (dmd) — процедуру дистилляции диффузионной модели (учителя) в одношаговый генератор изображений (студента), который хорошо сохраняет качество картинок и их соответствие текстовому описанию. авторы предлагают учить одношаговый генератор изображений p_\theta(x) за счет минимизации обратной kl-дивергенции между распределением студента и распределением данных: kl(p_\theta(x) || p_{data}) . напомним, что в таком kl-е мат-ожидание берется по распределению студента и следовательно при оценке методом монте-карло примеры сэмплируются из текущей версии модели-студента. тем не менее, посчитать обратный kl явно все еще не получится. однако главная идея работы заключается в том, что мы можем оценить производную обратного kl-я, используя прямой проход уже обученных диффузионных моделей: одна модель оценивает скор реального распределения (\nabla_x \log p_data(x)) , а другая скор распределения студента (\nabla_x \log p_\theta(x)) . отметим, что в самом начале у нас есть только одна модель, которая оценивает \nabla_x \log p_data(x) , а вторую нужно учить на текущих генерациях студентов тем самым адаптируя её под изменяющееся распределение студента по ходу обучения. процедура напоминает обучение gan-a, но с другим обджективом. внимательный читатель обратит внимание, что диффузия обычно предсказывает скоры для зашумленных распределений, а не для чистых, как описано выше. но в данном методе это не баг, а фича, о которой лучше почитать в самой статье. важный нюанс при минимизации обратного kl — он склонен к потере мод распределения, что часто выражается в генерации однотипных картинок для одного текстового описания. чтобы побороть проблему, авторы предлагают добавить дополнительный дистилляционный лосс, который заставляет модель не забывать куски распределения учителя. дополнительный обджектив представляет собой регрессионный лосс, когда сначала собираются пары (шум — картинка), сгенерированные учителем, и затем по mse многошаговый выход учителя сближается с одношаговым студентом по тому же входному шуму. таким образом, итоговая процедура обучения состоит из двух лоссов: обратный kl, отвечающий за качество генерации студента, и регрессионный лосс, стабилизирующий процедуру обучения и не дающий потерять разнообразие. по словам авторов статьи, такой подход позволяет добиться лучших результатов, чем предыдущие методы (на момент декабря 2023 года), достигая 2.62 fid на imagenet 64x64 и 11.49 fid на zero-shot coco-30k, что сравнимо с результатами stable diffusion, но при этом dmd работает гораздо быстрее, генерируя изображения со скоростью до 20 fps. однако, стоит обратить внимание, что метрики для генерации картинок — ненадежный показатель качества, поэтому в идеале нужно проводить оценку на людях, чего авторы не сделали. модели и кода в открытом доступе тоже не было, поэтому о реальном качестве результатов остается только догадываться. в оправдание авторов: спустя полгода команда выпустила улучшенную версию метода, к которой приложили и код, и модели, и полноценную оценку качества генерации. почитать о новом методе можно здесь. разбор подготовил ❣ дмитрий баранчук cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-09-24T12:28:13+00:00" href="./posts/4.html">2024-09-24 12:28 UTC</a></div>
      </div>
      <div class="post-body"><strong>One-step Diffusion with Distribution Matching Distillation</strong><br><br>С помощью диффузионных моделей можно генерировать изображения отличного качества по текстовому описанию, но без дистилляции они требуют десятки прогонов модели, что снижает их эффективность эффективными по сравнению с альтернативными парадигмами. Авторы <a href="https://arxiv.org/abs/2311.18828" rel="nofollow noopener noreferrer">сегодняшней статьи</a> разработали Distribution Matching Distillation (DMD) — процедуру дистилляции диффузионной модели (учителя) в одношаговый генератор изображений (студента), который хорошо сохраняет качество картинок и их соответствие текстовому описанию. <br><br>Авторы предлагают учить одношаговый генератор изображений <code>p_\theta(x)</code> за счет минимизации обратной KL-дивергенции между распределением студента и распределением данных: <code>KL(p_\theta(x) || p_{data})</code>. Напомним, что в таком KL-е мат-ожидание берется по распределению студента и следовательно при оценке методом Монте-Карло примеры сэмплируются из текущей версии модели-студента. <br><br>Тем не менее, посчитать обратный KL явно все еще не получится. Однако главная идея работы заключается в том, что мы можем оценить производную обратного KL-я, используя прямой проход уже обученных диффузионных моделей: одна модель оценивает скор реального распределения <code>(\nabla_x \log p_data(x))</code>, а другая скор распределения студента <code>(\nabla_x \log p_\theta(x))</code>. Отметим, что в самом начале у нас есть только одна модель, которая оценивает <code>\nabla_x \log p_data(x)</code>, а вторую нужно учить на текущих генерациях студентов тем самым адаптируя её под изменяющееся распределение студента по ходу обучения. Процедура напоминает обучение GAN-a, но с другим обджективом. <br><br>Внимательный читатель  обратит внимание, что диффузия обычно предсказывает скоры для зашумленных распределений, а не для чистых, как описано выше. Но в данном методе это не баг, а фича, о которой лучше почитать в самой <a href="https://arxiv.org/abs/2311.18828" rel="nofollow noopener noreferrer">статье.</a>   <br><br>Важный нюанс при минимизации обратного KL — он склонен к потере мод распределения, что часто выражается в генерации однотипных картинок для одного текстового описания. Чтобы побороть проблему, авторы предлагают добавить дополнительный дистилляционный лосс, который заставляет модель не забывать куски распределения учителя. Дополнительный обджектив представляет собой регрессионный лосс, когда сначала собираются пары (шум — картинка), сгенерированные учителем, и затем по MSE многошаговый выход учителя сближается с одношаговым студентом по тому же входному шуму. <br><br>Таким образом, итоговая процедура обучения состоит из двух лоссов: обратный KL, отвечающий за качество генерации студента, и регрессионный лосс, стабилизирующий процедуру обучения и не дающий потерять разнообразие.<br><br>По словам авторов статьи, такой подход позволяет добиться лучших результатов, чем предыдущие методы (на момент декабря 2023 года), достигая 2.62 FID на ImageNet 64x64 и 11.49 FID на zero-shot COCO-30k, что сравнимо с результатами Stable Diffusion, но при этом DMD работает гораздо быстрее, генерируя изображения со скоростью до 20 FPS. Однако, стоит обратить внимание, что метрики для генерации картинок — ненадежный показатель качества, поэтому в идеале нужно проводить оценку на людях, чего авторы не сделали. Модели и кода в открытом доступе тоже не было, поэтому о реальном качестве результатов остается только догадываться.<br><br>В оправдание авторов: спустя полгода команда выпустила улучшенную версию метода, к которой приложили и код, и модели, и полноценную оценку качества генерации. Почитать о новом методе можно <a href="https://arxiv.org/abs/2405.14867" rel="nofollow noopener noreferrer">здесь.</a><br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Дмитрий Баранчук</em><br><a href="https://t.me/+hatCKgr23S5iMjdi" rel="nofollow noopener noreferrer">CV Time</a></div>
      <div class="actions">
        <span>1 208 просмотров · 15 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/4" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/4.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="3" data-search="internvl визуальные модели в своем развитии несколько отстают от языковых, хотя и то, и то важно для создания сильного искусственного интеллекта (agi). преодолеть разрыв попытались авторы сегодняшней статьи, в которой представлена модель internvl. internvl отличается тем, что включает визуальный энкодер internvit-6b и языковую прослойку qllama с 8 миллиардами параметров. обучение проходило в несколько этапов: 1. контрастивный претрейн. здесь использовали миллионы пар изображений и текстов из датасетов laion, coyo, cc12m. на этом этапе модель училась сопоставлять картинки и их описание. 2. генеративный претрейн. здесь использовали пары с более чистыми изображениями из таких датасетов, как coco и textcaps. на этом этапе модель училась создавать текстовое описание для картинок. 3. файнтюнинг. в итоге получились две модели, которые призваны решать одну задачу: internvl-c (contrastive) и internvl-g (generative). первая, по сути, это модель раннего связывания, а вторая — позднего. на бенчмарках можно увидеть, какой результат дает ранняя и поздняя привязка изображения к тексту. так, в задаче мультиязычного поиска изображений по тексту на наборе данных xtd, средний показатель recall@10 составил 95,1% для internvl-c и 96,6% для internvl-g. в задаче zero-shot классификации видео на наборах данных kinetics-400, kinetics-600 и kinetics-700, internvl-c продемонстрировала точность 76,1%, 75,5% и 67,5% соответственно. разбор подготовил ❣ артём конев cv time internvl визуальные модели в своем развитии несколько отстают от языковых, хотя и то, и то важно для создания сильного искусственного интеллекта (agi). преодолеть разрыв попытались авторы сегодняшней статьи, в которой представлена модель internvl. internvl отличается тем, что включает визуальный энкодер internvit-6b и языковую прослойку qllama с 8 миллиардами параметров. обучение проходило в несколько этапов: 1. контрастивный претрейн. здесь использовали миллионы пар изображений и текстов из датасетов laion, coyo, cc12m. на этом этапе модель училась сопоставлять картинки и их описание. 2. генеративный претрейн. здесь использовали пары с более чистыми изображениями из таких датасетов, как coco и textcaps. на этом этапе модель училась создавать текстовое описание для картинок. 3. файнтюнинг. в итоге получились две модели, которые призваны решать одну задачу: internvl-c (contrastive) и internvl-g (generative). первая, по сути, это модель раннего связывания, а вторая — позднего. на бенчмарках можно увидеть, какой результат дает ранняя и поздняя привязка изображения к тексту. так, в задаче мультиязычного поиска изображений по тексту на наборе данных xtd, средний показатель recall@10 составил 95,1% для internvl-c и 96,6% для internvl-g. в задаче zero-shot классификации видео на наборах данных kinetics-400, kinetics-600 и kinetics-700, internvl-c продемонстрировала точность 76,1%, 75,5% и 67,5% соответственно. разбор подготовил ❣ артём конев cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-09-20T12:14:47+00:00" href="./posts/3.html">2024-09-20 12:14 UTC</a></div>
      </div>
      <div class="post-body"><strong>InternVL</strong><br><br>Визуальные модели в своем развитии несколько отстают от языковых, хотя и то, и то важно для создания сильного искусственного интеллекта (AGI). Преодолеть разрыв попытались авторы <a href="https://arxiv.org/abs/2312.14238" rel="nofollow noopener noreferrer">сегодняшней статьи,</a> в которой представлена модель InternVL. <br><br>InternVL отличается тем, что включает визуальный энкодер InternViT-6B и языковую прослойку QLLaMA с 8 миллиардами параметров. Обучение проходило в несколько этапов:<br><br><strong>1. Контрастивный претрейн.</strong> Здесь использовали миллионы пар изображений и текстов из датасетов LAION, COYO, CC12M. На этом этапе модель училась сопоставлять картинки и их описание.<br><br><strong>2. Генеративный претрейн.</strong> Здесь использовали пары с более чистыми изображениями из таких датасетов, как COCO и TextCaps. На этом этапе модель училась создавать текстовое описание для картинок. <br><br><strong>3. Файнтюнинг.</strong><br><br>В итоге получились две модели, которые призваны решать одну задачу: InternVL-C (contrastive) и InternVL-G (generative). Первая, по сути, это модель раннего связывания, а вторая — позднего. На бенчмарках можно увидеть, какой результат дает ранняя и поздняя привязка изображения к тексту. <br><br>Так, в задаче мультиязычного поиска изображений по тексту на наборе данных XTD, средний показатель recall@10 составил 95,1% для InternVL-C и 96,6% для InternVL-G. В задаче zero-shot классификации видео на наборах данных Kinetics-400, Kinetics-600 и Kinetics-700, InternVL-C продемонстрировала точность 76,1%, 75,5% и 67,5% соответственно.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Артём Конев</em><br><a href="https://t.me/+hatCKgr23S5iMjdi" rel="nofollow noopener noreferrer">CV Time</a></div>
      <div class="actions">
        <span>972 просмотров · 14 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/3" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/3.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="2" data-search="привет! это канал, полностью посвященный cv. здесь мы, специалисты по компьютерному зрению из яндекса, будем анализировать и комментировать свежие научные публикации, делиться опытом, рассказывать об интересных кейсах из практики и вместе с вами искать ответы на сложные вопросы. подписывайтесь и приглашайте друзей и коллег, если вам близка тема cv 👀🤖 привет! это канал, полностью посвященный cv. здесь мы, специалисты по компьютерному зрению из яндекса, будем анализировать и комментировать свежие научные публикации, делиться опытом, рассказывать об интересных кейсах из практики и вместе с вами искать ответы на сложные вопросы. подписывайтесь и приглашайте друзей и коллег, если вам близка тема cv 👀🤖">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-09-20T12:11:20+00:00" href="./posts/2.html">2024-09-20 12:11 UTC</a></div>
      </div>
      <div class="post-body">Привет! Это канал, полностью посвященный CV. Здесь мы, специалисты по компьютерному зрению из Яндекса, будем анализировать и комментировать свежие научные публикации, делиться опытом, рассказывать об интересных кейсах из практики и вместе с вами искать ответы на сложные вопросы. Подписывайтесь и приглашайте друзей и коллег, если вам близка тема CV 👀🤖</div>
      <div class="actions">
        <span>829 просмотров · 14 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/2" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/2.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    </div>
    
    <div class="pager static-pager" style="justify-content:center">
      <div class="page-links">
        <a class="nav-link" href="page-3.html">←</a>
        <a class="page-link" href="index.html">1</a> <a class="page-link" href="page-2.html">2</a> <a class="page-link" href="page-3.html">3</a> <a class="page-link current" href="page-4.html">4</a>
        <a class="nav-link disabled" href="#">→</a>
      </div>
    </div>
    
  </main>

  <footer class="footer">
    <div class="container">
      <div class="footer-inner">
        <span>based on <a href="https://github.com/ml-brand/tg-to-gh-pages" target="_blank" rel="noopener">tg-to-gh-pages</a> (created by <a href="https://github.com/ml-brand" target="_blank" rel="noopener">ML Brand</a>)</span>
        <a id="repoLink" href="https://github.com/ml-brand/tg-to-gh-pages" target="_blank" rel="noopener">Do the same with your channel.</a>
        <span class="footer-links">
          static copy ·
          <a href="../feed.xml" target="_blank" rel="noopener">RSS</a> ·
          <a href="../atom.xml" target="_blank" rel="noopener">Atom</a>
        </span>
      </div>
    </div>
  </footer>

  <script>
    window.__STATIC_POSTS = [{"id": 20, "media": [{"kind": "photo", "path": "../assets/media/20.jpg", "thumb": "../assets/media/thumbs/20_480.webp", "size": 54471, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/21.jpg", "thumb": "../assets/media/thumbs/21_480.webp", "size": 63057, "mime": "image/jpeg", "name": null}]}, {"id": 19, "media": []}, {"id": 18, "media": [{"kind": "video", "path": "../assets/media/18.mp4", "thumb": null, "size": 1359769, "mime": "video/mp4", "name": null}]}, {"id": 17, "media": []}, {"id": 16, "media": []}, {"id": 15, "media": [{"kind": "video", "path": "../assets/media/15.mp4", "thumb": null, "size": 4989775, "mime": "video/mp4", "name": null}]}, {"id": 13, "media": []}, {"id": 12, "media": []}, {"id": 9, "media": [{"kind": "video", "path": "../assets/media/9_IMG_1264.MP4.mp4", "thumb": null, "size": 524147, "mime": "video/mp4", "name": "IMG_1264.MP4"}]}, {"id": 8, "media": [{"kind": "video", "path": "../assets/media/8______.mp4", "thumb": null, "size": 2539105, "mime": "video/mp4", "name": "_____.mp4"}]}, {"id": 7, "media": []}, {"id": 6, "media": [{"kind": "photo", "path": "../assets/media/6.jpg", "thumb": "../assets/media/thumbs/6_480.webp", "size": 223109, "mime": "image/jpeg", "name": null}]}, {"id": 5, "media": [{"kind": "photo", "path": "../assets/media/5.jpg", "thumb": "../assets/media/thumbs/5_480.webp", "size": 79043, "mime": "image/jpeg", "name": null}]}, {"id": 4, "media": []}, {"id": 3, "media": []}, {"id": 2, "media": []}];
    window.__STATIC_META = {"title": "CV Time", "username": "timeforcv", "channel": "timeforcv", "last_sync_utc": "2026-02-10T07:45:40Z", "posts_count": 106, "last_seen_message_id": 237, "stats": {"new": 121, "updated": 5, "media_downloaded": 121}, "avatar": "assets/channel_avatar.jpg", "meta_schema_version": "1.0.0", "posts_schema_version": "1.0.0"};
  </script>
  <script src="../common.js"></script>
  <script src="../static.js"></script>
</body>
</html>
