<!doctype html>
<html lang="ru">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>CV Time — статическая версия (стр. 3/4)</title>
  <meta name="description" content="Статическая версия зеркала Telegram-канала" />
  <link rel="icon" href="../favicon.ico?v=2026-02-14T15%3A58%3A11Z" sizes="any" />
  <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32.png?v=2026-02-14T15%3A58%3A11Z" />
  <link rel="apple-touch-icon" href="../apple-touch-icon.png?v=2026-02-14T15%3A58%3A11Z" />

  <link rel="stylesheet" href="../style.css" />
  <script src="../metrika.js"></script>
</head>
<body>
  <header class="header">
    <div class="container">
      <div class="title-grid">
        <a class="grid-avatar" href="#" target="_blank" rel="noopener">
          <img id="channelAvatar" class="channel-avatar" src="../assets/channel_avatar.jpg" alt="Аватар канала"  />
        </a>
        <div class="grid-main">
          <div class="title-head">
            <div class="title-left">
              <a class="badge-chip" id="siteTitleWrap" href="#" target="_blank" rel="noopener"><h1 id="siteTitle">CV Time</h1></a>
            </div>
            <div class="hero-actions">
              <a id="subscribeBtn" class="subscribe-btn" href="https://t.me/+JoULEedmHyE5MmYy" target="_blank" rel="noopener" >Подписаться</a>
              <a class="icon-btn" href="../" aria-label="Перейти к динамической версии">↺</a>
              <button id="themeToggle" class="icon-btn" type="button" aria-label="Переключить тему"></button>
            </div>
          </div>
        </div>
        <div class="controls"></div>
      </div>
    </div>
  </header>

  
  <div id="promoBanner" class="promo-banner" hidden>
    <div class="container promo-inner">
      <span class="promo-text"><a href="https://t.me/addlist/5NH3RoVejEI1MGEy">Подпишись на все наши ML каналы. Они классные, отвечаем!</a></span>
      <button id="promoClose" class="promo-close" type="button" aria-label="Скрыть плашку">×</button>
    </div>
  </div>
  

  <main class="container">
    
    <div class="pager static-pager" style="justify-content:center">
      <div class="page-links">
        <a class="nav-link" href="page-2.html">←</a>
        <a class="page-link" href="index.html">1</a> <a class="page-link" href="page-2.html">2</a> <a class="page-link current" href="page-3.html">3</a> <a class="page-link" href="page-4.html">4</a>
        <a class="nav-link" href="page-4.html">→</a>
      </div>
    </div>
    
    <div id="posts" class="posts">
      
    <article class="post" data-post-id="85" data-search="foundationstereo: zero-shot stereo matching сегодня разбираем статью от nvidia. исследователи решают задачу определения глубины по двум изображениям, снятым с близко расположенных камер, то есть со стереопары. камеры смотрят в одном направлении, поэтому каждая 3d-точка попадает приблизительно на одну строку в обеих картинках, но в разных местах. это позволяет искать соответствия между пикселями одной и той же строки двух изображений, и, используя эти соответствия, восстанавливать глубину сцены. определив соответствия между точками на двух изображениях, можно вычислить диспаритет — сдвиг координат пикселя на одной картинке относительно другой. зная диспаритет, фокусное расстояние и расстояние между оптическими центрами камер, можно пересчитать его в глубину. исследователи из nvidia говорят, что сейчас нет модели стерео-матчинга, которая бы показывала хорошую zero-shot-генерализацию. текущие лучшие решения предлагается дообучать на целевой домен. в других задачах проблему генерализации уже удалось решить за счёт больших данных. например, segment anything обучили на огромном датасете, и модель успешно работает без дообучения. nvidia попробовала применить этот же подход к стерео-матчингу. они собрали фотореалистичный синтетический датасет fsd (картинка 2) из миллиона стереопар, превосходя по объёму и многообразию другие открытые датасеты. датасет выложен в открытый доступ. детали архитектуры из левого и правого изображений (картинка 1) извлекаются фичи из depth anything, конкатенируются с фичами из отдельной обучаемой свёрточной сети. из этой пары создаётся feature cost volume — объём фичей, где каждая описывает похожесть пикселя на левой картинке на пиксели в той же строке на правой картинке и корреляционный cost volume, где похожесть пикселей описывается единственным числом. такие cost volume’ы уже можно использовать для поиска диспаритета, но в них недостаёт глобального контекста картинок. чтобы его добавить, применяется операция ahcf (attentive hybrid cost filtering), особенность которой — использование информации из всего cost volume для получения значений в финальном пикселе выходного тензора; это делается с помощью глобального внимания в transformer-ветви ahcf и с помощью аналога separable-свёрток в свёрточной ветви ahcf. изменение по ablation даёт 10% улучшения по метрике bp-2: доля пикселей, где ошибка диспаритета больше 2 пикселей (0.221 → 0.197). дальше процесс похож на описанный в raft-stereo, но с некоторыми отличиями. в raft-stereo сеть получает на вход hidden state и срез из correlation cost volume. в foundation stereo получает срезы из correlation cost volume и feature cost volume. таким образом, вход в gru включает: — срез cost volume в соответствии с текущей оценкой диспаритета; — фичи левой картинки из отдельно обучаемой контекстной сети (так делалось и в raft-stereo); — саму текущую оценку диспаритета. gru обновляет внутреннее состояние и предсказывает поправку, итеративно уточняя диспаритет. детали обучения модель обучается на смеси fsd-датасета и других датасетов с smoothed l1-лоссом и экспоненциально затухающими l1-добавками для оценок на диспаритет с разных итераций gru-юнита. данные из fsd дополнительно фильтруют по bp-2, используя эту же модель, обученную на полном fsd-датасете, а затем обучают ёще раз. интересное из ablation study: — использование depth anything фичей как входов в feature cost volume не работает совсем (по метрике bp-2); — в separable-свертках для фильтрации feature cost volume используется ядро размера 17(!) по размерности диспаритета (но 1 по spatial-размерности); — добавление fsd-датасета в обучение даёт bp-2 на датасете middlebury в два раза лучше, чем без него. разбор подготовил ❣ леонид штанько cv time foundationstereo: zero-shot stereo matching сегодня разбираем статью от nvidia. исследователи решают задачу определения глубины по двум изображениям, снятым с близко расположенных камер, то есть со стереопары. камеры смотрят в одном направлении, поэтому каждая 3d-точка попадает приблизительно на одну строку в обеих картинках, но в разных местах. это позволяет искать соответствия между пикселями одной и той же строки двух изображений, и, используя эти соответствия, восстанавливать глубину сцены. определив соответствия между точками на двух изображениях, можно вычислить диспаритет — сдвиг координат пикселя на одной картинке относительно другой. зная диспаритет, фокусное расстояние и расстояние между оптическими центрами камер, можно пересчитать его в глубину. исследователи из nvidia говорят, что сейчас нет модели стерео-матчинга, которая бы показывала хорошую zero-shot-генерализацию. текущие лучшие решения предлагается дообучать на целевой домен. в других задачах проблему генерализации уже удалось решить за счёт больших данных. например, segment anything обучили на огромном датасете, и модель успешно работает без дообучения. nvidia попробовала применить этот же подход к стерео-матчингу. они собрали фотореалистичный синтетический датасет fsd ( картинка 2 ) из миллиона стереопар, превосходя по объёму и многообразию другие открытые датасеты. датасет выложен в открытый доступ. детали архитектуры из левого и правого изображений ( картинка 1 ) извлекаются фичи из depth anything, конкатенируются с фичами из отдельной обучаемой свёрточной сети. из этой пары создаётся feature cost volume — объём фичей, где каждая описывает похожесть пикселя на левой картинке на пиксели в той же строке на правой картинке и корреляционный cost volume, где похожесть пикселей описывается единственным числом. такие cost volume’ы уже можно использовать для поиска диспаритета, но в них недостаёт глобального контекста картинок. чтобы его добавить, применяется операция ahcf (attentive hybrid cost filtering), особенность которой — использование информации из всего cost volume для получения значений в финальном пикселе выходного тензора; это делается с помощью глобального внимания в transformer-ветви ahcf и с помощью аналога separable-свёрток в свёрточной ветви ahcf. изменение по ablation даёт 10% улучшения по метрике bp-2: доля пикселей, где ошибка диспаритета больше 2 пикселей (0.221 → 0.197). дальше процесс похож на описанный в raft-stereo , но с некоторыми отличиями. в raft-stereo сеть получает на вход hidden state и срез из correlation cost volume. в foundation stereo получает срезы из correlation cost volume и feature cost volume. таким образом, вход в gru включает: — срез cost volume в соответствии с текущей оценкой диспаритета; — фичи левой картинки из отдельно обучаемой контекстной сети (так делалось и в raft-stereo); — саму текущую оценку диспаритета. gru обновляет внутреннее состояние и предсказывает поправку, итеративно уточняя диспаритет. детали обучения модель обучается на смеси fsd-датасета и других датасетов с smoothed l1-лоссом и экспоненциально затухающими l1-добавками для оценок на диспаритет с разных итераций gru-юнита. данные из fsd дополнительно фильтруют по bp-2, используя эту же модель, обученную на полном fsd-датасете, а затем обучают ёще раз. интересное из ablation study: — использование depth anything фичей как входов в feature cost volume не работает совсем (по метрике bp-2); — в separable-свертках для фильтрации feature cost volume используется ядро размера 17(!) по размерности диспаритета (но 1 по spatial-размерности); — добавление fsd-датасета в обучение даёт bp-2 на датасете middlebury в два раза лучше, чем без него. разбор подготовил ❣ леонид штанько cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-03-28T07:35:14+00:00" href="./posts/85.html">2025-03-28 07:35 UTC</a></div>
      </div>
      <div class="post-body"><strong>FoundationStereo: Zero-Shot Stereo Matching</strong><br><br>Сегодня разбираем <a href="https://nvlabs.github.io/FoundationStereo" rel="nofollow noopener noreferrer">статью</a> от NVIDIA. Исследователи решают задачу определения глубины по двум изображениям, снятым с близко расположенных камер, то есть со стереопары. Камеры смотрят в одном направлении, поэтому каждая 3D-точка попадает приблизительно на одну строку в обеих картинках, но в разных местах. Это позволяет искать соответствия между пикселями одной и той же строки двух изображений, и, используя эти соответствия, восстанавливать глубину сцены.<br><br>Определив соответствия между точками на двух изображениях, можно вычислить диспаритет — сдвиг координат пикселя на одной картинке относительно другой. Зная диспаритет, фокусное расстояние и расстояние между оптическими центрами камер, можно пересчитать его в глубину.<br><br>Исследователи из NVIDIA говорят, что сейчас нет модели стерео-матчинга, которая бы показывала хорошую zero-shot-генерализацию. Текущие лучшие решения предлагается дообучать на целевой домен.<br><br>В других задачах проблему генерализации уже удалось решить за счёт больших данных. Например, Segment Anything обучили на огромном датасете, и модель успешно работает без дообучения. NVIDIA попробовала применить этот же подход к стерео-матчингу. Они собрали фотореалистичный синтетический датасет FSD (<em>картинка 2</em>) из миллиона стереопар, превосходя по объёму и многообразию другие открытые датасеты. Датасет <a href="https://github.com/xieyuankun/FSD-Dataset" rel="nofollow noopener noreferrer">выложен</a> в открытый доступ.<br><br><strong>Детали архитектуры</strong><br><br>Из левого и правого изображений (<em>картинка 1</em>) извлекаются фичи из Depth Anything, конкатенируются с фичами из отдельной обучаемой свёрточной сети. Из этой пары создаётся feature cost volume — объём фичей, где каждая описывает похожесть пикселя на левой картинке на пиксели в той же строке на правой картинке и корреляционный cost volume, где похожесть пикселей описывается единственным числом.<br><br>Такие cost volume’ы уже можно использовать для поиска диспаритета, но в них недостаёт глобального контекста картинок. Чтобы его добавить, применяется операция AHCF (Attentive Hybrid Cost Filtering), особенность которой — использование информации из всего cost volume для получения значений в финальном пикселе выходного тензора; это делается с помощью глобального внимания в transformer-ветви AHCF и с помощью аналога separable-свёрток в свёрточной ветви AHCF. Изменение по ablation даёт 10% улучшения по метрике BP-2: доля пикселей, где ошибка диспаритета больше 2 пикселей (0.221 → 0.197).<br><br>Дальше процесс похож на описанный в <a href="https://arxiv.org/abs/2109.07547" rel="nofollow noopener noreferrer">RAFT-Stereo</a>, но с некоторыми отличиями. В RAFT-Stereo сеть получает на вход hidden state и срез из correlation cost volume. В Foundation Stereo получает срезы из correlation cost volume и feature cost volume.<br><br>Таким образом, вход в GRU включает:<br><br> — срез cost volume в соответствии с текущей оценкой диспаритета;<br> — фичи левой картинки из отдельно обучаемой контекстной сети (так делалось и в RAFT-Stereo);<br> — саму текущую оценку диспаритета.<br>GRU обновляет внутреннее состояние и предсказывает поправку, итеративно уточняя диспаритет.<br><br><strong>Детали обучения</strong><br><br>Модель обучается на смеси FSD-датасета и других датасетов с smoothed L1-лоссом и экспоненциально затухающими L1-добавками для оценок на диспаритет с разных итераций GRU-юнита.<br><br>Данные из FSD дополнительно фильтруют по BP-2, используя эту же модель, обученную на полном FSD-датасете, а затем обучают ёще раз.<br><br><strong>Интересное из ablation study:</strong><br><br> — использование Depth Anything фичей как входов в feature cost volume не работает совсем (по метрике BP-2);<br> — в separable-свертках для фильтрации feature cost volume используется ядро размера 17(!) по размерности диспаритета (но 1 по spatial-размерности);<br> — добавление FSD-датасета в обучение даёт BP-2 на датасете Middlebury в два раза лучше, чем без него.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Леонид Штанько </em><br><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/85_480.webp" srcset="../assets/media/thumbs/85_480.webp 480w, ../assets/media/85.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="85" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/86_480.webp" srcset="../assets/media/thumbs/86_480.webp 480w, ../assets/media/86.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="85" data-image-index="1" /></div></div>
      <div class="actions">
        <span>6 981 просмотров · 27 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/85" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/85.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="84" data-search="яндекс выпустил диффузионную модель yandexart 2.5 сегодня мы зарелизили нашу лучшую модель text-to-image генерации yandexart 2.5. дополнительно приятно, что некоторые её создатели были авторами разборов в этом канале. как удалось добиться нужного качества: — увеличили размер латентного пространства. теперь автокодировщик vae работает с 16 каналами (вместо четырёх, как раньше). — применили технологию «супирования». когда независимо файнтюним несколько моделей и усредняем их веса. — улучшили датасет для обучения, который включает пары «текст-картинка». теперь для каждой картинки разными моделями генерируется несколько описаний, а также берётся текст, описывающий это изображение в интернете. метамодель выбирает лучший из предложенных текстов — именно он идёт в обучение. — увеличили и сам датасет. теперь его размер приблизился к 1 млрд пар картинок и описаний к ним. — ускорили модель с помощью multistep consistency distillation. благодаря этому подходу генерация стала в 9 раз быстрее. всё это и многое другое позволило переиграть midjourney 6.1 и некоторые sota-модели. в таблице показана доля побед yandexart 2.5 pro, а зелёным отмечены победы нашей модели в сравнении с другими. модель уже в шедевруме — пробуйте первыми, делитесь впечатлениями и результатами. cv time яндекс выпустил диффузионную модель yandexart 2.5 сегодня мы зарелизили нашу лучшую модель text-to-image генерации yandexart 2.5. дополнительно приятно, что некоторые её создатели были авторами разборов в этом канале. как удалось добиться нужного качества: — увеличили размер латентного пространства. теперь автокодировщик vae работает с 16 каналами (вместо четырёх, как раньше). — применили технологию « супирования ». когда независимо файнтюним несколько моделей и усредняем их веса. — улучшили датасет для обучения, который включает пары «текст-картинка». теперь для каждой картинки разными моделями генерируется несколько описаний, а также берётся текст, описывающий это изображение в интернете. метамодель выбирает лучший из предложенных текстов — именно он идёт в обучение. — увеличили и сам датасет. теперь его размер приблизился к 1 млрд пар картинок и описаний к ним. — ускорили модель с помощью multistep consistency distillation . благодаря этому подходу генерация стала в 9 раз быстрее. всё это и многое другое позволило переиграть midjourney 6.1 и некоторые sota-модели. в таблице показана доля побед yandexart 2.5 pro, а зелёным отмечены победы нашей модели в сравнении с другими. модель уже в шедевруме — пробуйте первыми, делитесь впечатлениями и результатами. cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-03-20T15:08:59+00:00" href="./posts/84.html">2025-03-20 15:08 UTC</a></div>
      </div>
      <div class="post-body"><strong>Яндекс выпустил диффузионную модель YandexART 2.5</strong><br><br>Сегодня мы зарелизили нашу лучшую модель text-to-image генерации YandexART 2.5. Дополнительно приятно, что некоторые её создатели были авторами разборов в этом канале.<br><br>Как удалось добиться нужного качества:<br><br>— Увеличили размер латентного пространства. Теперь автокодировщик VAE работает с 16 каналами (вместо четырёх, как раньше).<br><br>— Применили технологию «<a href="https://arxiv.org/abs/2203.05482" rel="nofollow noopener noreferrer">супирования</a>». Когда независимо файнтюним несколько моделей и усредняем их веса.<br><br>— Улучшили датасет для обучения, который включает пары «текст-картинка». Теперь для каждой картинки разными моделями генерируется несколько описаний, а также берётся текст, описывающий это изображение в интернете. Метамодель выбирает лучший из предложенных текстов — именно он идёт в обучение.<br><br>— Увеличили и сам датасет. Теперь его размер приблизился к 1 млрд пар картинок и описаний к ним. <br><br>— Ускорили модель с помощью <a href="https://arxiv.org/abs/2403.06807" rel="nofollow noopener noreferrer">multistep consistency distillation</a>. Благодаря этому подходу генерация стала в 9 раз быстрее.<br><br>Всё это и многое другое позволило переиграть Midjourney 6.1 и некоторые SOTA-модели. В таблице показана доля побед YandexART 2.5 Pro, а зелёным отмечены победы нашей модели в сравнении с другими.<br><br>Модель уже <a href="https://shedevrum.ai/" rel="nofollow noopener noreferrer">в Шедевруме</a> — пробуйте первыми, делитесь впечатлениями и результатами.<br><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/84_480.webp" srcset="../assets/media/thumbs/84_480.webp 480w, ../assets/media/84.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="84" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 261 просмотров · 37 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/84" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/84.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="83" data-search="videollama 3: frontier multimodal foundation models for image and video understanding сегодня разбираем статью о videollama 3. по сути — это «yet another vlm» со стандартной архитектурой, описанной на многим знакомой схеме, но есть интересные детали. авторы называют свою модель вижнцентричной (vision-centric) — она умеет работать и с видео, и с картинками. в решении используется визуальный энкодер, который обрабатывает изображения покадрово. картинки передаются в динамическом разрешении и проходят через визуальный трансформер в исходном размере. с видео можно поступить так же, но это приводит к взрывному росту числа токенов, который выходит за пределы контекста опенсорсных моделей. чтобы решить эту проблему, авторы вводят компонент под названием video compressor. с его помощью видео разбивают на патчи и оценивают разницу между кадрами. в каждом новом кадре считается только разница с патчами предыдущего кадра. все кадры кодируются, и каждое изображение превращается в несколько визуальных токенов. затем видеокомпрессор удаляет те, что содержат дублирующуюся информацию. патч считается дублирующим, если разница между соответствующими патчами двух соседних кадров меньше заданного значения. такой подход авторы называют differential frame pruner. он позволяет обрабатывать видео, сохраняя единый визуальный энкодер для картинок и видео. обучение проходит в четыре стадии: 1. vision encoder adaptation — обучают только визуальный энкодер и проекционный слой. используют siglip, который работает с фиксированными разрешениями, и адаптируют его под произвольные. процесс идёт в vlm-сетапе: визуально-языковая модель заморожена, а siglip и проекция — разморожены. обучение проводят на кэпшенах, документах и scene text (blip3-ocr-recap), охватывая разные домены. 2. vision-language alignment — аналог претрейна: вся сеть разморожена, обучают на максимальном объёме данных. 3. multi-task fine-tuning — используют более качественные данные. хотя их объём почти совпадает с претрейном, здесь больше детализированных срезов. 4. video-centric fine-tuning — основной упор на видео и текст, изображений в обучающей выборке меньше. интересен первый этап, где визуальный энкодер адаптируют к произвольному разрешению в vlm-сетапе. дальше обучение идёт по стандартному сценарию. детали реализации авторы используют опенсорсные датасеты для кэпшенов (coyo 700m, vl3-syn7m) и предлагают свой способ перекэпшенивания картинок. сначала делают базовую фильтрацию по aspect ratio и aesthetic score. затем применяют подход text-image similarity calculation: генерируют кэпшен через blip2, вычисляют clip-скор между ним и картинкой. если скор низкий, картинку считают сложной, плохой или нерелевантной — и отбрасывают. выбор blip2 неочевиден, поскольку он генерирует слабые кэпшены, но для фильтрации сложных изображений метод выглядит рабочим. дальше кластеризуют фичи через clip и выбирают изображения из каждого кластера в равных пропорциях. затем перекэпшенивают их с помощью internvl2-26b и получают набор синтетических кэпшенов, которые считают качественными. отдельно интересен способ подачи видео — timestep token. видео позиционно кодируют текстом, добавляя текстовые токены в соответствии с длительностью фрейма. влияет ли это на качество, неясно, ablation-экспериментов нет. другие работы, например qwen, используют отдельные позиционные эмбеддинги с темпоральным измерением, а здесь просто прописывают время текстом. тесты проводились на мультимодальных бенчмарках и показали, что модель стабильно опережает qwen2.5-vl, но подробного сравнения нет. в целом главная проблема статьи — отсутствие полноценных ablation-экспериментов. также интересно, что несмотря на название videollama3, llama здесь нет: в качестве языковой модели используют qwen2.5-2b, в качестве визуальной — siglip. обзор подготовил ❣ андрей чернов cv time videollama 3: frontier multimodal foundation models for image and video understanding сегодня разбираем статью о videollama 3. по сути — это «yet another vlm» со стандартной архитектурой, описанной на многим знакомой схеме , но есть интересные детали. авторы называют свою модель вижнцентричной (vision-centric) — она умеет работать и с видео, и с картинками. в решении используется визуальный энкодер, который обрабатывает изображения покадрово. картинки передаются в динамическом разрешении и проходят через визуальный трансформер в исходном размере. с видео можно поступить так же, но это приводит к взрывному росту числа токенов, который выходит за пределы контекста опенсорсных моделей. чтобы решить эту проблему, авторы вводят компонент под названием video compressor. с его помощью видео разбивают на патчи и оценивают разницу между кадрами. в каждом новом кадре считается только разница с патчами предыдущего кадра. все кадры кодируются, и каждое изображение превращается в несколько визуальных токенов. затем видеокомпрессор удаляет те, что содержат дублирующуюся информацию. патч считается дублирующим, если разница между соответствующими патчами двух соседних кадров меньше заданного значения. такой подход авторы называют differential frame pruner. он позволяет обрабатывать видео, сохраняя единый визуальный энкодер для картинок и видео. обучение проходит в четыре стадии : 1. vision encoder adaptation — обучают только визуальный энкодер и проекционный слой. используют siglip, который работает с фиксированными разрешениями, и адаптируют его под произвольные. процесс идёт в vlm-сетапе: визуально-языковая модель заморожена, а siglip и проекция — разморожены. обучение проводят на кэпшенах, документах и scene text (blip3-ocr-recap), охватывая разные домены. 2. vision-language alignment — аналог претрейна: вся сеть разморожена, обучают на максимальном объёме данных. 3. multi-task fine-tuning — используют более качественные данные. хотя их объём почти совпадает с претрейном, здесь больше детализированных срезов. 4. video-centric fine-tuning — основной упор на видео и текст, изображений в обучающей выборке меньше. интересен первый этап, где визуальный энкодер адаптируют к произвольному разрешению в vlm-сетапе. дальше обучение идёт по стандартному сценарию. детали реализации авторы используют опенсорсные датасеты для кэпшенов (coyo 700m, vl3-syn7m) и предлагают свой способ перекэпшенивания картинок. сначала делают базовую фильтрацию по aspect ratio и aesthetic score. затем применяют подход text-image similarity calculation: генерируют кэпшен через blip2, вычисляют clip-скор между ним и картинкой. если скор низкий, картинку считают сложной, плохой или нерелевантной — и отбрасывают. выбор blip2 неочевиден, поскольку он генерирует слабые кэпшены, но для фильтрации сложных изображений метод выглядит рабочим. дальше кластеризуют фичи через clip и выбирают изображения из каждого кластера в равных пропорциях. затем перекэпшенивают их с помощью internvl2-26b и получают набор синтетических кэпшенов, которые считают качественными. отдельно интересен способ подачи видео — timestep token. видео позиционно кодируют текстом, добавляя текстовые токены в соответствии с длительностью фрейма. влияет ли это на качество, неясно, ablation-экспериментов нет. другие работы, например qwen, используют отдельные позиционные эмбеддинги с темпоральным измерением, а здесь просто прописывают время текстом. тесты проводились на мультимодальных бенчмарках и показали, что модель стабильно опережает qwen2.5-vl, но подробного сравнения нет. в целом главная проблема статьи — отсутствие полноценных ablation-экспериментов. также интересно, что несмотря на название videollama3, llama здесь нет: в качестве языковой модели используют qwen2.5-2b, в качестве визуальной — siglip. обзор подготовил ❣ андрей чернов cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-03-18T08:02:57+00:00" href="./posts/83.html">2025-03-18 08:02 UTC</a></div>
      </div>
      <div class="post-body"><strong>VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding</strong><br><br>Сегодня разбираем <a href="https://arxiv.org/abs/2501.13106" rel="nofollow noopener noreferrer">статью</a> о VideoLLaMA 3. По сути — это «yet another VLM» со стандартной архитектурой, описанной на многим <a href="https://arxiv.org/html/2304.08485v2/x1.png" rel="nofollow noopener noreferrer">знакомой схеме</a>, но есть интересные детали.<br><br>Авторы называют свою модель вижнцентричной (vision-centric) — она умеет работать и с видео, и с картинками. В решении используется визуальный энкодер, который обрабатывает изображения покадрово. Картинки передаются в динамическом разрешении и проходят через визуальный трансформер в исходном размере.<br><br>С видео можно поступить так же, но это приводит к взрывному росту числа токенов, который выходит за пределы контекста опенсорсных моделей. Чтобы решить эту проблему, авторы вводят компонент под названием Video Compressor.<br><br>С его помощью видео разбивают на патчи и оценивают разницу между кадрами. В каждом новом кадре считается только разница с патчами предыдущего кадра. Все кадры кодируются, и каждое изображение превращается в несколько визуальных токенов. Затем видеокомпрессор удаляет те, что содержат дублирующуюся информацию. Патч считается дублирующим, если разница между соответствующими патчами двух соседних кадров меньше заданного значения.<br><br>Такой подход авторы называют Differential Frame Pruner. Он позволяет обрабатывать видео, сохраняя единый визуальный энкодер для картинок и видео. <br><br><strong>Обучение проходит в четыре стадии</strong>:<br><strong><br>1. Vision Encoder Adaptation</strong> — обучают только визуальный энкодер и проекционный слой. Используют SigLIP, который работает с фиксированными разрешениями, и адаптируют его под произвольные. Процесс идёт в VLM-сетапе: визуально-языковая модель заморожена, а SigLIP и проекция — разморожены. Обучение проводят на кэпшенах, документах и Scene Text (BLIP3-OCR-Recap), охватывая разные домены.<br><br><strong>2. Vision-Language Alignment</strong> — аналог претрейна: вся сеть разморожена, обучают на максимальном объёме данных. <br><br><strong>3. Multi-task Fine-tuning </strong>— используют более качественные данные. Хотя их объём почти совпадает с претрейном, здесь больше детализированных срезов.<br><br><strong>4. Video-centric Fine-tuning </strong>— основной упор на видео и текст, изображений в обучающей выборке меньше.<br><br>Интересен первый этап, где визуальный энкодер адаптируют к произвольному разрешению в VLM-сетапе. Дальше обучение идёт по стандартному сценарию.<br><br><strong>Детали реализации<br></strong><br>Авторы используют опенсорсные датасеты для кэпшенов (COYO 700M, VL3-Syn7M) и предлагают свой способ перекэпшенивания картинок.<br><br>Сначала делают базовую фильтрацию по Aspect Ratio и Aesthetic Score. Затем применяют подход Text-Image Similarity Calculation: генерируют кэпшен через BLIP2, вычисляют CLIP-скор между ним и картинкой. Если скор низкий, картинку считают сложной, плохой или нерелевантной — и отбрасывают. Выбор BLIP2 неочевиден, поскольку он генерирует слабые кэпшены, но для фильтрации сложных изображений метод выглядит рабочим.<br><br>Дальше кластеризуют фичи через CLIP и выбирают изображения из каждого кластера в равных пропорциях. Затем перекэпшенивают их с помощью InternVL2-26B и получают набор синтетических кэпшенов, которые считают качественными.<br><br>Отдельно интересен способ подачи видео — Timestep Token. Видео позиционно кодируют текстом, добавляя текстовые токены в соответствии с длительностью фрейма. Влияет ли это на качество, неясно, ablation-экспериментов нет. Другие работы, например Qwen, используют отдельные позиционные эмбеддинги с темпоральным измерением, а здесь просто прописывают время текстом.<br><br>Тесты проводились на мультимодальных бенчмарках и показали, что модель стабильно опережает Qwen2.5-VL, но подробного сравнения нет. В целом главная проблема статьи — отсутствие полноценных ablation-экспериментов.<br><br>Также интересно, что несмотря на название VideoLLaMA3, Llama здесь нет: в качестве языковой модели используют Qwen2.5-2B, в качестве визуальной — SigLIP. <br><br><em>Обзор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Андрей Чернов</em><br><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/83_480.webp" srcset="../assets/media/thumbs/83_480.webp 480w, ../assets/media/83.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="83" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 051 просмотров · 21 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/83" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/83.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="77" data-search="личный опыт инженеров яндекса — антон клочков продолжаем рассказывать об ml`щиках в яндексе, их успехах и трендах, на которые они делают ставку. сегодня наш герой — руководитель подгруппы распознавания текста в vlm антон клочков. больше карточек — по хештэгу #yamlpeople. cv time личный опыт инженеров яндекса — антон клочков продолжаем рассказывать об ml`щиках в яндексе, их успехах и трендах, на которые они делают ставку. сегодня наш герой — руководитель подгруппы распознавания текста в vlm антон клочков. больше карточек — по хештэгу #yamlpeople. cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-03-14T07:37:17+00:00" href="./posts/77.html">2025-03-14 07:37 UTC</a></div>
      </div>
      <div class="post-body"><strong>Личный опыт инженеров Яндекса — Антон Клочков</strong><br><br>Продолжаем рассказывать об ML`щиках в Яндексе, их успехах и трендах, на которые они делают ставку. Сегодня наш герой  — руководитель подгруппы распознавания текста в VLM Антон Клочков.<br><br>Больше карточек — по хештэгу #YaMLpeople.<br><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/77_480.webp" srcset="../assets/media/thumbs/77_480.webp 480w, ../assets/media/77.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="77" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/78_480.webp" srcset="../assets/media/thumbs/78_480.webp 480w, ../assets/media/78.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="77" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/79_480.webp" srcset="../assets/media/thumbs/79_480.webp 480w, ../assets/media/79.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="77" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/80_480.webp" srcset="../assets/media/thumbs/80_480.webp 480w, ../assets/media/80.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="77" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/81_480.webp" srcset="../assets/media/thumbs/81_480.webp 480w, ../assets/media/81.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="77" data-image-index="4" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/82_480.webp" srcset="../assets/media/thumbs/82_480.webp 480w, ../assets/media/82.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="77" data-image-index="5" /></div></div>
      <div class="actions">
        <span>3 975 просмотров · 20 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/77" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/77.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="76" data-search="тематическая подборка статей за февраль: картиночные модели спешим со свежей подборкой интересных статей. в этот раз — о моделях для работы с изображениями. clip остаётся в центре внимания исследователей, но вопросы к его фичам не исчезают. также делимся несколькими работами по архитектуре нейросетей и оптимизации для мобильных устройств. image-to-text cross the gap: exposing the intra-modal misalignment in clip via modality inversion статья подтверждает наше наблюдение, что фичи clip плохо подходят для image-to-image retrieval. авторы углубляются в проблему, используя текстовую и картиночную инверсию, но практических решений не предлагают. clip behaves like a bag-of-words model cross-modally but not uni-modally исследователи показывают, что фичи clip ведут себя как «мешок слов» только при взаимодействии между доменами картинок и текстов, а внутри одного домена сохраняют структуру. например, «синий куб и красный шар» ≠ «красный шар и синий куб» — то же верно и для изображений. это значит, что проблема может быть не в самих эмбеддингах, а в их междоменном взаимодействии. авторы предлагают обучить линейный слой с negative-текстами поверх текстовой модели — на синтетическом датасете этот подход показывает неплохие результаты. disentangling clip features for enhanced localized understanding в статье предлагают дополнительные лоссы, которые помогают «распутать» фичи clip. авторы дообучают головы поверх текстовой и визуальной частей. метод выглядит специфично, но к некоторым идеям стоит присмотреться. clip-up: a simple and efficient mixture-of-experts clip training recipe with sparse upcycling модель инициализируется из обычного clip, но в каждом втором mlp-блоке заменяют слои на смесь из восьми экспертов, из которых активируются два. для обучения предлагают использовать комбинацию из шести лоссов. архитектура scaling laws in patchification: an image is worth 50,176 tokens and more в статье утверждают, что уменьшение размера патча в трансформерах с 16×16 до 1×1 улучшает качество модели. при этом для моделей, у которых уменьшен размер входного патча, не нужна сложная архитектура головы при адаптации под dense-задачи (например, сегментация и оценка глубины). iformer: integrating convnet and transformer for mobile application в статье описана архитектура со свёртками и аттеншном, заточенная под инференс на iphone. основой служит convnext, который дорабатывают, чтобы сделать сеть более лёгкой. помимо этого, предлагают использовать слои аттешна с одной головой для модуляции карт признаков — было бы интересно сравнить это с более простым и популярным блоком «squeeze and excitation». подборку подготовил ❣ артём конев cv time тематическая подборка статей за февраль: картиночные модели спешим со свежей подборкой интересных статей. в этот раз — о моделях для работы с изображениями. clip остаётся в центре внимания исследователей, но вопросы к его фичам не исчезают. также делимся несколькими работами по архитектуре нейросетей и оптимизации для мобильных устройств. image-to-text cross the gap: exposing the intra-modal misalignment in clip via modality inversion статья подтверждает наше наблюдение, что фичи clip плохо подходят для image-to-image retrieval. авторы углубляются в проблему, используя текстовую и картиночную инверсию, но практических решений не предлагают. clip behaves like a bag-of-words model cross-modally but not uni-modally исследователи показывают, что фичи clip ведут себя как «мешок слов» только при взаимодействии между доменами картинок и текстов, а внутри одного домена сохраняют структуру. например, «синий куб и красный шар» ≠ «красный шар и синий куб» — то же верно и для изображений. это значит, что проблема может быть не в самих эмбеддингах, а в их междоменном взаимодействии. авторы предлагают обучить линейный слой с negative-текстами поверх текстовой модели — на синтетическом датасете этот подход показывает неплохие результаты. disentangling clip features for enhanced localized understanding в статье предлагают дополнительные лоссы, которые помогают «распутать» фичи clip. авторы дообучают головы поверх текстовой и визуальной частей. метод выглядит специфично, но к некоторым идеям стоит присмотреться. clip-up: a simple and efficient mixture-of-experts clip training recipe with sparse upcycling модель инициализируется из обычного clip, но в каждом втором mlp-блоке заменяют слои на смесь из восьми экспертов, из которых активируются два. для обучения предлагают использовать комбинацию из шести лоссов. архитектура scaling laws in patchification: an image is worth 50,176 tokens and more в статье утверждают, что уменьшение размера патча в трансформерах с 16×16 до 1×1 улучшает качество модели. при этом для моделей, у которых уменьшен размер входного патча, не нужна сложная архитектура головы при адаптации под dense-задачи (например, сегментация и оценка глубины). iformer: integrating convnet and transformer for mobile application в статье описана архитектура со свёртками и аттеншном, заточенная под инференс на iphone. основой служит convnext, который дорабатывают, чтобы сделать сеть более лёгкой. помимо этого, предлагают использовать слои аттешна с одной головой для модуляции карт признаков — было бы интересно сравнить это с более простым и популярным блоком «squeeze and excitation». подборку подготовил ❣ артём конев cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-03-11T07:34:20+00:00" href="./posts/76.html">2025-03-11 07:34 UTC</a></div>
      </div>
      <div class="post-body"><strong>Тематическая подборка статей за февраль: картиночные модели</strong><br><br>Спешим со свежей подборкой интересных статей. В этот раз — о моделях для работы с изображениями. CLIP остаётся в центре внимания исследователей, но вопросы к его фичам не исчезают. Также делимся несколькими работами по архитектуре нейросетей и оптимизации для мобильных устройств.<br><br><strong>Image-to-Text</strong><br> <br><a href="https://arxiv.org/abs/2502.04263" rel="nofollow noopener noreferrer"><strong>Cross the Gap: Exposing the Intra-modal Misalignment in CLIP via Modality Inversion</strong></a><br>Статья подтверждает наше наблюдение, что фичи CLIP плохо подходят для image-to-image retrieval. Авторы углубляются в проблему, используя текстовую и картиночную инверсию, но практических решений не предлагают.<br> <br><a href="https://arxiv.org/abs/2502.03566" rel="nofollow noopener noreferrer"><strong>CLIP Behaves like a Bag-of-Words Model Cross-modally but not Uni-modally</strong></a><br>Исследователи показывают, что фичи CLIP ведут себя как «мешок слов» только при взаимодействии между доменами картинок и текстов, а внутри одного домена сохраняют структуру. Например, «синий куб и красный шар» ≠ «красный шар и синий куб» — то же верно и для изображений. Это значит, что проблема может быть не в самих эмбеддингах, а в их междоменном взаимодействии. Авторы предлагают обучить линейный слой с negative-текстами поверх текстовой модели — на синтетическом датасете этот подход показывает неплохие результаты.<br> <br><a href="https://arxiv.org/abs/2502.02977" rel="nofollow noopener noreferrer"><strong>Disentangling CLIP Features for Enhanced Localized Understanding</strong></a><br>В статье предлагают дополнительные лоссы, которые помогают «распутать» фичи CLIP. Авторы дообучают головы поверх текстовой и визуальной частей. Метод выглядит специфично, но к некоторым идеям стоит присмотреться.<br> <br><a href="https://arxiv.org/abs/2502.00965" rel="nofollow noopener noreferrer"><strong>CLIP-UP: A Simple and Efficient Mixture-of-Experts CLIP Training Recipe with Sparse Upcycling</strong></a><br>Модель инициализируется из обычного CLIP, но в каждом втором MLP-блоке заменяют слои на смесь из восьми экспертов, из которых активируются два. Для обучения предлагают использовать комбинацию из шести лоссов.<br> <br><strong>Архитектура</strong><br> <br><a href="https://arxiv.org/abs/2502.03738" rel="nofollow noopener noreferrer"><strong>Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More</strong></a><br>В статье утверждают, что уменьшение размера патча в трансформерах с 16×16 до 1×1 улучшает качество модели. При этом для моделей, у которых уменьшен размер входного патча, не нужна сложная архитектура головы при адаптации под dense-задачи (например, сегментация и оценка глубины).<br> <br><a href="https://arxiv.org/abs/2501.15369" rel="nofollow noopener noreferrer"><strong>iFormer: Integrating ConvNet and Transformer for Mobile Application</strong></a><br>В статье описана архитектура со свёртками и аттеншном, заточенная под инференс на iPhone. Основой служит ConvNeXt, который дорабатывают, чтобы сделать сеть более лёгкой. Помимо этого, предлагают использовать слои аттешна с одной головой для модуляции карт признаков — было бы интересно сравнить это с более простым и популярным блоком «squeeze and excitation».<br><br><em>Подборку подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Артём Конев<br></em><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a></div>
      <div class="actions">
        <span>2 504 просмотров · 19 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/76" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/76.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="75" data-search="эволюция диффузионок: главные вехи последних лет вчера на хабре вышла большая и захватывающая статья исследователя yandex research сергея кастрюлина об истории развития диффузионных моделей. среди прочего в ней рассказано о борьбе каскадной и латентной парадигм, дилемме между свёрточными моделями и трансформерами, дистилляции как решении проблемы медленной генерации и многом другом. в этом посте мы уместили краткий обзор части работ, которые упоминаются в статье, — очень советуем прочитать полную версию. simple diffusion: end-to-end diffusion for high resolution images один из трендов начала 2023 года — противостояние каскадной и латентной диффузии. обе парадигмы ставят задачей уменьшить размерность пространства, в котором происходит генерация объектов, поскольку считается, что генерация в высоком разрешении — слишком сложная задача. в своей работе авторы из google brain показывают, что диффузионную модель можно обучить сразу генерировать изображения в высоком разрешении без усложнений в виде каскадных схем и автокодировщиков. в статье на хабре рассказано, с помощью каких приёмов это было достигнуто. sdxl: improving latent diffusion models for high-resolution image synthesis ещё одна громкая публикация, на этот раз от stability.ai. описанная в ней модель sdxl — первая по-настоящему большая опенсорс-модель с 2,8 миллиарда параметров (что немало и сегодня). механизм, при котором конкатятся текстовые эмбеддинги из нескольких энкодеров, популярен до сих пор. кроме этого, sdxl остаётся сильным бейзлайном по качеству генерации. emu: enhancing image generation models using photogenic needles in a haystack emu — первая text-to-image модель от meta*. авторы показывают, что для sft важнее качество данных, а не их объём. в полной статье можно подробнее прочитать главное о разделении обучения на pretraining и sft. а ещё именно в emu впервые предложили использовать vae, в которых было больше четырёх каналов — сейчас таким уже никого не удивишь. improving image captioning with better use of captions релиз обновлённой dall-e — событие, которое вышло далеко за рамки ml. модель выгодно отличалась на фоне современников. во многом openai удалось достигнуть этого благодаря новому подходу к обучению: 95% пар «картинка — текст» заменили на правильную синтетику. pixart-α: fast training of diffusion transformer for photorealistic text-to-image synthesis трансформерную архитектуру dit для диффузионных моделей предложили ещё в конце 2022 года. в pixart-α её доработали, добавив возможность использовать тексты в качестве условия для генерации. совместив это с изменениями в данных и обучении, авторы добились высоких результатов при минимальных вычислительных затратах. adversarial diffusion distillation статья, которую первоначально захейтили и отклонили рецензенты. именно в ней stability.ai предложила первый метод дистилляции для решения проблемы долгого инференса. его использовали в модели sdxl‑turbo и смогли генерировать изображения, сопоставимые по качеству с генерациями исходной модели, но на порядок быстрее. add до сих пор остаётся популярным методом дистилляции. sana, kolors, flux и другие современные модели вторая часть статьи посвящена обзору более свежих разработок. летом 2024-го вышел масштабный техрепорт kolors — таким китайские исследователи балуют нечасто. в нём они, среди прочего, говорят об использовании glm, мультиязычной генеративной модели, в качестве текстового энкодера. в ноябре того же года nvidia представила модель sana с возможностью без дополнительных super‑resolution‑моделей генерировать изображения в 4к. а в последнее время фокус сместился в сторону закрытых моделей, таких как ideogram, recraft, midjourney и flux, о которых известно не так много. кроме более полного экскурса в эволюцию диффузионок за последние два года, в статье упоминают cv week, бесплатный интенсив шада о диффузионных моделях. о нём у нас был пост с комментариями спикеров — будет полезно, если захочется пробежаться по ключевым тезисам. cv time ___ meta признана экстремистской организацией, а facebook и instagram запрещены на территории рф эволюция диффузионок: главные вехи последних лет вчера на хабре вышла большая и захватывающая статья исследователя yandex research сергея кастрюлина об истории развития диффузионных моделей. среди прочего в ней рассказано о борьбе каскадной и латентной парадигм, дилемме между свёрточными моделями и трансформерами, дистилляции как решении проблемы медленной генерации и многом другом. в этом посте мы уместили краткий обзор части работ, которые упоминаются в статье, — очень советуем прочитать полную версию. simple diffusion: end-to-end diffusion for high resolution images один из трендов начала 2023 года — противостояние каскадной и латентной диффузии. обе парадигмы ставят задачей уменьшить размерность пространства, в котором происходит генерация объектов, поскольку считается, что генерация в высоком разрешении — слишком сложная задача. в своей работе авторы из google brain показывают, что диффузионную модель можно обучить сразу генерировать изображения в высоком разрешении без усложнений в виде каскадных схем и автокодировщиков. в статье на хабре рассказано, с помощью каких приёмов это было достигнуто. sdxl: improving latent diffusion models for high-resolution image synthesis ещё одна громкая публикация, на этот раз от stability.ai . описанная в ней модель sdxl — первая по-настоящему большая опенсорс-модель с 2,8 миллиарда параметров (что немало и сегодня). механизм, при котором конкатятся текстовые эмбеддинги из нескольких энкодеров, популярен до сих пор. кроме этого, sdxl остаётся сильным бейзлайном по качеству генерации. emu: enhancing image generation models using photogenic needles in a haystack emu — первая text-to-image модель от meta*. авторы показывают, что для sft важнее качество данных, а не их объём. в полной статье можно подробнее прочитать главное о разделении обучения на pretraining и sft. а ещё именно в emu впервые предложили использовать vae, в которых было больше четырёх каналов — сейчас таким уже никого не удивишь. improving image captioning with better use of captions релиз обновлённой dall-e — событие, которое вышло далеко за рамки ml. модель выгодно отличалась на фоне современников. во многом openai удалось достигнуть этого благодаря новому подходу к обучению: 95% пар «картинка — текст» заменили на правильную синтетику. pixart-α: fast training of diffusion transformer for photorealistic text-to-image synthesis трансформерную архитектуру dit для диффузионных моделей предложили ещё в конце 2022 года. в pixart-α её доработали, добавив возможность использовать тексты в качестве условия для генерации. совместив это с изменениями в данных и обучении, авторы добились высоких результатов при минимальных вычислительных затратах. adversarial diffusion distillation статья, которую первоначально захейтили и отклонили рецензенты. именно в ней stability.ai предложила первый метод дистилляции для решения проблемы долгого инференса. его использовали в модели sdxl‑turbo и смогли генерировать изображения, сопоставимые по качеству с генерациями исходной модели, но на порядок быстрее. add до сих пор остаётся популярным методом дистилляции. sana , kolors , flux и другие современные модели вторая часть статьи посвящена обзору более свежих разработок. летом 2024-го вышел масштабный техрепорт kolors — таким китайские исследователи балуют нечасто. в нём они, среди прочего, говорят об использовании glm, мультиязычной генеративной модели, в качестве текстового энкодера. в ноябре того же года nvidia представила модель sana с возможностью без дополнительных super‑resolution‑моделей генерировать изображения в 4к. а в последнее время фокус сместился в сторону закрытых моделей, таких как ideogram, recraft, midjourney и flux, о которых известно не так много. кроме более полного экскурса в эволюцию диффузионок за последние два года, в статье упоминают cv week , бесплатный интенсив шада о диффузионных моделях. о нём у нас был пост с комментариями спикеров — будет полезно, если захочется пробежаться по ключевым тезисам. cv time ___ meta признана экстремистской организацией, а facebook и instagram запрещены на территории рф">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-03-05T09:26:51+00:00" href="./posts/75.html">2025-03-05 09:26 UTC</a></div>
      </div>
      <div class="post-body"><strong>Эволюция диффузионок: главные вехи последних лет</strong><br><br>Вчера на Хабре вышла большая и захватывающая <a href="https://habr.com/ru/companies/yandex/articles/886466/" rel="nofollow noopener noreferrer">статья</a> исследователя Yandex Research Сергея Кастрюлина об истории развития диффузионных моделей. Среди прочего в ней рассказано о борьбе каскадной и латентной парадигм, дилемме между свёрточными моделями и трансформерами, дистилляции как решении проблемы медленной генерации и многом другом. В этом посте мы уместили краткий обзор части работ, которые упоминаются в статье, — очень советуем прочитать полную версию. <br><br><a href="https://arxiv.org/abs/2301.11093" rel="nofollow noopener noreferrer"><strong>Simple diffusion: End-to-end diffusion for high resolution images</strong></a><br>Один из трендов начала 2023 года — противостояние каскадной и латентной диффузии. Обе парадигмы ставят задачей уменьшить размерность пространства, в котором происходит генерация объектов, поскольку считается, что генерация в высоком разрешении — слишком сложная задача. В своей работе авторы из Google Brain показывают, что диффузионную модель можно обучить сразу генерировать изображения в высоком разрешении без усложнений в виде каскадных схем и автокодировщиков. В статье на Хабре рассказано, с помощью каких приёмов это было достигнуто. <br><br><a href="https://arxiv.org/abs/2307.01952" rel="nofollow noopener noreferrer"><strong>SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis</strong></a><br>Ещё одна громкая публикация, на этот раз от <a rel="nofollow noopener noreferrer">Stability.ai</a>. Описанная в ней модель  SDXL — первая по-настоящему большая опенсорс-модель с 2,8 миллиарда параметров (что немало и сегодня). Механизм, при котором конкатятся текстовые эмбеддинги из нескольких энкодеров, популярен до сих пор. Кроме этого, SDXL остаётся сильным бейзлайном по качеству генерации.<br><br><a href="https://arxiv.org/abs/2309.15807" rel="nofollow noopener noreferrer"><strong>Emu: Enhancing Image Generation Models Using Photogenic Needles in a Haystack</strong></a><br>EMU — первая text-to-image модель от Meta*. Авторы показывают, что для SFT важнее качество данных, а не их объём. В полной статье можно подробнее прочитать главное о разделении обучения на pretraining и SFT. А ещё именно в EMU впервые предложили использовать VAE, в которых было больше четырёх каналов — сейчас таким уже никого не удивишь.  <br><br><a href="https://arxiv.org/abs/2006.11807" rel="nofollow noopener noreferrer"><strong>Improving Image Captioning with Better Use of Captions</strong></a><br>Релиз обновлённой DALL-E — событие, которое вышло далеко за рамки ML. Модель выгодно отличалась на фоне современников. Во многом OpenAI удалось достигнуть этого благодаря новому подходу к обучению: 95% пар «картинка — текст» заменили на правильную синтетику. <br><br><a href="https://arxiv.org/abs/2310.00426" rel="nofollow noopener noreferrer"><strong>PixArt-α: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis</strong></a><br>Трансформерную архитектуру DiT для диффузионных моделей предложили ещё в конце 2022 года. В PixArt-α её доработали, добавив возможность использовать тексты в качестве условия для генерации. Совместив это с изменениями в данных и обучении, авторы добились высоких результатов при минимальных вычислительных затратах.<br><br><a href="https://arxiv.org/abs/2311.17042" rel="nofollow noopener noreferrer"><strong>Adversarial Diffusion Distillation</strong></a><br>Статья, которую первоначально захейтили и отклонили рецензенты. Именно в ней <a rel="nofollow noopener noreferrer">Stability.ai</a> предложила первый метод дистилляции для решения проблемы долгого инференса. Его использовали в модели SDXL‑Turbo и смогли генерировать изображения, сопоставимые по качеству с генерациями исходной модели, но на порядок быстрее. ADD до сих пор остаётся популярным методом дистилляции.<br><br><a href="https://arxiv.org/abs/2410.10629" rel="nofollow noopener noreferrer"><strong>SANA</strong></a><strong>, </strong><a href="https://github.com/Kwai-Kolors/Kolors/blob/master/imgs/Kolors_paper.pdf" rel="nofollow noopener noreferrer"><strong>Kolors</strong></a><strong>, </strong><a href="https://www.flux.ai/p" rel="nofollow noopener noreferrer"><strong>FLUX </strong></a><strong>и другие современные модели<br></strong>Вторая часть статьи посвящена обзору более свежих разработок. Летом 2024-го вышел масштабный техрепорт Kolors — таким китайские исследователи балуют нечасто. В нём они, среди прочего, говорят об использовании GLM, мультиязычной генеративной модели, в качестве текстового энкодера. В ноябре того же года Nvidia представила модель SANA с возможностью без дополнительных Super‑Resolution‑моделей генерировать изображения в 4К. А в последнее время фокус сместился в сторону закрытых моделей, таких как Ideogram, Recraft, MidJourney и FLUX, о которых известно не так много.<br><br>Кроме более полного экскурса в эволюцию диффузионок за последние два года, в статье упоминают <a href="https://shad.yandex.ru/cvweek#program" rel="nofollow noopener noreferrer">CV Week</a>, бесплатный интенсив ШАДа о диффузионных моделях. О нём у нас был <a href="https://t.me/timeforcv/65" rel="nofollow noopener noreferrer">пост</a> с комментариями спикеров — будет полезно, если захочется пробежаться по ключевым тезисам.<br><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><br><br><em>___<br>Meta признана экстремистской организацией, а Facebook и Instagram запрещены на территории РФ</em><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/75_480.webp" srcset="../assets/media/thumbs/75_480.webp 480w, ../assets/media/75.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="75" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 400 просмотров · 31 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/75" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/75.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="74" data-search="insightedit: towards better instruction following for image editing авторы сегодняшней статьи рассказывают о том, как построили модель insightedit для трёх задач редактирования изображений — добавления, удаления и замены объекта. для этого: — собрали автоматический пайплайн генерации данных для обучения задаче редактирования изображений по промпту; — сгенерировали с помощью этого пайплайна датасет advancededit; — на собранном датасете обучили модель insightedit. сначала авторы сгенерировали caption для исходного набора картинок: простые описания (вида «голубая футболка») и сложные (вида «футболка на мужчине, похожая на хлопковую, содержит голубые элементы»). потом использовали llm, чтобы создать список объектов, сгенерировали каждому из них маску с помощью groundedsam и отфильтровали маски с низким скором уверенности. и, наконец, составили простые инструкции, как и что заменить, и с помощью инпейнтинга сгенерировали отредактированное изображение. а чтобы модель не переобучилась, добавили в датасет перефразированные llm формулировки. такой автоматизированный пайплайн использовали для создания датасета advancededit. в качестве исходных данных взяли датасет pixels, который содержит более 1 миллиона фотографий высокого разрешения. модель insideedit состоит из трёх модулей: понимания, объединения и генерации. модуль понимания использует mllm, чтобы понять по промпту, что нужно редактировать. модуль объединения улучшает взаимодействие промпта и исходного изображения. а модуль генерации — создаёт целевое изображение: редактирует исходное, обуславливаясь векторами признаков из предыдущих модулей. для оценки качества обученной модели авторы сравнивают clipscore объекта редактирования с эмбеддингом целевого объекта. а чтобы убедиться, что модель не изменила остальные объекты, применяют psnr, ssim и lpips по фону изображения. для более точной оценки эффектов редактирования и соответствия человеческим предпочтениям — используют viescore. обзор подготовил ❣ александр шишеня cv time insightedit: towards better instruction following for image editing авторы сегодняшней статьи рассказывают о том, как построили модель insightedit для трёх задач редактирования изображений — добавления, удаления и замены объекта. для этого: — собрали автоматический пайплайн генерации данных для обучения задаче редактирования изображений по промпту; — сгенерировали с помощью этого пайплайна датасет advancededit; — на собранном датасете обучили модель insightedit. сначала авторы сгенерировали caption для исходного набора картинок: простые описания (вида «голубая футболка») и сложные (вида «футболка на мужчине, похожая на хлопковую, содержит голубые элементы»). потом использовали llm, чтобы создать список объектов, сгенерировали каждому из них маску с помощью groundedsam и отфильтровали маски с низким скором уверенности. и, наконец, составили простые инструкции, как и что заменить, и с помощью инпейнтинга сгенерировали отредактированное изображение. а чтобы модель не переобучилась, добавили в датасет перефразированные llm формулировки. такой автоматизированный пайплайн использовали для создания датасета advancededit. в качестве исходных данных взяли датасет pixels, который содержит более 1 миллиона фотографий высокого разрешения. модель insideedit состоит из трёх модулей: понимания, объединения и генерации. модуль понимания использует mllm, чтобы понять по промпту, что нужно редактировать. модуль объединения улучшает взаимодействие промпта и исходного изображения. а модуль генерации — создаёт целевое изображение: редактирует исходное, обуславливаясь векторами признаков из предыдущих модулей. для оценки качества обученной модели авторы сравнивают clipscore объекта редактирования с эмбеддингом целевого объекта. а чтобы убедиться, что модель не изменила остальные объекты, применяют psnr, ssim и lpips по фону изображения. для более точной оценки эффектов редактирования и соответствия человеческим предпочтениям — используют viescore . обзор подготовил ❣ александр шишеня cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-02-26T07:35:45+00:00" href="./posts/74.html">2025-02-26 07:35 UTC</a></div>
      </div>
      <div class="post-body"><strong>InsightEdit: Towards Better Instruction Following for Image Editing</strong><br><br>Авторы сегодняшней <a href="https://arxiv.org/abs/2411.17323" rel="nofollow noopener noreferrer">статьи</a> рассказывают о том, как построили модель InsightEdit для трёх задач редактирования изображений — добавления, удаления и замены объекта. Для этого:<br><br>— собрали автоматический пайплайн генерации данных для обучения задаче редактирования изображений по промпту; <br>— сгенерировали с помощью этого пайплайна датасет AdvancedEdit; <br>— на собранном датасете обучили модель InsightEdit.<br><br>Сначала авторы сгенерировали caption для исходного набора картинок: простые описания (вида «голубая футболка») и сложные (вида «футболка на мужчине, похожая на хлопковую, содержит голубые элементы»). Потом использовали LLM, чтобы создать список объектов, сгенерировали каждому из них маску с помощью GroundedSAM и отфильтровали маски с низким скором уверенности. И, наконец, составили простые инструкции, как и что заменить, и с помощью инпейнтинга сгенерировали отредактированное изображение. А чтобы модель не переобучилась, добавили в датасет перефразированные LLM формулировки. <br><br>Такой автоматизированный пайплайн использовали для создания датасета AdvancedEdit. В качестве исходных данных взяли датасет Pixels, который содержит более 1 миллиона фотографий высокого разрешения. <br><br>Модель InsideEdit состоит из трёх модулей: понимания, объединения и генерации. Модуль понимания использует MLLM, чтобы понять по промпту, что нужно редактировать. Модуль объединения улучшает взаимодействие промпта и исходного изображения. А модуль генерации — создаёт целевое изображение: редактирует исходное, обуславливаясь векторами признаков из предыдущих модулей.<br><br>Для оценки качества обученной модели авторы сравнивают CLIPScore объекта редактирования с эмбеддингом целевого объекта. А чтобы убедиться, что модель не изменила остальные объекты, применяют PSNR, SSIM и LPIPS по фону изображения. Для более точной оценки эффектов редактирования и соответствия человеческим предпочтениям — используют <a href="https://arxiv.org/abs/2312.14867" rel="nofollow noopener noreferrer">VIEScore</a>.<br><br><em>Обзор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Александр Шишеня</em><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/74_480.webp" srcset="../assets/media/thumbs/74_480.webp 480w, ../assets/media/74.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="74" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 977 просмотров · 17 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/74" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/74.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="72" data-search="тематическая подборка статей: генерация спешим поделиться очередной подборкой публикаций — на этот раз о генерации. обучение meissonic: revitalizing masked generative transformers for efficient high-resolution text-to-image synthesis авторы обучили 1b-трансформер для генерации изображений, используя vq-vae и masked image modelling. по словам инженеров, модель достигла уровня sd xl, что они считают успехом. improving long-text alignment for text-to-image diffusion models исследователи пробуют модифицировать stable diffusion для работы с длинными текстами. для этого их нарезают на сегменты, которые по отдельности кодируют через clip. также проанализировали clip-реворд, разделив его на text-relevant и text-irrelevant части (последние стремятся сделать картинки более комплексными). дополнительно в статье показано, что t5-энкодер можно на файнтюне добавлять к clip-предобученной модели. fluid: scaling autoregressive text-to-image generative models with continuous tokens в статье попарно сравнивают четыре подхода к генерации изображений трансформерами: авторегрессивная генерация vs генерация токенов на рандомных позициях, а также предсказание continuous-токенов vs предсказание дискретных токенов из словаря. побеждает предсказание continuous-токенов на случайных позициях, но авторам не удалось выровнять качество continuous и дискретных автоэнкодеров — эта часть вызывает вопросы. авторы отдельно замечают, что лосс на валидации хорошо коррелирует с метриками. об этом упоминалось и в статье о movie gen (пункт 3.6.1, абзац «correlation between validation loss and human evaluation»). бенчмарки kitten: a knowledge-intensive evaluation of image generation on visual entities в работе создали корзину концептов из «википедии» с изображениями-референсами и замерили на ней imagen, flux, stable diffusion и прочие модели. выяснилось, что те из них, что на вход принимают картинку-референс (instructimagen, dreambooth), лучше воспроизводят концепты, но часто ценой худшего следования промпту. также авторы сравнили разметку людьми с автооценкой через clip/dino, и ранжирование моделей поменялось (корреляция 0,3–0,5, что указывает на возможность подобрать лучшие модели). видео koala-36m: a large-scale video dataset improving consistency between fine-grained conditions and video content исследователи собрали датасет для обучения видеодиффузионной модели: описали процесс video splitting, схему кэпшнинга и фильтрации. в работе по набору классификаторов учатся предсказывать, насколько видео подходит для добавления в обучающее множество, а также кондишнят генерацию на классификаторы по видео. movie gen: a cast of media foundation models большая статья, в которой meta адаптирует архитектуру llama3 с 30b параметров для генерации видео. интересные моменты: — используют три текстовых энкодера: ul2, byt5 и clip, которые во время обучения считаются на лету; — в byt5 подают только текст, который нужно отрисовать (предлагают помещать его в кавычки в промпте); — тюнят llama3 для переформулировки промптов, приближая их к трейну; — добавляют дополнительный лосс для борьбы с точками-артефактами при обучении vae; — обучение начинается с изображений разрешением 256px; — получают финальную модель путём усреднения весов моделей, дообученных на разных датасетах и гиперпараметрах. другое on the effectiveness of dataset alignment for fake image detection авторы описывают хитрый способ обучить классификатор синтетических картинок. для этого реальные изображения (неважно какие) кодируются и декодируются через vae — и дальше к ним относятся, как к синтетическим. таким образом получается датасет пар картинок, которые отличаются только артефактами vae — на нём можно обучить детектор синтетических картинок. но есть нюансы: пайплайн может быть чувствителен к постобработке картинок, и может плохо переноситься на модели с сильно отличающимися vae. обзор подготовил ❣ артём конев cv time ___ meta признана экстремистской организацией, а facebook и instagram запрещены на территории рф тематическая подборка статей: генерация спешим поделиться очередной подборкой публикаций — на этот раз о генерации. обучение meissonic: revitalizing masked generative transformers for efficient high-resolution text-to-image synthesis авторы обучили 1b-трансформер для генерации изображений, используя vq-vae и masked image modelling. по словам инженеров, модель достигла уровня sd xl, что они считают успехом. improving long-text alignment for text-to-image diffusion models исследователи пробуют модифицировать stable diffusion для работы с длинными текстами. для этого их нарезают на сегменты, которые по отдельности кодируют через clip. также проанализировали clip-реворд, разделив его на text-relevant и text-irrelevant части (последние стремятся сделать картинки более комплексными). дополнительно в статье показано, что t5-энкодер можно на файнтюне добавлять к clip-предобученной модели. fluid: scaling autoregressive text-to-image generative models with continuous tokens в статье попарно сравнивают четыре подхода к генерации изображений трансформерами: авторегрессивная генерация vs генерация токенов на рандомных позициях, а также предсказание continuous-токенов vs предсказание дискретных токенов из словаря. побеждает предсказание continuous-токенов на случайных позициях, но авторам не удалось выровнять качество continuous и дискретных автоэнкодеров — эта часть вызывает вопросы. авторы отдельно замечают, что лосс на валидации хорошо коррелирует с метриками. об этом упоминалось и в статье о movie gen (пункт 3.6.1, абзац «correlation between validation loss and human evaluation»). бенчмарки kitten: a knowledge-intensive evaluation of image generation on visual entities в работе создали корзину концептов из «википедии» с изображениями-референсами и замерили на ней imagen, flux, stable diffusion и прочие модели. выяснилось, что те из них, что на вход принимают картинку-референс (instructimagen, dreambooth), лучше воспроизводят концепты, но часто ценой худшего следования промпту. также авторы сравнили разметку людьми с автооценкой через clip/dino, и ранжирование моделей поменялось (корреляция 0,3–0,5, что указывает на возможность подобрать лучшие модели). видео koala-36m: a large-scale video dataset improving consistency between fine-grained conditions and video content исследователи собрали датасет для обучения видеодиффузионной модели: описали процесс video splitting, схему кэпшнинга и фильтрации. в работе по набору классификаторов учатся предсказывать, насколько видео подходит для добавления в обучающее множество, а также кондишнят генерацию на классификаторы по видео. movie gen: a cast of media foundation models большая статья, в которой meta адаптирует архитектуру llama3 с 30b параметров для генерации видео. интересные моменты: — используют три текстовых энкодера: ul2, byt5 и clip, которые во время обучения считаются на лету; — в byt5 подают только текст, который нужно отрисовать (предлагают помещать его в кавычки в промпте); — тюнят llama3 для переформулировки промптов, приближая их к трейну; — добавляют дополнительный лосс для борьбы с точками-артефактами при обучении vae; — обучение начинается с изображений разрешением 256px; — получают финальную модель путём усреднения весов моделей, дообученных на разных датасетах и гиперпараметрах. другое on the effectiveness of dataset alignment for fake image detection авторы описывают хитрый способ обучить классификатор синтетических картинок. для этого реальные изображения (неважно какие) кодируются и декодируются через vae — и дальше к ним относятся, как к синтетическим. таким образом получается датасет пар картинок, которые отличаются только артефактами vae — на нём можно обучить детектор синтетических картинок. но есть нюансы: пайплайн может быть чувствителен к постобработке картинок, и может плохо переноситься на модели с сильно отличающимися vae. обзор подготовил ❣ артём конев cv time ___ meta признана экстремистской организацией, а facebook и instagram запрещены на территории рф">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-02-19T08:04:07+00:00" href="./posts/72.html">2025-02-19 08:04 UTC</a></div>
      </div>
      <div class="post-body"><strong>Тематическая подборка статей: генерация<br></strong><br>Спешим поделиться очередной подборкой публикаций — на этот раз о генерации. <br><br><strong>Обучение</strong><br><br><a href="https://arxiv.org/abs/2410.08261" rel="nofollow noopener noreferrer"><strong>Meissonic: Revitalizing Masked Generative Transformers for Efficient High-Resolution Text-to-Image Synthesis</strong></a><br>Авторы обучили 1B-трансформер для генерации изображений, используя VQ-VAE и masked image modelling. По словам инженеров, модель достигла уровня SD XL, что они считают успехом.<br><br><a href="https://arxiv.org/abs/2410.11817" rel="nofollow noopener noreferrer"><strong>Improving Long-Text Alignment for Text-to-Image Diffusion Models</strong></a><br>Исследователи пробуют модифицировать Stable Diffusion для работы с длинными текстами. Для этого их нарезают на сегменты, которые по отдельности кодируют через CLIP. Также проанализировали CLIP-реворд, разделив его на text-relevant и text-irrelevant части (последние стремятся сделать картинки более комплексными). Дополнительно в статье показано, что T5-энкодер можно на файнтюне добавлять к CLIP-предобученной модели.<br><br><a href="https://arxiv.org/abs/2410.13863" rel="nofollow noopener noreferrer"><strong>Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens</strong></a><strong><br></strong>В статье попарно сравнивают четыре подхода к генерации изображений трансформерами: авторегрессивная генерация vs генерация токенов на рандомных позициях, а также предсказание continuous-токенов vs предсказание дискретных токенов из словаря. <br><br>Побеждает предсказание continuous-токенов на случайных позициях, но авторам не удалось выровнять качество continuous и дискретных автоэнкодеров — эта часть вызывает вопросы. Авторы отдельно замечают, что лосс на валидации хорошо коррелирует с метриками. Об этом упоминалось и в статье о Movie Gen (пункт 3.6.1, абзац «Correlation between validation loss and human evaluation»).<br><br><strong>Бенчмарки<br></strong><br><a href="https://arxiv.org/abs/2410.11824" rel="nofollow noopener noreferrer"><strong>KITTEN: A Knowledge-Intensive Evaluation of Image Generation on Visual Entities</strong></a><br>В работе создали корзину концептов из «Википедии» с изображениями-референсами и замерили на ней Imagen, Flux, Stable Diffusion и прочие модели. Выяснилось, что те из них, что на вход принимают картинку-референс (InstructImagen, DreamBooth), лучше воспроизводят концепты, но часто ценой худшего следования промпту. Также авторы сравнили разметку людьми с автооценкой через CLIP/DINO, и ранжирование моделей поменялось (корреляция 0,3–0,5, что указывает на возможность подобрать лучшие модели).<br><br><strong>Видео</strong><br><br><a href="https://arxiv.org/abs/2410.08260" rel="nofollow noopener noreferrer"><strong>Koala-36M: A Large-scale Video Dataset Improving Consistency between Fine-grained Conditions and Video Content</strong></a><br>Исследователи собрали датасет для обучения видеодиффузионной модели: описали процесс video splitting, схему кэпшнинга и фильтрации. В работе по набору классификаторов учатся предсказывать, насколько видео подходит для добавления в обучающее множество, а также кондишнят генерацию на классификаторы по видео.<br><br><a href="https://arxiv.org/abs/2410.13720" rel="nofollow noopener noreferrer"><strong>Movie Gen: A Cast of Media Foundation Models</strong></a><br>Большая статья, в которой Meta адаптирует архитектуру Llama3 с 30B параметров для генерации видео. Интересные моменты:<br><br>— используют три текстовых энкодера: UL2, ByT5 и CLIP, которые во время обучения считаются на лету;<br>— в ByT5 подают только текст, который нужно отрисовать (предлагают помещать его в кавычки в промпте);<br>— тюнят Llama3 для переформулировки промптов, приближая их к трейну;<br>— добавляют дополнительный лосс для борьбы с точками-артефактами при обучении VAE;<br>— обучение начинается с изображений разрешением 256px;<br>— получают финальную модель путём усреднения весов моделей, дообученных на разных датасетах и гиперпараметрах.<br><br><strong>Другое</strong><br><br><a href="https://arxiv.org/abs/2410.11835" rel="nofollow noopener noreferrer"><strong>On the Effectiveness of Dataset Alignment for Fake Image Detection</strong></a><br>Авторы описывают хитрый способ обучить классификатор синтетических картинок. Для этого реальные изображения (неважно какие) кодируются и декодируются через VAE — и дальше к ним относятся, как к синтетическим.<br><br>Таким образом получается датасет пар картинок, которые отличаются только артефактами VAE — на нём можно обучить детектор синтетических картинок. Но есть нюансы: пайплайн может быть чувствителен к постобработке картинок, и может плохо переноситься на модели с сильно отличающимися VAE.<br><br>Обзор подготовил <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Артём Конев<br><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><br>___<br><em>Meta признана экстремистской организацией, а Facebook и Instagram запрещены на территории РФ</em></div>
      <div class="actions">
        <span>2 348 просмотров · 18 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/72" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/72.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="65" data-search="недавно инженеры из яндекса вместе со школой анализа данных провели открытый онлайн-интенсив по компьютерному зрению и рассказали о генеративных диффузионных моделях. получилась крепкая база для ml`щика — с понятными видеолекциями и практическими разборами. делимся этими материалами с вами — на лендинге они удобно сгруппированы по темам. а в карточках наши спикеры рассказывают, чем вам может быть полезна каждая из освещённых тем. приятного чтения и увлекательного просмотра! недавно инженеры из яндекса вместе со школой анализа данных провели открытый онлайн-интенсив по компьютерному зрению и рассказали о генеративных диффузионных моделях. получилась крепкая база для ml`щика — с понятными видеолекциями и практическими разборами. делимся этими материалами с вами — на лендинге они удобно сгруппированы по темам. а в карточках наши спикеры рассказывают, чем вам может быть полезна каждая из освещённых тем. приятного чтения и увлекательного просмотра!">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-02-11T07:35:29+00:00" href="./posts/65.html">2025-02-11 07:35 UTC</a></div>
      </div>
      <div class="post-body">Недавно инженеры из Яндекса вместе со Школой анализа данных провели открытый онлайн-интенсив по компьютерному зрению и рассказали о генеративных диффузионных моделях.<br><br>Получилась крепкая база для ML`щика — с понятными видеолекциями и практическими разборами. <br><br>Делимся этими материалами с вами — <a href="https://shad.yandex.ru/cvweek#program." rel="nofollow noopener noreferrer">на лендинге</a> они удобно сгруппированы по темам. А в карточках наши спикеры рассказывают, чем вам может быть полезна каждая из освещённых тем. <br><br>Приятного чтения и увлекательного просмотра!<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/65_480.webp" srcset="../assets/media/thumbs/65_480.webp 480w, ../assets/media/65.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="65" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/66_480.webp" srcset="../assets/media/thumbs/66_480.webp 480w, ../assets/media/66.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="65" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/67_480.webp" srcset="../assets/media/thumbs/67_480.webp 480w, ../assets/media/67.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="65" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/68_480.webp" srcset="../assets/media/thumbs/68_480.webp 480w, ../assets/media/68.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="65" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/69_480.webp" srcset="../assets/media/thumbs/69_480.webp 480w, ../assets/media/69.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="65" data-image-index="4" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/70_480.webp" srcset="../assets/media/thumbs/70_480.webp 480w, ../assets/media/70.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="65" data-image-index="5" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/71_480.webp" srcset="../assets/media/thumbs/71_480.webp 480w, ../assets/media/71.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="65" data-image-index="6" /></div></div>
      <div class="actions">
        <span>7 278 просмотров · 26 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/65" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/65.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="64" data-search="unireal: универсальная модель для генерации и редактирования изображений разбираем статью unireal от исследователей из университета гонконга. редактирование изображений — обширная область, в которой есть разнообразные подходы, в частности, известные controlnet и instructpix2pix. однако в случае с unireal авторы хотели создать универсальную модель, способную из коробки решать разные типы задач. на тизерной странице есть примеры её работы. модель может изменить фон исходной картинки, убрать или заменить изображение, добавить новые объекты, поменять стиль, создать композицию из объектов. архитектура модель построена на диффузионном трансформере с full attention. архитектура включает следующие компоненты: — энкодер t5 для обработки текстовых токенов; — vae-энкодеры для изображений; — специальные токены для работы с несколькими изображениями (например, img1 для входного изображения и res1 для результирующего). картинки могут выполнять разную роль: быть фоновым изображением (canvas image), давать сигнал, вроде указания границ или глубины (control image), или просто участвовать в качестве объекта на сцене (asset image). для каждой категории изображений есть обучаемые токены (learnable category embeddings). они добавляются вместе с картинкой, как промпт. авторы используют обучаемые контекстные промпты с несколькими сценариями: реалистичными, синтетическими, статическими, динамическими, а также с референсным объектом. данные качественных датасетов для редактирования изображений довольно много, например: instructpix2pix, ultraedit, vton-hd. но все же их оказалось недостаточно, поэтому исследователи добавили этап обучения на видеоданных. использовали два типа предобучения: — с помощью видеоклипов, из которых случайным образом выбирались два кадра, а также добавлялись описания происходящего в клипе. для генерации синтетических описаний применяли модель gpt-4 mini. — генерация описаний изображений с привязкой к границам объектов (bounding boxes) с помощью vlm kosmos-2. эти границы комбинировались с segment anything model (sam) для получения масок. так создавалась синтетическая разметка видео для задач вставки объектов и заполнения отсутствующих частей изображения (inpainting). модель предобучалась на этой смеси: сначала на видеоданных, затем на публичных датасетах. исследователи делают акцент на том, что для финального результата были важны все компоненты. результаты сравнение на бенчмарках emu edit и magicbrush в задачах редактирования изображений показало, что unireal успешно справляется со сложными задачами, такими как добавление и удаление объектов, в то время как базовые модели допускают в них ошибки. для генерации референсных объектов на фоне модель сравнивается с textual inversion, dreambooth, blip-diffusion и другими. не во всех случаях она превосходит конкурентов по метрикам, но показывает хорошие результаты в sbs-замерах. сейчас модель неплохо работает с двумя-тремя изображениями, но для генерации на десяти и более изображениях требуется больше данных и доработка архитектуры. обзор подготовил ❣ денис кузнеделев cv time unireal: универсальная модель для генерации и редактирования изображений разбираем статью unireal от исследователей из университета гонконга. редактирование изображений — обширная область, в которой есть разнообразные подходы, в частности, известные controlnet и instructpix2pix. однако в случае с unireal авторы хотели создать универсальную модель, способную из коробки решать разные типы задач. на тизерной странице есть примеры её работы. модель может изменить фон исходной картинки, убрать или заменить изображение, добавить новые объекты, поменять стиль, создать композицию из объектов. архитектура модель построена на диффузионном трансформере с full attention. архитектура включает следующие компоненты: — энкодер t5 для обработки текстовых токенов; — vae-энкодеры для изображений; — специальные токены для работы с несколькими изображениями (например, img1 для входного изображения и res1 для результирующего). картинки могут выполнять разную роль: быть фоновым изображением (canvas image), давать сигнал, вроде указания границ или глубины (control image), или просто участвовать в качестве объекта на сцене (asset image). для каждой категории изображений есть обучаемые токены (learnable category embeddings). они добавляются вместе с картинкой, как промпт. авторы используют обучаемые контекстные промпты с несколькими сценариями: реалистичными, синтетическими, статическими, динамическими, а также с референсным объектом. данные качественных датасетов для редактирования изображений довольно много, например: instructpix2pix, ultraedit, vton-hd. но все же их оказалось недостаточно, поэтому исследователи добавили этап обучения на видеоданных. использовали два типа предобучения: — с помощью видеоклипов, из которых случайным образом выбирались два кадра, а также добавлялись описания происходящего в клипе. для генерации синтетических описаний применяли модель gpt-4 mini. — генерация описаний изображений с привязкой к границам объектов (bounding boxes) с помощью vlm kosmos-2. эти границы комбинировались с segment anything model (sam) для получения масок. так создавалась синтетическая разметка видео для задач вставки объектов и заполнения отсутствующих частей изображения (inpainting). модель предобучалась на этой смеси: сначала на видеоданных, затем на публичных датасетах. исследователи делают акцент на том, что для финального результата были важны все компоненты. результаты сравнение на бенчмарках emu edit и magicbrush в задачах редактирования изображений показало, что unireal успешно справляется со сложными задачами, такими как добавление и удаление объектов, в то время как базовые модели допускают в них ошибки. для генерации референсных объектов на фоне модель сравнивается с textual inversion, dreambooth, blip-diffusion и другими. не во всех случаях она превосходит конкурентов по метрикам, но показывает хорошие результаты в sbs-замерах. сейчас модель неплохо работает с двумя-тремя изображениями, но для генерации на десяти и более изображениях требуется больше данных и доработка архитектуры. обзор подготовил ❣ денис кузнеделев cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-02-07T08:17:55+00:00" href="./posts/64.html">2025-02-07 08:17 UTC</a></div>
      </div>
      <div class="post-body"><strong>UniReal: универсальная модель для генерации и редактирования изображений</strong><br><br>Разбираем <a href="https://arxiv.org/abs/2412.07774" rel="nofollow noopener noreferrer">статью UniReal</a> от исследователей из университета Гонконга. Редактирование изображений — обширная область, в которой есть разнообразные подходы, в частности, известные ControlNet и InstructPix2Pix. Однако в случае с UniReal авторы хотели создать универсальную модель, способную из коробки решать разные типы задач. <br><br>На <a href="https://xavierchen34.github.io/UniReal-Page/" rel="nofollow noopener noreferrer">тизерной странице</a> есть примеры её работы. Модель может изменить фон исходной картинки, убрать или заменить изображение, добавить новые объекты, поменять стиль, создать композицию из объектов.<br><br><strong>Архитектура</strong><br><br>Модель построена на диффузионном трансформере с Full Attention. Архитектура включает следующие компоненты:<br><br>— энкодер T5 для обработки текстовых токенов;<br>— VAE-энкодеры для изображений;<br>— специальные токены для работы с несколькими изображениями (например, IMG1 для входного изображения и RES1 для результирующего). <br><br>Картинки могут выполнять разную роль: быть фоновым изображением (canvas image), давать сигнал, вроде указания границ или глубины (control image), или просто участвовать в качестве объекта на сцене (asset image). Для каждой категории изображений есть обучаемые токены (learnable category embeddings). Они добавляются вместе с картинкой, как промпт. <br><br>Авторы используют обучаемые контекстные промпты с несколькими сценариями: реалистичными, синтетическими, статическими, динамическими, а также с референсным объектом. <br><br><strong>Данные</strong><br><br>Качественных датасетов для редактирования изображений довольно много, например: InstructPix2Pix, UltraEdit, VTON-HD. Но все же их оказалось недостаточно, поэтому исследователи добавили этап обучения на видеоданных. Использовали два типа предобучения:<br><br>— С помощью видеоклипов, из которых случайным образом выбирались два кадра, а также добавлялись описания происходящего в клипе. Для генерации синтетических описаний применяли модель GPT-4 mini.<br><br>— Генерация описаний изображений с привязкой к границам объектов (bounding boxes) с помощью VLM Kosmos-2. Эти границы комбинировались с Segment Anything Model (SAM) для получения масок. Так создавалась синтетическая разметка видео для задач вставки объектов и заполнения отсутствующих частей изображения (inpainting).<br><br>Модель предобучалась на этой смеси: сначала на видеоданных, затем на публичных датасетах. Исследователи делают акцент на том, что для финального результата были важны все компоненты.<br><br><strong>Результаты</strong><br><br>Сравнение на бенчмарках EMU Edit и MagicBrush в задачах редактирования изображений показало, что UniReal успешно справляется со сложными задачами, такими как добавление и удаление объектов, в то время как базовые модели допускают в них ошибки.<br><br>Для генерации референсных объектов на фоне модель сравнивается с Textual Inversion, DreamBooth, BLIP-Diffusion и другими. Не во всех случаях она превосходит конкурентов по метрикам, но показывает хорошие результаты в SBS-замерах. <br><br>Сейчас модель неплохо работает с двумя-тремя изображениями, но для генерации на десяти и более изображениях требуется больше данных и доработка архитектуры.<br><br><em>Обзор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Денис Кузнеделев</em><br><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/64_480.webp" srcset="../assets/media/thumbs/64_480.webp 480w, ../assets/media/64.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="64" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 481 просмотров · 14 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/64" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/64.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="60" data-search="личные итоги года инженеров яндекса — роман исаченко продолжаем серию постов от ml&#x27;щиков из яндекса, в которых они подводят профессиональные итоги прошедшего года и строят планы на будущий. сегодня на очереди — руководитель подгруппы дискриминативного анализа изображений роман исаченко. он рассказал о личных достижениях и поделился взглядом на тренды в ml. больше карточек от инженеров — по хештэгу #yamlpeople. личные итоги года инженеров яндекса — роман исаченко продолжаем серию постов от ml&amp;#x27;щиков из яндекса, в которых они подводят профессиональные итоги прошедшего года и строят планы на будущий. сегодня на очереди — руководитель подгруппы дискриминативного анализа изображений роман исаченко. он рассказал о личных достижениях и поделился взглядом на тренды в ml. больше карточек от инженеров — по хештэгу #yamlpeople.">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-02-04T08:05:41+00:00" href="./posts/60.html">2025-02-04 08:05 UTC</a></div>
      </div>
      <div class="post-body"><strong>Личные итоги года инженеров Яндекса — Роман Исаченко</strong><br><br>Продолжаем серию постов от ML&#x27;щиков из Яндекса, в которых они подводят профессиональные итоги прошедшего года и строят планы на будущий. <br><br>Сегодня на очереди — руководитель подгруппы дискриминативного анализа изображений Роман Исаченко. Он рассказал о личных достижениях и поделился взглядом на тренды в ML.<br><br>Больше карточек от инженеров — по хештэгу #YaMLpeople.<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/60_480.webp" srcset="../assets/media/thumbs/60_480.webp 480w, ../assets/media/60.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="60" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/61_480.webp" srcset="../assets/media/thumbs/61_480.webp 480w, ../assets/media/61.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="60" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/62_480.webp" srcset="../assets/media/thumbs/62_480.webp 480w, ../assets/media/62.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="60" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/63_480.webp" srcset="../assets/media/thumbs/63_480.webp 480w, ../assets/media/63.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="60" data-image-index="3" /></div></div>
      <div class="actions">
        <span>2 043 просмотров · 42 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/60" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/60.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="59" data-search="опыт модели aquila-vl-2b: почему не всегда стоит доверять бенчмаркам сегодня разберём работу infinity-mm и описанную в ней модель aquila-vl-2b. эта маленькая vlm с двумя миллиардами параметров интересна тем, что смогла обойти в своём классе qwen и internvl, которые редко уступают первые позиции лидербордов. расскажем, как в топе mmbench оказалась неизвестная модель и почему иногда бенчмарки могут искажать реальную ситуацию. работа представляет собой репорт о проведённом эксперименте. материал не содержит наукоёмких изменений в пайплайне обучения или архитектуре модели. авторы сосредоточены на теме данных и отвечают на вопрос: как при ограниченных ресурсах стать sota vlm, пусть даже не в самом популярном классе маленьких моделек. один из ключевых тезисов: при обучении vlm имеет смысл масштабировать объём sft-данных. с этой целью авторы собрали всё доступное из опенсорса и получили датасет из нескольких десятков миллионов инстрактов, а также сгенерировали в дополнение небольшую часть синтетики. все данные фильтровались, проходили дедупликацию и проверку на разнообразие. итоговый мультимодальный датасет — это и есть infinity-mm из названия статьи. из-за ограничения в вычислительных ресурсах исследователи использовали для генерации и чистки датасета опенсорсные модели, в частности активно прибегали к помощи qwen. пайплайн для генерации синтетических данных выглядит следующим образом: — собирается база изображений, их размечают с помощью опенсорс-модели, которая тегирует объекты на картинке. на основе тегов формируется дерево типов задач, для которых целесообразно создавать инстракты. — опенсорсными моделями (преимущественно minicpm и qwen) генерируют инстракт по картинке, тегу и тематике. происходит автофильтрация через эту же модель (ей дают сгенерированный инстракт и спрашивают, насколько он валиден). — затем получают ответ по синтетическому инстракту — та же модель снова отвечает на вопрос, который сама придумала. — следующий шаг — фильтрации ответа. тут интересное решение: опенсорсная модель считает лосс по полученной паре и, если он высокий, пример исключается. так исследователи автоматически отфильтровали 5% самых «шумных» данных. этих несложных манипуляций хватило, чтобы обогнать модели, которыми авторы генерировали и фильтровали свои данные. скорее всего, так произошло, потому что синтетику целенаправленно собирали под конкретный бенчмарк (mmbench). и в таком случае модель может непредсказуемо вести себя на других задачах. можно сделать вывод, что бенчмарки лучше использовать исключительно как «градусник», чтобы следить за изменениями в области. а вот для оценки эффективности моделей надёжнее ориентироваться на sbs-замеры (side-by-side), которые позволяют проводить прямое сравнение в реальных условиях. а как вы оцениваете опыт aquila-vl-2b и доверяете ли ещё бенчмаркам? обзор подготовил ❣ алексей григорьев cv time опыт модели aquila-vl-2b: почему не всегда стоит доверять бенчмаркам сегодня разберём работу infinity-mm и описанную в ней модель aquila-vl-2b. эта маленькая vlm с двумя миллиардами параметров интересна тем, что смогла обойти в своём классе qwen и internvl, которые редко уступают первые позиции лидербордов. расскажем, как в топе mmbench оказалась неизвестная модель и почему иногда бенчмарки могут искажать реальную ситуацию. работа представляет собой репорт о проведённом эксперименте. материал не содержит наукоёмких изменений в пайплайне обучения или архитектуре модели. авторы сосредоточены на теме данных и отвечают на вопрос: как при ограниченных ресурсах стать sota vlm, пусть даже не в самом популярном классе маленьких моделек. один из ключевых тезисов: при обучении vlm имеет смысл масштабировать объём sft-данных. с этой целью авторы собрали всё доступное из опенсорса и получили датасет из нескольких десятков миллионов инстрактов, а также сгенерировали в дополнение небольшую часть синтетики. все данные фильтровались, проходили дедупликацию и проверку на разнообразие. итоговый мультимодальный датасет — это и есть infinity-mm из названия статьи. из-за ограничения в вычислительных ресурсах исследователи использовали для генерации и чистки датасета опенсорсные модели, в частности активно прибегали к помощи qwen. пайплайн для генерации синтетических данных выглядит следующим образом: — собирается база изображений, их размечают с помощью опенсорс-модели, которая тегирует объекты на картинке. на основе тегов формируется дерево типов задач, для которых целесообразно создавать инстракты. — опенсорсными моделями (преимущественно minicpm и qwen) генерируют инстракт по картинке, тегу и тематике. происходит автофильтрация через эту же модель (ей дают сгенерированный инстракт и спрашивают, насколько он валиден). — затем получают ответ по синтетическому инстракту — та же модель снова отвечает на вопрос, который сама придумала. — следующий шаг — фильтрации ответа. тут интересное решение: опенсорсная модель считает лосс по полученной паре и, если он высокий, пример исключается. так исследователи автоматически отфильтровали 5% самых «шумных» данных. этих несложных манипуляций хватило, чтобы обогнать модели, которыми авторы генерировали и фильтровали свои данные. скорее всего, так произошло, потому что синтетику целенаправленно собирали под конкретный бенчмарк (mmbench). и в таком случае модель может непредсказуемо вести себя на других задачах. можно сделать вывод, что бенчмарки лучше использовать исключительно как «градусник», чтобы следить за изменениями в области. а вот для оценки эффективности моделей надёжнее ориентироваться на sbs-замеры (side-by-side), которые позволяют проводить прямое сравнение в реальных условиях. а как вы оцениваете опыт aquila-vl-2b и доверяете ли ещё бенчмаркам? обзор подготовил ❣ алексей григорьев cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-01-31T08:06:23+00:00" href="./posts/59.html">2025-01-31 08:06 UTC</a></div>
      </div>
      <div class="post-body"><strong>Опыт модели Aquila-VL-2B: почему не всегда стоит доверять бенчмаркам</strong><br><br>Сегодня разберём <a href="https://arxiv.org/abs/2410.18558" rel="nofollow noopener noreferrer">работу Infinity-MM</a> и описанную в ней модель Aquila-VL-2B. Эта маленькая VLM с двумя миллиардами параметров интересна тем, что смогла обойти в своём классе Qwen и InternVL, которые редко уступают первые позиции лидербордов. Расскажем, как в топе MMBench оказалась неизвестная модель и почему иногда бенчмарки могут искажать реальную ситуацию.<br><br>Работа представляет собой репорт о проведённом эксперименте. Материал не содержит наукоёмких изменений в пайплайне обучения или архитектуре модели. Авторы сосредоточены на теме данных и отвечают на вопрос: как при ограниченных ресурсах стать SOTA VLM, пусть даже не в самом популярном классе маленьких моделек.<br><br>Один из ключевых тезисов: при обучении VLM имеет смысл масштабировать объём SFT-данных. С этой целью авторы собрали всё доступное из опенсорса и получили датасет из нескольких десятков миллионов инстрактов, а также сгенерировали в дополнение небольшую часть синтетики. Все данные фильтровались, проходили дедупликацию и проверку на разнообразие. Итоговый мультимодальный датасет — это и есть Infinity-MM из названия статьи.<br><br>Из-за ограничения в вычислительных ресурсах исследователи использовали для генерации и чистки датасета опенсорсные модели, в частности активно прибегали к помощи Qwen.<br><br>Пайплайн для генерации синтетических данных выглядит следующим образом:<br><br>— Собирается база изображений, их размечают с помощью опенсорс-модели, которая тегирует объекты на картинке.<br>На основе тегов формируется дерево типов задач, для которых целесообразно создавать инстракты. <br><br>— Опенсорсными моделями (преимущественно MiniCPM и Qwen) генерируют инстракт по картинке, тегу и тематике. Происходит автофильтрация через эту же модель (ей дают сгенерированный инстракт и спрашивают, насколько он валиден). <br><br>— Затем получают ответ по синтетическому инстракту — та же модель снова отвечает на вопрос, который сама придумала.<br><br>— Следующий шаг — фильтрации ответа. Тут интересное решение: опенсорсная модель считает лосс по полученной паре и, если он высокий, пример исключается. Так исследователи автоматически отфильтровали 5% самых «шумных» данных.<br><br>Этих несложных манипуляций хватило, чтобы обогнать модели, которыми авторы генерировали и фильтровали свои данные. Скорее всего, так произошло, потому что  синтетику целенаправленно собирали под конкретный бенчмарк (MMBench). И в таком случае модель может непредсказуемо вести себя на других задачах.<br><br>Можно сделать вывод, что бенчмарки лучше использовать исключительно как «градусник», чтобы следить за изменениями в области. А вот для оценки эффективности моделей надёжнее ориентироваться на SBS-замеры (Side-by-Side), которые позволяют проводить прямое сравнение в реальных условиях. <br><br>А как вы оцениваете опыт Aquila-VL-2B и доверяете ли ещё бенчмаркам?<br><br><br><em>Обзор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Алексей Григорьев</em><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/59_480.webp" srcset="../assets/media/thumbs/59_480.webp 480w, ../assets/media/59.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="59" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 136 просмотров · 24 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/59" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/59.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="58" data-search="лучшие cv-статьи за 2024 год: подборка от авторов канала часть 3 сегодня делимся подборкой от александра шишени, ведущего разработчика службы компьютерного зрения в яндексе. он выбрал интересные направления и статьи в них, чтобы нам не пришлось рыться в тоннах публикаций. авторегрессионная генерация изображений switti: designing scale-wise transformers for text-to-image synthesis команда исследователей из yandex research обучила и опубликовала в открытом доступе собственную авторегрессионную модель для генерации изображений по тексту. за основу взяли архитектуру star и провели ряд улучшений: — добавили дополнительные нормализационные слои для стабилизации обучения; — убрали авторегрессию на прошлые разрешения — тем самым ускорили генерацию и уменьшили потребление памяти; — отключили технику classifier-free-guidance (cfg) на высоких разрешениях — получили значительное ускорение без ухудшения качества генерации. в результате switti имеет паритет по качеству с диффузионками, но при этом генерирует в 7 раз быстрее оригинальной sdxl-модели и в 2 раза быстрее её ускоренных версий. можно также прочитать подробный разбор решения в трёх частях: первая, вторая, третья. visual autoregressive modeling: scalable image generation via next-scale prediction один из недостатков авторегрессионных моделей — низкая скорость генерации изображений. чтобы решить эту и другие проблемы, bytedance предложили альтернативную авторегрессионную парадигму, которая включает: — многомасштабный vq-vae для одновременного кодирования и декодирования изображений на нескольких уровнях разрешения; — обучение трансформера для последовательной генерации токенов изображения с постепенным увеличением масштаба. в результате удалось добиться качества, сопоставимого с современными диффузионками, обученными на imagenet, и при этом превзойти их по скорости. infinity∞: scaling bitwise autoregressive modeling for high-resolution image synthesis авторы infinity∞ продолжают развивать идеи масштабируемой авторегрессионной генерации. их решение использует битовую токенизацию с бесконечным словарём и механизмом самокоррекции. результат — заметно повышается качество генерируемых изображений. диффузионные модели neural flow diffusion models: learnable forward process for improved diffusion modelling на стадии инференса диффузионных моделей приходится делать много итераций, так как траектории сэмплирования, определяемые моделью, существенно отклоняются от прямых. это во многом связано с тем, что процесс зашумления в стандартной диффузии задаётся фиксированной формулой. авторы предлагают обобщить диффузионные модели с подходом flow-matching, добавив обучаемое зашумление. это позволяет адаптировать процесс зашумления так, чтобы траектории минимально отклонялись от прямых. интересно, что такая постановка приводит к решению задачи оптимального транспорта. ✨а ещё в канале ml underhood можно полистать карточки, где автор подборки александар шишеня рассказывает, над чем трудился в прошедшем году и какие события в сфере ml считает главными. cv time лучшие cv-статьи за 2024 год: подборка от авторов канала часть 3 сегодня делимся подборкой от александра шишени, ведущего разработчика службы компьютерного зрения в яндексе. он выбрал интересные направления и статьи в них, чтобы нам не пришлось рыться в тоннах публикаций. авторегрессионная генерация изображений switti: designing scale-wise transformers for text-to-image synthesis команда исследователей из yandex research обучила и опубликовала в открытом доступе собственную авторегрессионную модель для генерации изображений по тексту. за основу взяли архитектуру star и провели ряд улучшений: — добавили дополнительные нормализационные слои для стабилизации обучения; — убрали авторегрессию на прошлые разрешения — тем самым ускорили генерацию и уменьшили потребление памяти; — отключили технику classifier-free-guidance (cfg) на высоких разрешениях — получили значительное ускорение без ухудшения качества генерации. в результате switti имеет паритет по качеству с диффузионками, но при этом генерирует в 7 раз быстрее оригинальной sdxl-модели и в 2 раза быстрее её ускоренных версий. можно также прочитать подробный разбор решения в трёх частях: первая , вторая , третья . visual autoregressive modeling: scalable image generation via next-scale prediction один из недостатков авторегрессионных моделей — низкая скорость генерации изображений. чтобы решить эту и другие проблемы, bytedance предложили альтернативную авторегрессионную парадигму, которая включает: — многомасштабный vq-vae для одновременного кодирования и декодирования изображений на нескольких уровнях разрешения; — обучение трансформера для последовательной генерации токенов изображения с постепенным увеличением масштаба. в результате удалось добиться качества, сопоставимого с современными диффузионками, обученными на imagenet, и при этом превзойти их по скорости. infinity∞: scaling bitwise autoregressive modeling for high-resolution image synthesis авторы infinity∞ продолжают развивать идеи масштабируемой авторегрессионной генерации. их решение использует битовую токенизацию с бесконечным словарём и механизмом самокоррекции. результат — заметно повышается качество генерируемых изображений. диффузионные модели neural flow diffusion models: learnable forward process for improved diffusion modelling на стадии инференса диффузионных моделей приходится делать много итераций, так как траектории сэмплирования, определяемые моделью, существенно отклоняются от прямых. это во многом связано с тем, что процесс зашумления в стандартной диффузии задаётся фиксированной формулой. авторы предлагают обобщить диффузионные модели с подходом flow-matching, добавив обучаемое зашумление. это позволяет адаптировать процесс зашумления так, чтобы траектории минимально отклонялись от прямых. интересно, что такая постановка приводит к решению задачи оптимального транспорта. ✨ а ещё в канале ml underhood можно полистать карточки , где автор подборки александар шишеня рассказывает, над чем трудился в прошедшем году и какие события в сфере ml считает главными. cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-01-27T13:06:52+00:00" href="./posts/58.html">2025-01-27 13:06 UTC</a></div>
      </div>
      <div class="post-body"><strong>Лучшие CV-статьи за 2024 год: подборка от авторов канала</strong><br><em>Часть 3</em><br><br>Сегодня делимся подборкой от Александра Шишени, ведущего разработчика службы компьютерного зрения в Яндексе. Он выбрал интересные направления и статьи в них, чтобы нам не пришлось рыться в тоннах публикаций.<br><br><strong>Авторегрессионная генерация изображений</strong><br><br><a href="https://arxiv.org/abs/2412.01819" rel="nofollow noopener noreferrer"><strong>SWITTI: Designing Scale-Wise Transformers for Text-to-Image Synthesis</strong></a><br>Команда исследователей из Yandex Research обучила и опубликовала в открытом доступе собственную авторегрессионную модель для генерации изображений по тексту. За основу взяли архитектуру <a href="https://arxiv.org/html/2406.10797v1" rel="nofollow noopener noreferrer">STAR</a> и провели ряд улучшений:<br>— добавили дополнительные нормализационные слои для стабилизации обучения;<br>— убрали авторегрессию на прошлые разрешения — тем самым ускорили генерацию и уменьшили потребление памяти;<br>— отключили технику classifier-free-guidance (CFG) на высоких разрешениях — получили значительное ускорение без ухудшения качества генерации. <br>В результате <a href="https://huggingface.co/spaces/dbaranchuk/Switti" rel="nofollow noopener noreferrer">Switti</a> имеет паритет по качеству с диффузионками, но при этом генерирует в 7 раз быстрее оригинальной SDXL-модели и в 2 раза быстрее её ускоренных версий. <br>Можно также прочитать подробный разбор решения в трёх частях: <a href="https://t.me/timeforcv/40" rel="nofollow noopener noreferrer">первая</a>, <a href="https://t.me/timeforcv/41" rel="nofollow noopener noreferrer">вторая</a>, <a href="https://t.me/timeforcv/42" rel="nofollow noopener noreferrer">третья</a>.<br><br><a href="https://arxiv.org/abs/2404.02905" rel="nofollow noopener noreferrer"><strong>Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction </strong></a><br>Один из недостатков авторегрессионных моделей — низкая скорость генерации изображений. Чтобы решить эту и другие проблемы, ByteDance предложили альтернативную авторегрессионную парадигму, которая включает:<br>— многомасштабный VQ-VAE для одновременного кодирования и декодирования изображений на нескольких уровнях разрешения;<br>— обучение трансформера для последовательной генерации токенов изображения с постепенным увеличением масштаба. <br>В результате удалось добиться качества, сопоставимого с современными диффузионками, обученными на ImageNet, и при этом превзойти их по скорости. <br><br><a href="https://arxiv.org/abs/2412.04431" rel="nofollow noopener noreferrer"><strong>Infinity∞: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis</strong></a><br>Авторы Infinity∞ продолжают развивать идеи масштабируемой авторегрессионной генерации. Их решение использует битовую токенизацию с бесконечным словарём и механизмом самокоррекции. Результат — заметно повышается качество генерируемых изображений.<br><br><strong>Диффузионные модели</strong><br><br><a href="https://arxiv.org/abs/2404.12940" rel="nofollow noopener noreferrer">Neural Flow Diffusion Models: Learnable Forward Process for Improved Diffusion Modelling </a><br>На стадии инференса диффузионных моделей приходится делать много итераций, так как траектории сэмплирования, определяемые моделью, существенно отклоняются от прямых. <br><br>Это во многом связано с тем, что процесс зашумления в стандартной диффузии задаётся фиксированной формулой. Авторы предлагают обобщить диффузионные модели с подходом flow-matching, добавив обучаемое зашумление. Это позволяет адаптировать процесс зашумления так, чтобы траектории минимально отклонялись от прямых. Интересно, что такая постановка приводит к решению задачи оптимального транспорта.<br><br><tg-emoji emoji-id="5220158626571766442">✨</tg-emoji>А ещё в канале ML Underhood можно <a href="https://t.me/MLunderhood/54" rel="nofollow noopener noreferrer">полистать карточки</a>, где автор подборки Александар Шишеня рассказывает, над чем трудился в прошедшем году и какие события в сфере ML считает главными.<br><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a></div>
      <div class="actions">
        <span>2 076 просмотров · 19 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/58" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/58.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="57" data-search="лучшие cv-статьи за 2024 год: подборка от авторов канала часть 2 продолжаем делиться самыми полезными публикациями прошлого года. сегодня на очереди три статьи, которые рекомендует изучить денис кузнеделев, исследователь yandex research. improved distribution matching distillation for fast image synthesis авторы пытаются решить вопрос быстрой и качественной генерации картинок. в первой версии подхода из статьи one-step diffusion with distribution matching distillation предложили обучать вспомогательную модель (функцию fake score), чтобы она оценивала, насколько генерации «быстрой» модели похожи на результаты «медленной» модели-учителя. подход работал неплохо, но всё же картинки от «быстрой» модели были хуже учительских. в новой статье авторы существенно модифицировали решение. теперь «быстрая» модель обновляется не каждый раз, а через несколько шагов функции fake score. также добавили ещё одну вспомогательную модель-дискриминатор, которая оценивает реалистичность картинок, сгенерированных «быстрой» моделью. с этими изменениями удалось значительно улучшить качество при генерации в один и четыре шага. movie gen: a cast of media foundation models осенью компания meta* представила семейство моделей moviegen для генерации видео. с фундаментальной точки зрения технический отчёт проекта предлагает немногое, но содержит ряд нетривиальных идей. в их числе — специальная регуляризация при обучении сети-автокодировщика, архитектурные решения для стабилизации и масштабирования обучения, специальное расписание диффузии. процесс сбора и подготовки данных включает как чисто картиночные данные, так и короткие видеоклипы разного разрешения и содержания. модели умеют: — генерировать видео по тексту; — генерировать видео с целевым объектом (человеком, животным, предметом); — редактировать видео на основе инструкций. дополнительно к видеомоделям исследователи и инженеры из meta обучили модель для озвучки видео — moviegen-audio. visual autoregressive modeling: scalable image generation via next-scale prediction авторы переосмысливают авторегрессию в картиночных моделях и предлагают токенизировать изображение по масштабу, а не по пространственным патчам. новый класс моделей обгоняет предшественников по времени генерации, при этом не уступая по качеству. подробнее расскажем о статье в следующей части подборки. cv time ___ meta признана экстремистской организацией, а facebook и instagram запрещены на территории рф лучшие cv-статьи за 2024 год: подборка от авторов канала часть 2 продолжаем делиться самыми полезными публикациями прошлого года. сегодня на очереди три статьи, которые рекомендует изучить денис кузнеделев, исследователь yandex research. improved distribution matching distillation for fast image synthesis авторы пытаются решить вопрос быстрой и качественной генерации картинок. в первой версии подхода из статьи one-step diffusion with distribution matching distillation предложили обучать вспомогательную модель (функцию fake score), чтобы она оценивала, насколько генерации «быстрой» модели похожи на результаты «медленной» модели-учителя. подход работал неплохо, но всё же картинки от «быстрой» модели были хуже учительских. в новой статье авторы существенно модифицировали решение. теперь «быстрая» модель обновляется не каждый раз, а через несколько шагов функции fake score. также добавили ещё одну вспомогательную модель-дискриминатор, которая оценивает реалистичность картинок, сгенерированных «быстрой» моделью. с этими изменениями удалось значительно улучшить качество при генерации в один и четыре шага. movie gen: a cast of media foundation models осенью компания meta* представила семейство моделей moviegen для генерации видео. с фундаментальной точки зрения технический отчёт проекта предлагает немногое, но содержит ряд нетривиальных идей. в их числе — специальная регуляризация при обучении сети-автокодировщика, архитектурные решения для стабилизации и масштабирования обучения, специальное расписание диффузии. процесс сбора и подготовки данных включает как чисто картиночные данные, так и короткие видеоклипы разного разрешения и содержания. модели умеют: — генерировать видео по тексту; — генерировать видео с целевым объектом (человеком, животным, предметом); — редактировать видео на основе инструкций. дополнительно к видеомоделям исследователи и инженеры из meta обучили модель для озвучки видео — moviegen-audio. visual autoregressive modeling: scalable image generation via next-scale prediction авторы переосмысливают авторегрессию в картиночных моделях и предлагают токенизировать изображение по масштабу, а не по пространственным патчам. новый класс моделей обгоняет предшественников по времени генерации, при этом не уступая по качеству. подробнее расскажем о статье в следующей части подборки. cv time ___ meta признана экстремистской организацией, а facebook и instagram запрещены на территории рф">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-01-22T07:34:47+00:00" href="./posts/57.html">2025-01-22 07:34 UTC</a></div>
      </div>
      <div class="post-body"><strong>Лучшие CV-статьи за 2024 год: подборка от авторов канала</strong><br><em>Часть 2</em><br><br>Продолжаем делиться самыми полезными публикациями прошлого года. Сегодня на очереди три статьи, которые рекомендует изучить Денис Кузнеделев, исследователь Yandex Research.<br><br><a href="https://arxiv.org/abs/2405.14867" rel="nofollow noopener noreferrer"><strong>Improved Distribution Matching Distillation for Fast Image Synthesis</strong></a><br><br>Авторы пытаются решить вопрос быстрой и качественной генерации картинок. В первой версии подхода из статьи <a href="https://arxiv.org/abs/2311.18828" rel="nofollow noopener noreferrer">One-step Diffusion with Distribution Matching Distillation</a> предложили обучать вспомогательную модель (функцию fake score), чтобы она оценивала, насколько генерации «быстрой» модели похожи на результаты «медленной» модели-учителя. Подход работал неплохо, но всё же картинки от «быстрой» модели были хуже учительских. <br><br>В новой статье авторы существенно модифицировали решение. Теперь «быстрая» модель обновляется не каждый раз, а через несколько шагов функции fake score. Также добавили ещё одну вспомогательную модель-дискриминатор, которая оценивает реалистичность картинок, сгенерированных «быстрой» моделью. С этими изменениями удалось значительно улучшить качество при генерации в один и четыре шага. <br><strong><br></strong><a href="https://arxiv.org/abs/2410.13720" rel="nofollow noopener noreferrer"><strong>Movie Gen: A Cast of Media Foundation Models</strong></a><br><br>Осенью компания Meta* представила семейство моделей MovieGen для генерации видео. С фундаментальной точки зрения технический отчёт проекта предлагает немногое, но содержит ряд нетривиальных идей. В их числе — специальная регуляризация при обучении сети-автокодировщика, архитектурные решения для стабилизации и масштабирования обучения, специальное расписание диффузии. <br><br>Процесс сбора и подготовки данных включает как чисто картиночные данные, так и короткие видеоклипы разного разрешения и содержания. Модели умеют: <br><br>— генерировать видео по тексту; <br>— генерировать видео с целевым объектом (человеком, животным, предметом);<br>— редактировать видео на основе инструкций. <br><br>Дополнительно к видеомоделям исследователи и инженеры из Meta обучили модель для озвучки видео — MovieGen-Audio.<br><br><a href="https://arxiv.org/abs/2404.02905" rel="nofollow noopener noreferrer"><strong>Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction</strong></a><br><br>Авторы переосмысливают авторегрессию в картиночных моделях и предлагают токенизировать изображение по масштабу, а не по пространственным патчам. Новый класс моделей обгоняет предшественников по времени генерации, при этом не уступая по качеству. Подробнее расскажем о статье в следующей части подборки.<br><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><br>___<br><em>Meta признана экстремистской организацией, а Facebook и Instagram запрещены на территории РФ</em></div>
      <div class="actions">
        <span>2 303 просмотров · 11 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/57" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/57.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="56" data-search="analyzing the language of visual tokens визуальные токены ≠ текстовые, но в мультимодальных моделях (например, llava и chameleon) они часто используются схожим образом, без искусственного разделения. авторы сегодняшней статьи исследуют, насколько близки естественные языки, образованные текстовыми токенами, и визуальные языки, образованные токенами визуальными. для этого они сопоставляют визуальные языки coco, imagenet и других популярных датасетов, используя vq-vae-like токенайзеры и линеаризацию токенов изображений в 1d-последовательности (как на картинке). а потом сравнивают полученные для визуальных языков закономерности с зависимостями для естественных. ключевые результаты: — в отличие от естественных языков, визуальные не подчиняются zipf’s law: среди визуальных токенов нет превалирующих (для текстовых это, например, артикль the). — визуальные языки разнообразнее естественных: уникальные изображения чаще состоят из новых токенов. авторы показывают это с помощью процесса yule-simon. — визуальные языки «естественны» — удовлетворяют benford’s law. — пытаясь сжать визуальные токены кодированием хаффмана, исследователи пришли к выводу, что визуальные языки хаотичнее, комплекснее и менее избыточны, чем естественные. — визуальные токены изображений чаще соотносятся с частями объектов, а не с целыми предметами. эмпирические исследования показали: хотя технически визуальные токены похожи на текстовые, составленные из них языки не совпадают. по мнению авторов, это важно учитывать в обучении моделей: нейросети, которые работают с визуальными токенами, более подвержены переобучению. а комплексность и разнообразие визуальных языков требуют более долгой тренировки. обзор подготовил ❣ никита буров cv time analyzing the language of visual tokens визуальные токены ≠ текстовые, но в мультимодальных моделях (например, llava и chameleon) они часто используются схожим образом, без искусственного разделения. авторы сегодняшней статьи исследуют, насколько близки естественные языки, образованные текстовыми токенами, и визуальные языки, образованные токенами визуальными. для этого они сопоставляют визуальные языки coco, imagenet и других популярных датасетов, используя vq-vae-like токенайзеры и линеаризацию токенов изображений в 1d-последовательности (как на картинке). а потом сравнивают полученные для визуальных языков закономерности с зависимостями для естественных. ключевые результаты: — в отличие от естественных языков, визуальные не подчиняются zipf’s law: среди визуальных токенов нет превалирующих (для текстовых это, например, артикль the). — визуальные языки разнообразнее естественных: уникальные изображения чаще состоят из новых токенов. авторы показывают это с помощью процесса yule-simon. — визуальные языки «естественны» — удовлетворяют benford’s law. — пытаясь сжать визуальные токены кодированием хаффмана, исследователи пришли к выводу, что визуальные языки хаотичнее, комплекснее и менее избыточны, чем естественные. — визуальные токены изображений чаще соотносятся с частями объектов, а не с целыми предметами. эмпирические исследования показали: хотя технически визуальные токены похожи на текстовые, составленные из них языки не совпадают. по мнению авторов, это важно учитывать в обучении моделей: нейросети, которые работают с визуальными токенами, более подвержены переобучению. а комплексность и разнообразие визуальных языков требуют более долгой тренировки. обзор подготовил ❣ никита буров cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-01-16T07:33:38+00:00" href="./posts/56.html">2025-01-16 07:33 UTC</a></div>
      </div>
      <div class="post-body"><strong>Analyzing The Language of Visual Tokens</strong><br><br>Визуальные токены ≠ текстовые, но в мультимодальных моделях (например, LLaVA и Chameleon) они часто используются схожим образом, без искусственного разделения. <br> <br>Авторы сегодняшней <a href="https://arxiv.org/pdf/2411.05001" rel="nofollow noopener noreferrer">статьи</a> исследуют, насколько близки естественные языки, образованные текстовыми токенами, и визуальные языки, образованные токенами визуальными. Для этого они сопоставляют визуальные языки COCO, ImageNet и других популярных датасетов, используя VQ-VAE-like токенайзеры и линеаризацию токенов изображений в 1D-последовательности (как на картинке). А потом сравнивают полученные для визуальных языков закономерности с зависимостями для естественных. <br><br><strong>Ключевые результаты: </strong><br><br>— В отличие от естественных языков, визуальные не подчиняются Zipf’s law: среди визуальных токенов нет превалирующих (для текстовых это, например, артикль the). <br>— Визуальные языки разнообразнее естественных: уникальные изображения чаще состоят из новых токенов. Авторы показывают это с помощью процесса Yule-Simon.<br>— Визуальные языки «естественны» — удовлетворяют Benford’s law. <br>— Пытаясь сжать визуальные токены кодированием Хаффмана, исследователи пришли к выводу, что визуальные языки хаотичнее, комплекснее и менее избыточны, чем естественные.<br>— Визуальные токены изображений чаще соотносятся с частями объектов, а не с целыми предметами.<br><br>Эмпирические исследования показали: хотя технически визуальные токены похожи на текстовые, составленные из них языки не совпадают. <br><br>По мнению авторов, это важно учитывать в обучении моделей: нейросети, которые работают с визуальными токенами, более подвержены переобучению. А комплексность и разнообразие визуальных языков требуют более долгой тренировки. <br><br><em>Обзор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Никита Буров</em><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/56_480.webp" srcset="../assets/media/thumbs/56_480.webp 480w, ../assets/media/56.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="56" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 592 просмотров · 40 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/56" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/56.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="55" data-search="лучшие cv-статьи за 2024 год: подборка от авторов канала часть 1 лучшие публикации прошедшего года не теряют актуальности в новом. мы попросили постоянных авторов канала отметить самые полезные пейперы из 2024-го — несём вам первую часть подборки. scaling rectified flow transformers for high-resolution image synthesis исследователи совместили набирающую популярность модель flow matching (rectified flow) и нейросетевую архитектуру dit (diffusion transformer), чтобы адаптировать их для генерации изображений по тексту. именно эта работа лежит в основе модели stable diffusion 3. deep compression autoencoder for efficient high-resolution diffusion models в статье предложено новое архитектурное семейство картиночных кодировщиков deepcompression-ae. они позволяют сжимать изображения в 64 раза (по каждой стороне) с минимальными потерями. dc-ae значительно уменьшает затраты по времени и памяти при генерации, а также обладает высокой точностью реконструкции. internvl: scaling up vision foundation models and aligning for generic visual-linguistic tasks модели семейства internvl регулярно оказываются в топе бенчмарков и составляют конкуренцию проприетарным моделям, вроде gemini и gpt-4o. авторы придерживаются открытого подхода к исследованиям — все веса моделей доступны для свободного использования. основная статья была опубликована в конце 2023 года, но в 2024 вышли значимые обновления для версий 1.5, 2 и 2.5. playground v3: improving text-to-image alignment with deep-fusion large language models как и предыдущая, эта статья даёт возможность узнать детали устройства state-of-the-art модели, в данном случае — text-to-image. авторы приводят подробности об архитектуре, сборе датасета и стабилизации процесса обучения. chameleon: mixed-modal early-fusion foundation models статья задала тренд на развитие моделей, которые умеют нативно работать с изображениями как в дискриминативном, так и в генеративном форматах. такая модель не только ответит на вопрос по изображению, но и при необходимости сгенерирует в своём ответе картинку. law of vision representation in mllms выбор картиночного бэкбона для мультимодальных llm обычно происходит эмпирически: перебираем n вариантов и берём лучший по соотношению скорости и качества. в статье сделана попытка с научной точки зрения ответить на вопрос, что такое хороший картиночный бэкбон. для этого авторы ввели свойства alignment и correspondence, которым должны соответствовать кандидаты. в дополнение можно прочитать неформальный блогпост от автора. продолжение следует. статьи отобрали и прокомментировали ❣ александр устюжанин и артём конев cv time лучшие cv-статьи за 2024 год: подборка от авторов канала часть 1 лучшие публикации прошедшего года не теряют актуальности в новом. мы попросили постоянных авторов канала отметить самые полезные пейперы из 2024-го — несём вам первую часть подборки. scaling rectified flow transformers for high-resolution image synthesis исследователи совместили набирающую популярность модель flow matching (rectified flow) и нейросетевую архитектуру dit (diffusion transformer), чтобы адаптировать их для генерации изображений по тексту. именно эта работа лежит в основе модели stable diffusion 3. deep compression autoencoder for efficient high-resolution diffusion models в статье предложено новое архитектурное семейство картиночных кодировщиков deepcompression-ae. они позволяют сжимать изображения в 64 раза (по каждой стороне) с минимальными потерями. dc-ae значительно уменьшает затраты по времени и памяти при генерации, а также обладает высокой точностью реконструкции. internvl: scaling up vision foundation models and aligning for generic visual-linguistic tasks модели семейства internvl регулярно оказываются в топе бенчмарков и составляют конкуренцию проприетарным моделям, вроде gemini и gpt-4o. авторы придерживаются открытого подхода к исследованиям — все веса моделей доступны для свободного использования. основная статья была опубликована в конце 2023 года, но в 2024 вышли значимые обновления для версий 1.5 , 2 и 2.5 . playground v3: improving text-to-image alignment with deep-fusion large language models как и предыдущая, эта статья даёт возможность узнать детали устройства state-of-the-art модели, в данном случае — text-to-image. авторы приводят подробности об архитектуре, сборе датасета и стабилизации процесса обучения. chameleon: mixed-modal early-fusion foundation models статья задала тренд на развитие моделей, которые умеют нативно работать с изображениями как в дискриминативном, так и в генеративном форматах. такая модель не только ответит на вопрос по изображению, но и при необходимости сгенерирует в своём ответе картинку. law of vision representation in mllms выбор картиночного бэкбона для мультимодальных llm обычно происходит эмпирически: перебираем n вариантов и берём лучший по соотношению скорости и качества. в статье сделана попытка с научной точки зрения ответить на вопрос, что такое хороший картиночный бэкбон. для этого авторы ввели свойства alignment и correspondence, которым должны соответствовать кандидаты. в дополнение можно прочитать неформальный блогпост от автора. продолжение следует. статьи отобрали и прокомментировали ❣ александр устюжанин и артём конев cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-01-10T08:02:37+00:00" href="./posts/55.html">2025-01-10 08:02 UTC</a></div>
      </div>
      <div class="post-body"><strong>Лучшие CV-статьи за 2024 год: подборка от авторов канала</strong><br><em>Часть 1</em><br><br>Лучшие публикации прошедшего года не теряют актуальности в новом. Мы попросили постоянных авторов канала отметить самые полезные пейперы из 2024-го — несём вам первую часть подборки.<br><br><a href="https://arxiv.org/abs/2403.03206" rel="nofollow noopener noreferrer"><strong>Scaling Rectified Flow Transformers for High-Resolution Image Synthesis </strong></a><br>Исследователи совместили набирающую популярность модель Flow Matching (Rectified Flow) и нейросетевую архитектуру DiT (Diffusion Transformer), чтобы адаптировать их для генерации изображений по тексту. Именно эта работа лежит в основе модели Stable Diffusion 3.<br><br><a href="https://arxiv.org/abs/2410.10733" rel="nofollow noopener noreferrer"><strong>Deep Compression Autoencoder for Efficient High-Resolution Diffusion Models</strong></a><br>В статье предложено новое архитектурное семейство картиночных кодировщиков DeepCompression-AE. Они позволяют сжимать изображения в 64 раза (по каждой стороне) с минимальными потерями. DC-AE значительно уменьшает затраты по времени и памяти при генерации, а также обладает высокой точностью реконструкции.<br><br><a href="https://arxiv.org/pdf/2312.14238" rel="nofollow noopener noreferrer"><strong>InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks</strong></a><br>Модели семейства InternVL регулярно оказываются в топе бенчмарков и составляют конкуренцию проприетарным моделям, вроде Gemini и GPT-4o. Авторы придерживаются открытого подхода к исследованиям — все веса моделей доступны для свободного использования. Основная статья была опубликована в конце 2023 года, но в 2024 вышли значимые обновления для версий <a href="https://arxiv.org/pdf/2404.16821" rel="nofollow noopener noreferrer">1.5</a>, <a href="https://internvl.github.io/blog/2024-07-02-InternVL-2.0/" rel="nofollow noopener noreferrer">2</a> и <a href="https://arxiv.org/pdf/2412.05271" rel="nofollow noopener noreferrer">2.5</a>. <br><br><a href="https://arxiv.org/pdf/2409.10695" rel="nofollow noopener noreferrer"><strong>Playground v3: Improving Text-to-Image Alignment with Deep-Fusion Large Language Models</strong></a><br>Как и предыдущая, эта статья даёт возможность узнать детали устройства state-of-the-art модели, в данном случае — text-to-image. Авторы приводят подробности об архитектуре, сборе датасета и стабилизации процесса обучения.<br><br><a href="https://arxiv.org/pdf/2405.09818v1" rel="nofollow noopener noreferrer"><strong>Chameleon: Mixed-Modal Early-Fusion Foundation Models</strong></a><br>Статья задала тренд на развитие моделей, которые умеют нативно работать с изображениями как в дискриминативном, так и в генеративном форматах. Такая модель не только ответит на вопрос по изображению, но и при необходимости сгенерирует в своём ответе картинку.<br> <br><a href="https://arxiv.org/pdf/2408.16357" rel="nofollow noopener noreferrer"><strong>Law of Vision Representation in MLLMs</strong></a><br>Выбор картиночного бэкбона для мультимодальных LLM обычно происходит эмпирически: перебираем N вариантов и берём лучший по соотношению скорости и качества. В статье сделана попытка с научной точки зрения ответить на вопрос, что такое хороший картиночный бэкбон. Для этого авторы ввели свойства Alignment и Correspondence, которым должны соответствовать кандидаты. В дополнение можно прочитать <a href="https://huggingface.co/blog/Borise/law-vision-representation-in-mllms" rel="nofollow noopener noreferrer">неформальный блогпост</a> от автора. <br><br>Продолжение следует.<br><br><em>Статьи отобрали и прокомментировали </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Александр Устюжанин и Артём Конев</em><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a></div>
      <div class="actions">
        <span>2 676 просмотров · 19 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/55" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/55.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="54" data-search="🏆 итоги года: самые популярные посты в cv time 2024-й на финишной прямой! потихоньку настраиваемся на подведение итогов и освежаем в памяти самые популярные посты уходящего года. уверены, это будет почти так же уютно, как пересматривать «один дома» (только про компьютерное зрение). minimalist vision with freeform pixels алиса родионова разобрала статью с eccv-24, получившую награду best paper award. авторы создали прототип автономной по электропитанию камеры. вместо обычной матрицы в ней установлены 24 фотодиода с масками-фильтрами, которые выполняют роль первого слоя нейросети. такая архитектура позволяет адаптировать камеру под разные задачи и получать результаты с помощью всего 8 пикселей. в обзоре — подробнее о том, как устроено решение. интересное с eccv 2024 ещё один пост с eccv-24. дарья виноградова, александр устюжанин и сергей кастрюлин отобрали самые интересные доклады первого дня конференции. в их числе «механизм реалистичности» для сокращения доменного сдвига в 3d-рендеринге лиц, подходы к улучшению реализма в text-to-3d и image-to-3d моделях, а также концепция разделения knowledge и memory в нейросетях. как llama 3.1 работает с изображениями роман исаченко объяснил, как устроена мультимодальная llama 3.1: какие архитектурные решения в основе, на каких данных её обучали и как она показывает себя на бенчмарках. он подметил несколько интересных трюков. среди них — подмена весов на этапе файнтюна через hot-swap и дополнительный пост-претрейн на датасете с редкими скриншотами и таблицами. dart: denoising autoregressive transformer for scalable text-to-image generation, часть 2 александр шишеня в двух постах рассказал, как устроена dart, диффузионная авторегрессионная модель для генерации изображений. в разборе вы найдёте детали о полезных модификациях, вроде dart-ar с ускорением обучения и dart-fm с flow matching для повышения качества генераций. вторая часть оказалась ещё популярнее первой, — видимо, вам хотелось поскорее узнать развязку. pyramidal flow matching for efficient video generative modeling александр маркович разобрал, как sora-like модели создают видео. авторы статьи предлагают отказаться от высокого разрешения ради скорости обучения и инференса. интересная часть — пирамидальный подход: вместо многослойной генерации видео с постепенным апскейлом, как обычно, моделируют все разрешения сразу. впечатления от eccv 2024 мы попросили инженеров яндекса подвести личные итоги конференции и рассказать, чем она запомнилась. в посте собрали рекомендации свежих работ по cv и любопытные заметки на полях. среди трендов заметили фокус на исследованиях за пределами области text-to-image и спад интереса к узким темам. все детали — на наших красочных карточках. movie gen: a cast of media foundation models денис кузнеделев взял оригинальный технический отчёт проекта и разобрал решения, использованные в моделях для генерации видео movie gen. некоторые идеи оказались нетривиальными, например, регуляризация при обучении сети-автокодировщика, архитектура для стабилизации и масштабирования обучения, специальное расписание диффузии. больше подробностей — по ссылке. cv time ___ meta признана экстремистской организацией, а facebook и instagram запрещены на территории рф 🏆 итоги года: самые популярные посты в cv time 2024-й на финишной прямой! потихоньку настраиваемся на подведение итогов и освежаем в памяти самые популярные посты уходящего года. уверены, это будет почти так же уютно, как пересматривать «один дома» (только про компьютерное зрение). minimalist vision with freeform pixels алиса родионова разобрала статью с eccv-24, получившую награду best paper award. авторы создали прототип автономной по электропитанию камеры. вместо обычной матрицы в ней установлены 24 фотодиода с масками-фильтрами, которые выполняют роль первого слоя нейросети. такая архитектура позволяет адаптировать камеру под разные задачи и получать результаты с помощью всего 8 пикселей. в обзоре — подробнее о том, как устроено решение. интересное с eccv 2024 ещё один пост с eccv-24. дарья виноградова, александр устюжанин и сергей кастрюлин отобрали самые интересные доклады первого дня конференции. в их числе «механизм реалистичности» для сокращения доменного сдвига в 3d-рендеринге лиц, подходы к улучшению реализма в text-to-3d и image-to-3d моделях, а также концепция разделения knowledge и memory в нейросетях. как llama 3.1 работает с изображениями роман исаченко объяснил, как устроена мультимодальная llama 3.1: какие архитектурные решения в основе, на каких данных её обучали и как она показывает себя на бенчмарках. он подметил несколько интересных трюков. среди них — подмена весов на этапе файнтюна через hot-swap и дополнительный пост-претрейн на датасете с редкими скриншотами и таблицами. dart: denoising autoregressive transformer for scalable text-to-image generation, часть 2 александр шишеня в двух постах рассказал, как устроена dart, диффузионная авторегрессионная модель для генерации изображений. в разборе вы найдёте детали о полезных модификациях, вроде dart-ar с ускорением обучения и dart-fm с flow matching для повышения качества генераций. вторая часть оказалась ещё популярнее первой, — видимо, вам хотелось поскорее узнать развязку. pyramidal flow matching for efficient video generative modeling александр маркович разобрал, как sora-like модели создают видео. авторы статьи предлагают отказаться от высокого разрешения ради скорости обучения и инференса. интересная часть — пирамидальный подход: вместо многослойной генерации видео с постепенным апскейлом, как обычно, моделируют все разрешения сразу. впечатления от eccv 2024 мы попросили инженеров яндекса подвести личные итоги конференции и рассказать, чем она запомнилась. в посте собрали рекомендации свежих работ по cv и любопытные заметки на полях. среди трендов заметили фокус на исследованиях за пределами области text-to-image и спад интереса к узким темам. все детали — на наших красочных карточках. movie gen: a cast of media foundation models денис кузнеделев взял оригинальный технический отчёт проекта и разобрал решения, использованные в моделях для генерации видео movie gen. некоторые идеи оказались нетривиальными, например, регуляризация при обучении сети-автокодировщика, архитектура для стабилизации и масштабирования обучения, специальное расписание диффузии. больше подробностей — по ссылке. cv time ___ meta признана экстремистской организацией, а facebook и instagram запрещены на территории рф">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-12-27T08:00:45+00:00" href="./posts/54.html">2024-12-27 08:00 UTC</a></div>
      </div>
      <div class="post-body"><tg-emoji emoji-id="5341761825170017804">🏆</tg-emoji> <strong>Итоги года: самые популярные посты в CV Time</strong><br><br>2024-й на финишной прямой! Потихоньку настраиваемся на подведение итогов и освежаем в памяти самые популярные посты уходящего года. Уверены, это будет почти так же уютно, как пересматривать «Один дома» (только про компьютерное зрение).<br><br><a href="https://t.me/timeforcv/23" rel="nofollow noopener noreferrer"><strong>Minimalist Vision with Freeform Pixels</strong></a><br>Алиса Родионова разобрала статью с ECCV-24, получившую награду Best Paper Award. Авторы создали прототип автономной по электропитанию камеры. Вместо обычной матрицы в ней установлены 24 фотодиода с масками-фильтрами, которые выполняют роль первого слоя нейросети. Такая архитектура позволяет адаптировать камеру под разные задачи и получать результаты с помощью всего 8 пикселей. В обзоре — подробнее о том, как устроено решение. <br><br><a href="https://t.me/timeforcv/6" rel="nofollow noopener noreferrer"><strong>Интересное с ECCV 2024 </strong></a><br>Ещё один пост с ECCV-24. Дарья Виноградова, Александр Устюжанин и Сергей Кастрюлин отобрали самые интересные доклады первого дня конференции. В их числе «механизм реалистичности» для сокращения доменного сдвига в 3D-рендеринге лиц, подходы к улучшению реализма в text-to-3D и image-to-3D моделях, а также концепция разделения Knowledge и Memory в нейросетях.<br><br><a href="https://t.me/timeforcv/25" rel="nofollow noopener noreferrer"><strong>Как LLaMA 3.1 работает с изображениями</strong></a><br>Роман Исаченко объяснил, как устроена мультимодальная LLaMA 3.1: какие архитектурные решения в основе, на каких данных её обучали и как она показывает себя на бенчмарках. Он подметил несколько интересных трюков. Среди них — подмена весов на этапе файнтюна через hot-swap и дополнительный пост-претрейн на датасете с редкими скриншотами и таблицами.<br><br><a href="https://t.me/timeforcv/36" rel="nofollow noopener noreferrer"><strong>DART: Denoising Autoregressive Transformer for Scalable Text-to-Image Generation, часть 2</strong></a><br>Александр Шишеня в двух постах рассказал, как устроена DART, диффузионная авторегрессионная модель для генерации изображений. В разборе вы найдёте детали о полезных модификациях, вроде DART-AR с ускорением обучения и DART-FM с Flow Matching для повышения качества генераций. Вторая часть оказалась ещё популярнее первой, — видимо, вам хотелось поскорее узнать развязку. <br><br><a href="https://t.me/timeforcv/37" rel="nofollow noopener noreferrer"><strong>Pyramidal Flow Matching for Efficient Video Generative Modeling </strong></a><br>Александр Маркович разобрал, как Sora-like модели создают видео. Авторы статьи предлагают отказаться от высокого разрешения ради скорости обучения и инференса. Интересная часть — пирамидальный подход: вместо многослойной генерации видео с постепенным апскейлом, как обычно, моделируют все разрешения сразу. <br><br><a href="https://t.me/timeforcv/28" rel="nofollow noopener noreferrer"><strong>Впечатления от ECCV 2024</strong></a><br>Мы попросили инженеров Яндекса подвести личные итоги конференции и рассказать, чем она запомнилась. В посте собрали рекомендации свежих работ по CV и любопытные заметки на полях. Среди трендов заметили фокус на исследованиях за пределами области text-to-image и спад интереса к узким темам. Все детали — на наших красочных карточках.<br><br><a href="https://t.me/timeforcv/24" rel="nofollow noopener noreferrer"><strong>Movie Gen: A Cast of Media Foundation Models</strong></a><br>Денис Кузнеделев взял оригинальный технический отчёт проекта и разобрал решения, использованные в моделях для генерации видео Movie Gen. Некоторые идеи оказались нетривиальными, например, регуляризация при обучении сети-автокодировщика, архитектура для стабилизации и масштабирования обучения, специальное расписание диффузии. Больше подробностей — по ссылке.<br><br><a href="https://t.me/+jA1a8SVHTxIzNmNi" rel="nofollow noopener noreferrer">CV Time</a><br>___<br><em>Meta признана экстремистской организацией, а Facebook и Instagram запрещены на территории РФ</em></div>
      <div class="actions">
        <span>2 461 просмотров · 19 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/54" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/54.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="53" data-search="llava-cot: let vision language models reason step-by-step сегодня разберём статью о подходе к обучению и инференсу vlm, вдохновлённом o1-preview от openai. авторы начали со сбора 100 тысяч сэмплов из открытых vqa-бенчмарков (и пообещали выложить получившийся датасет!). потом для этих сэмплов с помощью gpt-4o сгенерировали cot-синтетику со следующими блоками: ⚪summary — развернутое описание решаемой проблемы; ⚪caption — описание изображения с учетом деталей, релевантных задаче; ⚪reasoning — step-by-step решение задачи; ⚪conclusion — финальный ответ. после на этих данных сделали full-finetune поверх llama-3.2-11b-vision-instruct (кстати, всего на восьми h100). уже на этом этапе модель стала заметно умнее своего бейзлайна: 56,6 → 63,5 средних попугаев. но авторы выбили еще полтора попугая за счет собственного inference-time скейлинга: stage level beam search. по сути, это обычный bs. только ветвление происходит на уровне целых блоков cot, а не на уровне отдельных предложений. по замерам авторов, их модель в максимальном сетапе обходит gemini-1.5-pro и приближается к claude3.5-sonnet (см. табличку). до gpt-4o, правда, еще далековато. обзор подготовил ❣ павел штыков cv time llava-cot: let vision language models reason step-by-step сегодня разберём статью о подходе к обучению и инференсу vlm, вдохновлённом o1-preview от openai. авторы начали со сбора 100 тысяч сэмплов из открытых vqa-бенчмарков (и пообещали выложить получившийся датасет!). потом для этих сэмплов с помощью gpt-4o сгенерировали cot-синтетику со следующими блоками: ⚪ summary — развернутое описание решаемой проблемы; ⚪ caption — описание изображения с учетом деталей, релевантных задаче; ⚪ reasoning — step-by-step решение задачи; ⚪ conclusion — финальный ответ. после на этих данных сделали full-finetune поверх llama-3.2-11b-vision-instruct (кстати, всего на восьми h100). уже на этом этапе модель стала заметно умнее своего бейзлайна: 56,6 → 63,5 средних попугаев. но авторы выбили еще полтора попугая за счет собственного inference-time скейлинга: stage level beam search. по сути, это обычный bs. только ветвление происходит на уровне целых блоков cot, а не на уровне отдельных предложений. по замерам авторов, их модель в максимальном сетапе обходит gemini-1.5-pro и приближается к claude3.5-sonnet (см. табличку). до gpt-4o, правда, еще далековато. обзор подготовил ❣ павел штыков cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-12-25T07:47:11+00:00" href="./posts/53.html">2024-12-25 07:47 UTC</a></div>
      </div>
      <div class="post-body"><strong>LLaVa-CoT: Let Vision Language Models Reason Step-by-Step</strong><br><br>Сегодня разберём <a href="https://huggingface.co/papers/2411.10440" rel="nofollow noopener noreferrer">статью</a> о подходе к обучению и инференсу VLM, вдохновлённом o1-preview от OpenAI.<br><br>Авторы начали со сбора 100 тысяч сэмплов из открытых VQA-бенчмарков (и пообещали выложить получившийся датасет!). Потом для этих сэмплов с помощью GPT-4o сгенерировали CoT-синтетику со следующими блоками: <br><br><tg-emoji emoji-id="5341470192595653209">⚪</tg-emoji>Summary — развернутое описание решаемой проблемы; <br><tg-emoji emoji-id="5341470192595653209">⚪</tg-emoji>Caption — описание изображения с учетом деталей, релевантных задаче;<br><tg-emoji emoji-id="5341470192595653209">⚪</tg-emoji>Reasoning — step-by-step решение задачи;<br><tg-emoji emoji-id="5341470192595653209">⚪</tg-emoji>Conclusion — финальный ответ.<br><br>После на этих данных сделали full-finetune поверх Llama-3.2-11B-Vision-Instruct (кстати, всего на восьми H100). <br><br>Уже на этом этапе модель стала заметно умнее своего бейзлайна: 56,6 → 63,5 средних попугаев. Но авторы выбили еще полтора попугая за счет собственного inference-time скейлинга: Stage level Beam Search. По сути, это обычный BS. Только ветвление происходит на уровне целых блоков CoT, а не на уровне отдельных предложений.<br><br>По замерам авторов, их модель в максимальном сетапе обходит Gemini-1.5-Pro и приближается к Claude3.5-Sonnet (см. табличку). До GPT-4o, правда, еще далековато.<br><br><em>Обзор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Павел Штыков</em><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/53_480.webp" srcset="../assets/media/thumbs/53_480.webp 480w, ../assets/media/53.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="53" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 185 просмотров · 21 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/53" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/53.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="45" data-search="роботы — круто а статьи о робототехнике — ещё круче. руководитель группы нейросетевых технологий yandex.robotics виктор юрченко рекомендует семь интересных работ по теме. все подробности — в карточках. cv time роботы — круто а статьи о робототехнике — ещё круче. руководитель группы нейросетевых технологий yandex.robotics виктор юрченко рекомендует семь интересных работ по теме. все подробности — в карточках. cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-12-18T10:53:41+00:00" href="./posts/45.html">2024-12-18 10:53 UTC</a></div>
      </div>
      <div class="post-body"><strong>Роботы — круто </strong><br><br>А статьи о робототехнике — ещё круче. Руководитель группы нейросетевых технологий Yandex.Robotics Виктор Юрченко рекомендует семь интересных работ по теме. Все подробности — в карточках.<br><br><a href="https://t.me/+ig6eNLS5BtplNTZi" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/45_480.webp" srcset="../assets/media/thumbs/45_480.webp 480w, ../assets/media/45.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="45" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/46_480.webp" srcset="../assets/media/thumbs/46_480.webp 480w, ../assets/media/46.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="45" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/47_480.webp" srcset="../assets/media/thumbs/47_480.webp 480w, ../assets/media/47.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="45" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/48_480.webp" srcset="../assets/media/thumbs/48_480.webp 480w, ../assets/media/48.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="45" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/49_480.webp" srcset="../assets/media/thumbs/49_480.webp 480w, ../assets/media/49.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="45" data-image-index="4" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/50_480.webp" srcset="../assets/media/thumbs/50_480.webp 480w, ../assets/media/50.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="45" data-image-index="5" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/51_480.webp" srcset="../assets/media/thumbs/51_480.webp 480w, ../assets/media/51.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="45" data-image-index="6" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/52_480.webp" srcset="../assets/media/thumbs/52_480.webp 480w, ../assets/media/52.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="45" data-image-index="7" /></div></div>
      <div class="actions">
        <span>6 671 просмотров · 38 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/45" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/45.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="43" data-search="тематическая подборка статей: картиночные модели сегодня у нас новая подборка с рекомендациями нескольких интересных статей. в центре внимания — модели для работы с изображениями и статьи, где авторы предлагают идеи для их развития и улучшения в разных аспектах. архитектура moh: multi-head attention as mixture-of-head attention авторы переносят mixture-of-experts в аттеншн: предлагают относиться к головам как к экспертам и активировать только часть из них. показывают, что так можно дотюнить обученные модели и что этот подход работает для широкого класса задач. на vit и llama3 получили неплохие результаты, но непонятно, можно ли теоретический профит перенести на практику. дообучение locality alignment improves vision-language models ресерчеры предлагают использовать схему с маскированием для дообучения clip-трансформеров, которое улучшает их качество как vlm-бэкбонов на пространственных задачах. when does perceptual alignment benefit vision representations? исследователи файнтюнят через lora бэкбоны на датасете nights (20 тысяч триплетов с разметкой, где отражено, какое из двух изображений более похоже на первое с т. з. человека) и пишут, что после этого модели извлекают более хорошие фичи для широкого класса задач. удивляет, что в 20 тысячах триплетов оказалось достаточно информации, чтобы заметно повлиять на качество модели. дистилляция tas: distilling arbitrary teacher and student via a hybrid assistant авторы говорят, что при дистилляции между моделями разных семейств (cnn/mlp/vit) можно использовать модель-ассистента, состоящую из блоков разного типа. это позволяет повысить гибкость и расширить потенциал дистилляции знаний в случае, если у модели-учителя и ученика разные архитектуры. датасеты worldcuisines: a massive-scale benchmark for multilingual and multicultural visual question answering on global cuisines датасет с блюдами разных стран и культур, 6 тысяч изображений. очень много авторов для такой узкой темы и разнообразная аннотация как бонус. подборку подготовил ❣ артём конев cv time тематическая подборка статей: картиночные модели сегодня у нас новая подборка с рекомендациями нескольких интересных статей. в центре внимания — модели для работы с изображениями и статьи, где авторы предлагают идеи для их развития и улучшения в разных аспектах. архитектура moh: multi-head attention as mixture-of-head attention авторы переносят mixture-of-experts в аттеншн: предлагают относиться к головам как к экспертам и активировать только часть из них. показывают, что так можно дотюнить обученные модели и что этот подход работает для широкого класса задач. на vit и llama3 получили неплохие результаты, но непонятно, можно ли теоретический профит перенести на практику. дообучение locality alignment improves vision-language models ресерчеры предлагают использовать схему с маскированием для дообучения clip-трансформеров, которое улучшает их качество как vlm-бэкбонов на пространственных задачах. when does perceptual alignment benefit vision representations? исследователи файнтюнят через lora бэкбоны на датасете nights (20 тысяч триплетов с разметкой, где отражено, какое из двух изображений более похоже на первое с т. з. человека) и пишут, что после этого модели извлекают более хорошие фичи для широкого класса задач. удивляет, что в 20 тысячах триплетов оказалось достаточно информации, чтобы заметно повлиять на качество модели. дистилляция tas: distilling arbitrary teacher and student via a hybrid assistant авторы говорят, что при дистилляции между моделями разных семейств (cnn/mlp/vit) можно использовать модель-ассистента, состоящую из блоков разного типа. это позволяет повысить гибкость и расширить потенциал дистилляции знаний в случае, если у модели-учителя и ученика разные архитектуры. датасеты worldcuisines: a massive-scale benchmark for multilingual and multicultural visual question answering on global cuisines датасет с блюдами разных стран и культур, 6 тысяч изображений. очень много авторов для такой узкой темы и разнообразная аннотация как бонус. подборку подготовил ❣ артём конев cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-12-13T12:32:24+00:00" href="./posts/43.html">2024-12-13 12:32 UTC</a></div>
      </div>
      <div class="post-body"><strong>Тематическая подборка статей: картиночные модели</strong><br><br>Сегодня у нас новая подборка с рекомендациями нескольких интересных статей. В центре внимания — модели для работы с изображениями и статьи, где авторы предлагают идеи для их развития и улучшения в разных аспектах.<br><br><strong>Архитектура</strong><br><a href="https://arxiv.org/abs/2410.11842" rel="nofollow noopener noreferrer">MoH: Multi-Head Attention as Mixture-of-Head Attention</a><br>Авторы переносят Mixture-of-Experts в аттеншн: предлагают относиться к головам как к экспертам и активировать только часть из них. Показывают, что так можно дотюнить обученные модели и что этот подход работает для широкого класса задач. На ViT и Llama3 получили неплохие результаты, но непонятно, можно ли теоретический профит перенести на практику.<br> <br><strong>Дообучение</strong><br><a href="https://arxiv.org/abs/2410.11087" rel="nofollow noopener noreferrer">Locality Alignment Improves Vision-Language Models</a><br>Ресерчеры предлагают использовать схему с маскированием для дообучения CLIP-трансформеров, которое улучшает их качество как VLM-бэкбонов на пространственных задачах.<br> <br><a href="https://arxiv.org/abs/2410.10817" rel="nofollow noopener noreferrer">When Does Perceptual Alignment Benefit Vision Representations?</a><br>Исследователи файнтюнят через LoRA бэкбоны на датасете Nights (20 тысяч триплетов с разметкой, где отражено, какое из двух изображений более похоже на первое с т. з. человека) и пишут, что после этого модели извлекают более хорошие фичи для широкого класса задач. Удивляет, что в 20 тысячах триплетов оказалось достаточно информации, чтобы заметно повлиять на качество модели.<br> <br><strong>Дистилляция</strong><br><a href="https://arxiv.org/abs/2410.12342" rel="nofollow noopener noreferrer">TAS: Distilling Arbitrary Teacher and Student via a Hybrid Assistant</a><br>Авторы говорят, что при дистилляции между моделями разных семейств (CNN/MLP/ViT) можно использовать модель-ассистента, состоящую из блоков разного типа. Это позволяет повысить гибкость и расширить потенциал дистилляции знаний в случае, если у модели-учителя и ученика разные архитектуры.<br> <br><strong>Датасеты</strong><br><a href="https://arxiv.org/abs/2410.12705" rel="nofollow noopener noreferrer">WorldCuisines: A Massive-Scale Benchmark for Multilingual and Multicultural Visual Question Answering on Global Cuisines</a><br>Датасет с блюдами разных стран и культур, 6 тысяч изображений. Очень много авторов для такой узкой темы и разнообразная аннотация как бонус.<br><br><em>Подборку подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Артём Конев</em><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a></div>
      <div class="actions">
        <span>2 678 просмотров · 23 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/43" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/43.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="42" data-search="switti: designing scale-wise transformers for text-to-image synthesis часть 3, эксперименты и результаты мы обучили 2.5b модель на внутреннем датасете из 100м картинок. в качестве бейзлайнов взяли сравнимые по размеру sd3-medium, lumina-next, stable diffusion xl и её дистиллированные версии: sdxl-turbo, dmd2; а также авторегрессионные модели: emu3, lumina-mgpt, llamagen и hart. для оценки качества использовали стандартные метрики: fid, clip, pickscore, image reward, — а также бенчмарк geneval и пользовательские предпочтения на корзинке из 128 запросов (parti prompts). юзеры оценивали релевантность, эстетичность, комплексность и дефектность изображений. switti значительно превзошла существующие авторегрессионные подходы, как по метрикам, так и по пользовательским предпочтениям. с диффузионками добились паритета по качеству, но при этом switti генерирует в 7 раз быстрее оригинальной sdxl-модели и в 2 раза быстрее её ускоренных версий. отметим, что это пока лишь шаг в развитии новой генеративной парадигмы и ещё есть, куда расти, чтобы дотянуть качество до уровня ведущих генеративных моделей: yaart, midjourney, flux, recraft и ideogram-v2. наша команда уже приступает к дальнейшему развитию switti. так что следите за обновлениями! обзор подготовил ❣ дмитрий баранчук cv time switti: designing scale-wise transformers for text-to-image synthesis часть 3, эксперименты и результаты мы обучили 2.5b модель на внутреннем датасете из 100м картинок. в качестве бейзлайнов взяли сравнимые по размеру sd3-medium, lumina-next, stable diffusion xl и её дистиллированные версии: sdxl-turbo, dmd2; а также авторегрессионные модели: emu3, lumina-mgpt, llamagen и hart. для оценки качества использовали стандартные метрики: fid, clip, pickscore, image reward, — а также бенчмарк geneval и пользовательские предпочтения на корзинке из 128 запросов (parti prompts). юзеры оценивали релевантность, эстетичность, комплексность и дефектность изображений. switti значительно превзошла существующие авторегрессионные подходы, как по метрикам, так и по пользовательским предпочтениям. с диффузионками добились паритета по качеству, но при этом switti генерирует в 7 раз быстрее оригинальной sdxl-модели и в 2 раза быстрее её ускоренных версий. отметим, что это пока лишь шаг в развитии новой генеративной парадигмы и ещё есть, куда расти, чтобы дотянуть качество до уровня ведущих генеративных моделей: yaart, midjourney, flux, recraft и ideogram-v2. наша команда уже приступает к дальнейшему развитию switti . так что следите за обновлениями! обзор подготовил ❣ дмитрий баранчук cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-12-06T12:32:01+00:00" href="./posts/42.html">2024-12-06 12:32 UTC</a></div>
      </div>
      <div class="post-body"><strong>Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis</strong><br><em>Часть 3, эксперименты и результаты</em><br><br>Мы обучили 2.5B модель на внутреннем датасете из 100М картинок. В качестве бейзлайнов взяли сравнимые по размеру SD3-Medium, Lumina-Next, Stable Diffusion XL и её дистиллированные версии: SDXL-Turbo, DMD2; а также авторегрессионные модели: Emu3, Lumina-mGPT, LlamaGen и HART. <br><br>Для оценки качества использовали стандартные метрики: FID, CLIP, Pickscore, Image Reward, — а также бенчмарк GenEval и пользовательские предпочтения на корзинке из 128 запросов (Parti Prompts). Юзеры оценивали релевантность, эстетичность, комплексность и дефектность изображений.<br><br><a href="https://huggingface.co/spaces/dbaranchuk/Switti" rel="nofollow noopener noreferrer">Switti</a> значительно превзошла существующие авторегрессионные подходы, как по метрикам, так и по пользовательским предпочтениям. С диффузионками добились паритета по качеству, но при этом Switti генерирует в 7 раз быстрее оригинальной SDXL-модели и в 2 раза быстрее её ускоренных версий.<br><br>Отметим, что это пока лишь шаг в развитии новой генеративной парадигмы и ещё есть, куда расти, чтобы дотянуть качество до уровня ведущих генеративных моделей: YaART, Midjourney, FLUX, Recraft и Ideogram-v2. Наша команда уже приступает к дальнейшему развитию <a href="https://yandex-research.github.io/switti/" rel="nofollow noopener noreferrer">Switti</a>. Так что следите за обновлениями!<br><em><br>Обзор подготовил </em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> <em>Дмитрий Баранчук</em><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/42_480.webp" srcset="../assets/media/thumbs/42_480.webp 480w, ../assets/media/42.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="42" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 354 просмотров · 37 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/42" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/42.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="41" data-search="switti: designing scale-wise transformers for text-to-image synthesis часть 2, методы приступив к созданию switti (scale-wise transformer for text-to-image), мы взяли за основу архитектуру star и начали постепенно её улучшать. оригинальная модель оказалась нестабильной при обучении, и, мотивируясь работами по диффузионным трансформерам, мы добавили в модель дополнительные нормализационные слои, что стабилизировало обучение и улучшило итоговое качество. затем мы обратили внимание, что модель на входе на текущем уровне уже получает информацию о всех предыдущих уровнях по построению и при этом дополнительно смотрит на них с помощью attention-слоев, что показалось нам лишним и неэффективным. убрав авторегрессию на прошлые разрешения, удалось ускорить пайплайн генерации для 512х512 изображений на 11%, уменьшить потребление памяти и немного улучшить качество генерации. также мы заметили, что модель слабо опирается на текстовые описания на самых последних уровнях генерации. это натолкнуло на мысль, что можно не использовать технику classifier-free-guidance (cfg) на уровнях высокого разрешения. напомним, что cfg играет важную для повышения качества генерации и соответствия запросу в text-to-image моделях, но при этом требует дополнительный прогон модели на каждом шаге. поэтому отключение cfg на последних уровнях значительно ускоряет генерацию. более того, мы заметили, что помимо скорости, модель также продуцирует меньше артефактов при генерации мелких деталей и позволяет использовать более высокие значения cfg без ущерба качеству. обзор подготовил ❣ дмитрий баранчук cv time switti: designing scale-wise transformers for text-to-image synthesis часть 2, методы приступив к созданию switti (scale-wise transformer for text-to-image) , мы взяли за основу архитектуру star и начали постепенно её улучшать. оригинальная модель оказалась нестабильной при обучении, и, мотивируясь работами по диффузионным трансформерам, мы добавили в модель дополнительные нормализационные слои, что стабилизировало обучение и улучшило итоговое качество. затем мы обратили внимание, что модель на входе на текущем уровне уже получает информацию о всех предыдущих уровнях по построению и при этом дополнительно смотрит на них с помощью attention-слоев, что показалось нам лишним и неэффективным. убрав авторегрессию на прошлые разрешения, удалось ускорить пайплайн генерации для 512х512 изображений на 11%, уменьшить потребление памяти и немного улучшить качество генерации. также мы заметили, что модель слабо опирается на текстовые описания на самых последних уровнях генерации. это натолкнуло на мысль, что можно не использовать технику classifier-free-guidance (cfg) на уровнях высокого разрешения. напомним, что cfg играет важную для повышения качества генерации и соответствия запросу в text-to-image моделях, но при этом требует дополнительный прогон модели на каждом шаге. поэтому отключение cfg на последних уровнях значительно ускоряет генерацию. более того, мы заметили, что помимо скорости, модель также продуцирует меньше артефактов при генерации мелких деталей и позволяет использовать более высокие значения cfg без ущерба качеству. обзор подготовил ❣ дмитрий баранчук cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-12-06T12:31:01+00:00" href="./posts/41.html">2024-12-06 12:31 UTC</a></div>
      </div>
      <div class="post-body"><strong>Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis</strong><br><em>Часть 2, методы</em><br><br>Приступив к созданию <a href="https://huggingface.co/papers/2412.01819" rel="nofollow noopener noreferrer">Switti (Scale-wise transformer for text-to-image)</a>, мы взяли за основу архитектуру STAR и начали постепенно её улучшать. Оригинальная модель оказалась нестабильной при обучении, и, мотивируясь работами по диффузионным трансформерам, мы добавили в модель дополнительные нормализационные слои, что стабилизировало обучение и улучшило итоговое качество. <br><br>Затем мы обратили внимание, что модель на входе на текущем уровне уже получает информацию о всех предыдущих уровнях по построению и при этом дополнительно смотрит на них с помощью attention-слоев, что показалось нам лишним и неэффективным. Убрав авторегрессию на прошлые разрешения, удалось ускорить пайплайн генерации для 512х512 изображений на 11%, уменьшить потребление памяти и немного улучшить качество генерации.<br><br>Также мы заметили, что модель слабо опирается на текстовые описания на самых последних уровнях генерации. Это натолкнуло на мысль, что можно не использовать технику classifier-free-guidance (CFG) на уровнях высокого разрешения. Напомним, что CFG играет важную для повышения качества генерации и соответствия запросу в text-to-image моделях, но при этом требует дополнительный прогон модели на каждом шаге. Поэтому отключение CFG на последних уровнях значительно ускоряет генерацию. Более того, мы заметили, что помимо скорости, модель также продуцирует меньше артефактов при генерации мелких деталей и позволяет использовать более высокие значения CFG без ущерба качеству.<br><br><em>Обзор подготовил </em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> <em>Дмитрий Баранчук</em><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/41_480.webp" srcset="../assets/media/thumbs/41_480.webp 480w, ../assets/media/41.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="41" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 806 просмотров · 26 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/41" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/41.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="40" data-search="switti: designing scale-wise transformers for text-to-image synthesis часть 1, предыстория и предшественники диффузионные модели уверенно занимают пьедестал почета в задаче генерации изображений по тексту. но существует альтернативная парадигма — авторегрессионные модели, которые генерируют изображения последовательно патч за патчем (маленькими кусочками, скажем, 16x16 пикселей). на практике они работают медленнее и уступают в качестве диффузионкам, поэтому долгое время считались неконкурентоспособными. весной исследователи из bytedance в работе var предложили альтернативную авторегрессионную парадигму для генерации изображений: из одного пикселя 1х1 генерируем картинку 2х2, потом 4х4 и так далее, причем каждое следующее разрешение предсказывается за один проход модели. в пиксельном пространстве работать все еще дорого, поэтому переходим в латентное пространство vae, где с помощью метода residual quantization (rq), представляем латентную переменную в виде пирамидки: нижние уровни соответствуют общей семантике, а верхние — мелким деталям и текстурам. на нижних уровнях мало патчей, поэтому прогон модели дешевле. стоимость возрастает с переходом на следующие уровни. во время генерации модель смотрит на прошлые разрешения с помощью causal трансформера и генерирует текущее. итоговую картинку получают, суммируя все предсказанные разрешения. авторы добились качества, сопоставимого с современными диффузионками на imagenet, при этом будучи значительно их быстрее. но генерация из фиксированного набора (1000 классов) не так интересна, как генерация по произвольным текстовым запросам. поэтому очевидный шаг — перенести идею в более прикладной сценарий. так появился star, который адаптировал подход для генерации изображений по тексту, но саму модель авторы так и не выложили. поэтому мы решили обучить свою генеративную модель и опубликовать её в открытом доступе, чтобы стимулировать дальнейшее развитие парадигмы. обзор подготовил ❣ дмитрий баранчук cv time switti: designing scale-wise transformers for text-to-image synthesis часть 1, предыстория и предшественники диффузионные модели уверенно занимают пьедестал почета в задаче генерации изображений по тексту. но существует альтернативная парадигма — авторегрессионные модели, которые генерируют изображения последовательно патч за патчем (маленькими кусочками, скажем, 16x16 пикселей). на практике они работают медленнее и уступают в качестве диффузионкам, поэтому долгое время считались неконкурентоспособными. весной исследователи из bytedance в работе var предложили альтернативную авторегрессионную парадигму для генерации изображений: из одного пикселя 1х1 генерируем картинку 2х2, потом 4х4 и так далее, причем каждое следующее разрешение предсказывается за один проход модели. в пиксельном пространстве работать все еще дорого, поэтому переходим в латентное пространство vae, где с помощью метода residual quantization (rq), представляем латентную переменную в виде пирамидки: нижние уровни соответствуют общей семантике, а верхние — мелким деталям и текстурам. на нижних уровнях мало патчей, поэтому прогон модели дешевле. стоимость возрастает с переходом на следующие уровни. во время генерации модель смотрит на прошлые разрешения с помощью causal трансформера и генерирует текущее. итоговую картинку получают, суммируя все предсказанные разрешения. авторы добились качества, сопоставимого с современными диффузионками на imagenet, при этом будучи значительно их быстрее. но генерация из фиксированного набора (1000 классов) не так интересна, как генерация по произвольным текстовым запросам. поэтому очевидный шаг — перенести идею в более прикладной сценарий. так появился star , который адаптировал подход для генерации изображений по тексту, но саму модель авторы так и не выложили. поэтому мы решили обучить свою генеративную модель и опубликовать её в открытом доступе, чтобы стимулировать дальнейшее развитие парадигмы. обзор подготовил ❣ дмитрий баранчук cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-12-06T12:30:28+00:00" href="./posts/40.html">2024-12-06 12:30 UTC</a></div>
      </div>
      <div class="post-body"><strong>Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis</strong><br><em>Часть 1, предыстория и предшественники</em><br><br>Диффузионные модели уверенно занимают пьедестал почета в задаче генерации изображений по тексту. Но существует альтернативная парадигма — авторегрессионные модели, которые генерируют изображения последовательно патч за патчем (маленькими кусочками, скажем, 16x16 пикселей). На практике они работают медленнее и уступают в качестве диффузионкам, поэтому долгое время считались неконкурентоспособными.<br><br>Весной исследователи из ByteDance в работе <a href="https://arxiv.org/abs/2404.02905" rel="nofollow noopener noreferrer">VAR</a> предложили альтернативную авторегрессионную парадигму для генерации изображений: из одного пикселя 1х1 генерируем картинку 2х2, потом 4х4 и так далее, причем каждое следующее разрешение предсказывается за один проход модели. В пиксельном пространстве работать все еще дорого, поэтому переходим в латентное пространство VAE, где с помощью метода Residual Quantization (RQ), представляем латентную переменную в виде пирамидки: нижние уровни соответствуют общей семантике, а верхние — мелким деталям и текстурам. <br><br>На нижних уровнях мало патчей, поэтому прогон модели дешевле. Стоимость возрастает с переходом на следующие уровни. Во время генерации модель смотрит на прошлые разрешения с помощью causal трансформера и генерирует текущее. Итоговую картинку получают, суммируя все предсказанные разрешения. Авторы добились качества, сопоставимого с современными диффузионками на ImageNet, при этом будучи значительно их быстрее.  <br><br>Но генерация из фиксированного набора (1000 классов) не так интересна, как генерация по произвольным текстовым запросам. Поэтому очевидный шаг — перенести идею в более прикладной сценарий. Так появился <a href="https://arxiv.org/html/2406.10797v1" rel="nofollow noopener noreferrer">STAR</a>, который адаптировал подход для генерации изображений по тексту, но саму модель авторы так и не выложили. Поэтому мы решили обучить свою генеративную модель и опубликовать её в открытом доступе, чтобы стимулировать дальнейшее развитие парадигмы. <br><br><em>Обзор подготовил </em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> <em>Дмитрий Баранчук</em><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/40_480.webp" srcset="../assets/media/thumbs/40_480.webp 480w, ../assets/media/40.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="40" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 755 просмотров · 25 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/40" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/40.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="39" data-search="тематическая подборка статей: галлюцинации vlm сегодня пробуем новый формат — делимся целой подборкой интересных статей по теме. забирайте в избранное и читайте полные материалы в свободное время! а в комментариях пишите, полезен ли такой формат и хотите ли вы, чтобы наши авторы разобрали одну или несколько статей подробно. magnifier prompt: tackling multimodal hallucination via extremely simple instructions авторы получают на бенчмарке pope буст, просто добавляя в промпт явную просьбу больше внимания обращать на изображение, а в случае конфликта знаний и содержимого изображения приоритизировать именно изображение. mllm can see? dynamic correction decoding for hallucination mitigation для борьбы с галлюцинациями авторы предлагают костыльный фикс: определять, когда в средних слоях vlm уверена в ответе, и пропагейтить эти знания до финальных слоев, где они могут затираться знаниями llm. таким образом они исправляют ситуацию, когда по мере прохождения в более глубокие слои language bias начинает перевешивать токены изображения. the curse of multi-modalities: evaluating hallucinations of large multimodal models across language, visual, and audio авторы винят в галлюцинациях &quot;over reliance on unimodal priors&quot; и&quot; spurious inter-modality correlations&quot;, а на основе своих находок делают бенчмарк для диагностики vlm. trust but verify: programmatic vlm evaluation in the wild бенчмарк со сложным пайплайном для оценки галлюцинирования: берем картинки с подробными кэпшнами, строим по ним граф сцены, подаем в llm, которая возвращает вопросы-ответы и программы верификации для них. авторы утверждают, что такой подход позволяет разложить модель на helpfulness и trustfulness + делают анализ по этому разбиению. спойлер: большинство моделей оказываются хороши только по одному из этих аспектов. mitigating hallucinations in large vision-language models via summary-guided decoding авторы считают, что по мере написания кэпшна vlm все больше начинает зависеть от language prior’а, а не от изображения; при этом есть трейдофф между числом галлюцинаций и качеством текста. для борьбы с проблемой предлагают укорачивать сгенерированный текст, а также используют дополнительную llm. подборку подготовил ❣ артём конев cv time тематическая подборка статей: галлюцинации vlm сегодня пробуем новый формат — делимся целой подборкой интересных статей по теме. забирайте в избранное и читайте полные материалы в свободное время! а в комментариях пишите, полезен ли такой формат и хотите ли вы, чтобы наши авторы разобрали одну или несколько статей подробно. magnifier prompt: tackling multimodal hallucination via extremely simple instructions авторы получают на бенчмарке pope буст, просто добавляя в промпт явную просьбу больше внимания обращать на изображение, а в случае конфликта знаний и содержимого изображения приоритизировать именно изображение. mllm can see? dynamic correction decoding for hallucination mitigation для борьбы с галлюцинациями авторы предлагают костыльный фикс: определять, когда в средних слоях vlm уверена в ответе, и пропагейтить эти знания до финальных слоев, где они могут затираться знаниями llm. таким образом они исправляют ситуацию, когда по мере прохождения в более глубокие слои language bias начинает перевешивать токены изображения. the curse of multi-modalities: evaluating hallucinations of large multimodal models across language, visual, and audio авторы винят в галлюцинациях &amp;quot;over reliance on unimodal priors&amp;quot; и&amp;quot; spurious inter-modality correlations&amp;quot;, а на основе своих находок делают бенчмарк для диагностики vlm. trust but verify: programmatic vlm evaluation in the wild бенчмарк со сложным пайплайном для оценки галлюцинирования: берем картинки с подробными кэпшнами, строим по ним граф сцены, подаем в llm, которая возвращает вопросы-ответы и программы верификации для них. авторы утверждают, что такой подход позволяет разложить модель на helpfulness и trustfulness + делают анализ по этому разбиению. спойлер: большинство моделей оказываются хороши только по одному из этих аспектов. mitigating hallucinations in large vision-language models via summary-guided decoding авторы считают, что по мере написания кэпшна vlm все больше начинает зависеть от language prior’а, а не от изображения; при этом есть трейдофф между числом галлюцинаций и качеством текста. для борьбы с проблемой предлагают укорачивать сгенерированный текст, а также используют дополнительную llm. подборку подготовил ❣ артём конев cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-11-29T11:11:42+00:00" href="./posts/39.html">2024-11-29 11:11 UTC</a></div>
      </div>
      <div class="post-body"><strong>Тематическая подборка статей: галлюцинации VLM</strong><br><br>Сегодня пробуем новый формат — делимся целой подборкой интересных статей по теме. Забирайте в избранное и читайте полные материалы в свободное время! А в комментариях пишите, полезен ли такой формат и хотите ли вы, чтобы наши авторы разобрали одну или несколько статей подробно.<br> <br><a href="https://arxiv.org/abs/2410.11701" rel="nofollow noopener noreferrer">Magnifier Prompt: Tackling Multimodal Hallucination via Extremely Simple Instructions</a><br>Авторы получают на бенчмарке POPE буст, просто добавляя в промпт явную просьбу больше внимания обращать на изображение, а в случае конфликта знаний и содержимого изображения приоритизировать именно изображение.<br> <br><a href="https://arxiv.org/abs/2410.11779" rel="nofollow noopener noreferrer">MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation</a><br>Для борьбы с галлюцинациями авторы предлагают костыльный фикс: определять, когда в средних слоях VLM уверена в ответе, и пропагейтить эти знания до финальных слоев, где они могут затираться знаниями LLM. Таким образом они исправляют ситуацию, когда по мере прохождения в более глубокие слои language bias начинает перевешивать токены изображения.<br> <br><a href="https://arxiv.org/abs/2410.12787" rel="nofollow noopener noreferrer">The Curse of Multi-Modalities: Evaluating Hallucinations of Large Multimodal Models across Language, Visual, and Audio</a><br>Авторы винят в галлюцинациях &quot;Over reliance on unimodal priors&quot; и&quot; Spurious inter-modality correlations&quot;, а на основе своих находок делают бенчмарк для диагностики VLM.<br> <br><a href="https://arxiv.org/abs/2410.13121" rel="nofollow noopener noreferrer">Trust but Verify: Programmatic VLM Evaluation in the Wild</a><br>Бенчмарк со сложным пайплайном для оценки галлюцинирования: берем картинки с подробными кэпшнами, строим по ним граф сцены, подаем в LLM, которая возвращает вопросы-ответы и программы верификации для них. Авторы утверждают, что такой подход позволяет разложить модель на helpfulness и trustfulness + делают анализ по этому разбиению. Спойлер: большинство моделей оказываются хороши только по одному из этих аспектов.<br> <br><a href="https://arxiv.org/abs/2410.13321" rel="nofollow noopener noreferrer">Mitigating Hallucinations in Large Vision-Language Models via Summary-Guided Decoding</a><br>Авторы считают, что по мере написания кэпшна VLM все больше начинает зависеть от language prior’а, а не от изображения; при этом есть трейдофф между числом галлюцинаций и качеством текста. Для борьбы с проблемой предлагают укорачивать сгенерированный текст, а также используют дополнительную LLM.<br><em><br>Подборку подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Артём Конев</em><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a></div>
      <div class="actions">
        <span>2 233 просмотров · 32 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/39" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/39.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="37" data-search="pyramidal flow matching for efficient video generative modeling чтобы сгенерировать видео, свежие sora-like-модели (как на первой картинке) создают каждый кадр из чистого шума, а потом расшумляют его. но сделать видео так гораздо сложнее, чем картинку: 1. нужно смоделировать не одно изображение, а целую последовательность. то есть, если один кадр размером хотя бы 500 пикселей, а видео должно длиться 5 секунд с частотой 30 fps, то нейросеть нарисует 150 кадров по 500 пикселей — потребуются значительные вычислительные мощности. 2. sora-like-модели не авторегрессионные, а значит, не могут создать видео длиннее тех, на которых обучались. это можно обойти трюками, например, генерируя кадры «внахлëст». авторы сегодняшнего препринта предлагают поступиться качеством изображений, чтобы повысить эффективность и обучения, и инференса: на низком пространственном разрешении и так много шума, а значит, можно генерировать кадры видео менее чёткими, чем обычные картинки. а ещё их модель by design использует фреймы из прошлого и поэтому в теории может предсказывать без склеек консистентные видео любой длины. как и все модные ребята, эти предлагают обучать модель на flow-matching-лоссе: предсказывать векторное поле пикселей. то есть, пытаться угадать, куда они сдвинутся и как изменят кадр с течением времени. в чём же пирамидальность? обычно для видео используют multistage-генерацию: сначала предсказывают кадры маленького разрешения, а потом апскейлят их. в препринте избавляются от multistage: моделируют все разрешения в одном лоссе. на каждом уровне пирамиды есть своя степень зашумления, равномерно распределëнная по высоте. внутри уровня картинка расшумляется специально предсказанным для него векторным полем. на последнем уровне шум обнуляется — получается чистый кадр. шум на разных уровнях пирамиды скоррелирован, чтобы добиться одинакового probability flow на всех этапах генерации и улучшить конечный результат. авторы не приводят сравнения с диффузионным лоссом. как думаете, сошлось бы? разбор подготовил ❣ александр маркович cv time pyramidal flow matching for efficient video generative modeling чтобы сгенерировать видео, свежие sora-like-модели (как на первой картинке) создают каждый кадр из чистого шума, а потом расшумляют его. но сделать видео так гораздо сложнее, чем картинку: 1. нужно смоделировать не одно изображение, а целую последовательность. то есть, если один кадр размером хотя бы 500 пикселей, а видео должно длиться 5 секунд с частотой 30 fps, то нейросеть нарисует 150 кадров по 500 пикселей — потребуются значительные вычислительные мощности. 2. sora-like-модели не авторегрессионные, а значит, не могут создать видео длиннее тех, на которых обучались. это можно обойти трюками, например, генерируя кадры «внахлëст». авторы сегодняшнего препринта предлагают поступиться качеством изображений, чтобы повысить эффективность и обучения, и инференса: на низком пространственном разрешении и так много шума, а значит, можно генерировать кадры видео менее чёткими, чем обычные картинки. а ещё их модель by design использует фреймы из прошлого и поэтому в теории может предсказывать без склеек консистентные видео любой длины. как и все модные ребята, эти предлагают обучать модель на flow-matching-лоссе: предсказывать векторное поле пикселей. то есть, пытаться угадать, куда они сдвинутся и как изменят кадр с течением времени. в чём же пирамидальность? обычно для видео используют multistage-генерацию: сначала предсказывают кадры маленького разрешения, а потом апскейлят их. в препринте избавляются от multistage: моделируют все разрешения в одном лоссе. на каждом уровне пирамиды есть своя степень зашумления, равномерно распределëнная по высоте. внутри уровня картинка расшумляется специально предсказанным для него векторным полем. на последнем уровне шум обнуляется — получается чистый кадр. шум на разных уровнях пирамиды скоррелирован, чтобы добиться одинакового probability flow на всех этапах генерации и улучшить конечный результат. авторы не приводят сравнения с диффузионным лоссом. как думаете, сошлось бы? разбор подготовил ❣ александр маркович cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-11-22T10:45:08+00:00" href="./posts/37.html">2024-11-22 10:45 UTC</a></div>
      </div>
      <div class="post-body"><strong>Pyramidal Flow Matching for Efficient Video Generative Modeling</strong><br><br>Чтобы сгенерировать видео, свежие Sora-like-модели (как на первой картинке) создают каждый кадр из чистого шума, а потом расшумляют его. Но сделать видео так гораздо сложнее, чем картинку: <br><br>1. Нужно смоделировать не одно изображение, а целую последовательность. То есть, если один кадр размером хотя бы 500 пикселей, а видео должно длиться 5 секунд с частотой 30 FPS, то нейросеть нарисует 150 кадров по 500 пикселей — потребуются значительные вычислительные мощности. <br>2. Sora-like-модели не авторегрессионные, а значит, не могут создать видео длиннее тех, на которых обучались. Это можно обойти трюками, например, генерируя кадры «внахлëст».<br><br>Авторы сегодняшнего <a href="https://arxiv.org/abs/2410.05954" rel="nofollow noopener noreferrer">препринта</a> предлагают поступиться качеством изображений, чтобы повысить эффективность и обучения, и инференса: на низком пространственном разрешении и так много шума, а значит, можно генерировать кадры видео менее чёткими, чем обычные картинки. А ещё их модель by design использует фреймы из прошлого и поэтому в теории может предсказывать без склеек консистентные видео любой длины. <br><br>Как и все модные ребята, эти предлагают обучать модель на flow-matching-лоссе: предсказывать векторное поле пикселей. То есть, пытаться угадать, куда они сдвинутся и как изменят кадр с течением времени. <br><br>В чём же пирамидальность? Обычно для видео используют multistage-генерацию: сначала предсказывают кадры маленького разрешения, а потом апскейлят их. В препринте избавляются от multistage: моделируют все разрешения в одном лоссе. <br><br>На каждом уровне пирамиды есть своя степень зашумления, равномерно распределëнная по высоте. Внутри уровня картинка расшумляется специально предсказанным для него векторным полем. На последнем уровне шум обнуляется — получается чистый кадр. <br><br>Шум на разных уровнях пирамиды скоррелирован, чтобы добиться одинакового probability flow на всех этапах генерации и улучшить конечный результат. <br><br>Авторы не приводят сравнения с диффузионным лоссом. Как думаете, сошлось бы?<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Александр Маркович</em><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/37_480.webp" srcset="../assets/media/thumbs/37_480.webp 480w, ../assets/media/37.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="37" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 600 просмотров · 28 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/37" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/37.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="36" data-search="dart: denoising autoregressive transformer for scalable text-to-image generation, часть 2 на иллюстрации к посту изображено устройство самой наивной имплементации такой модели — dart. у неё стандартный диффузионный loss, а её единственный существенный недостаток — слишком малый объём входных данных, 4000 токенов. это накладывает ограничение на скорость обучения модели. обойти ограничение помогает модификация dart-ar. при этом один шаг обучения dart-ar занимает столько же времени, как и dart: сходится быстрее, но требует значительно больше времени на инференсе. ещё одна модификация — dart-fm, с flow matching. схема усложняется: поверх основного алгоритма dart добавляют несколько прогонов простой нейросети. эта легковесная «голова» используется на стадии инференса: для итерирования между основными шагами расшумления, чтобы повысить качество генераций. статья представляет скорее теоретический, чем практический интерес: инференс занимает слишком много времени, а для сравнения результатов авторы выбрали далеко не самые свежие модели. разбор подготовил ❣ александр шишеня cv time dart: denoising autoregressive transformer for scalable text-to-image generation, часть 2 на иллюстрации к посту изображено устройство самой наивной имплементации такой модели — dart. у неё стандартный диффузионный loss, а её единственный существенный недостаток — слишком малый объём входных данных, 4000 токенов. это накладывает ограничение на скорость обучения модели. обойти ограничение помогает модификация dart-ar. при этом один шаг обучения dart-ar занимает столько же времени, как и dart: сходится быстрее, но требует значительно больше времени на инференсе. ещё одна модификация — dart-fm, с flow matching. схема усложняется: поверх основного алгоритма dart добавляют несколько прогонов простой нейросети. эта легковесная «голова» используется на стадии инференса: для итерирования между основными шагами расшумления, чтобы повысить качество генераций. статья представляет скорее теоретический, чем практический интерес: инференс занимает слишком много времени, а для сравнения результатов авторы выбрали далеко не самые свежие модели. разбор подготовил ❣ александр шишеня cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-11-15T15:12:05+00:00" href="./posts/36.html">2024-11-15 15:12 UTC</a></div>
      </div>
      <div class="post-body"><strong>DART: Denoising Autoregressive Transformer for Scalable Text-to-Image Generation, часть 2</strong><br><br>На иллюстрации к посту изображено устройство самой наивной имплементации такой модели — DART. У неё стандартный диффузионный loss, а её единственный существенный недостаток — слишком малый объём входных данных, 4000 токенов. Это накладывает ограничение на скорость обучения модели.<br><br>Обойти ограничение помогает модификация DART-AR. При этом один шаг обучения DART-AR занимает столько же времени, как и DART: сходится быстрее, но требует значительно больше времени на инференсе.<br><br>Ещё одна модификация — DART-FM, с Flow Matching. Схема усложняется: поверх основного алгоритма DART добавляют несколько прогонов простой нейросети. Эта легковесная «голова» используется на стадии инференса: для итерирования между основными шагами расшумления, чтобы повысить качество генераций. <br><br><a href="https://arxiv.org/abs/2410.08159" rel="nofollow noopener noreferrer">Статья</a> представляет скорее теоретический, чем практический интерес: инференс занимает слишком много времени, а для сравнения результатов авторы выбрали далеко не самые свежие модели.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Александр Шишеня</em><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/36_480.webp" srcset="../assets/media/thumbs/36_480.webp 480w, ../assets/media/36.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="36" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 605 просмотров · 30 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/36" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/36.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="35" data-search="dart: denoising autoregressive transformer for scalable text-to-image generation, часть 1 превратить авторегрессионную визуальную генерацию в диффузионную можно, если соблюдать три ограничения: 1. фиксировать процесс зашумления. 2. работать в парадигме, где модель зависит только от одного предыдущего состояния, а не от целой последовательности: тогда процесс становится марковским. 3. взвешивать loss на коэффициенты, которые зависят от timestamp и наложенного шума. авторы сегодняшнего препринта предлагают ослабить второе условие: добавить зависимость от предыдущих зашумлённых изображений. такую модель они называют dart — denoising autoregressive transformer или диффузионная авторегрессионная модель. саму последовательность при этом можно генерировать по-разному: 1. на каждом шаге генерировать частично расшумлëнное изображение — такой метод называется dart. 2. на каждой стадии расшумления авторегрессионно генерировать изображение по патчам — dart-ar (дарт с авторегрессией). 3. генерировать изображения, последовательно увеличивая их размер — matryoshka-dart. 4. кроме изображения, генерировать ещë и его текстовое описание — kaleydo-dart. в качестве трансформера для генерации на основе текстового промпта используется предобученная модель flan-t5-xl, а для генерации на основе заданного класса — дополнительные слои adaptive layernorm. разбор подготовил ❣ александр шишеня cv time dart: denoising autoregressive transformer for scalable text-to-image generation, часть 1 превратить авторегрессионную визуальную генерацию в диффузионную можно, если соблюдать три ограничения: 1. фиксировать процесс зашумления. 2. работать в парадигме, где модель зависит только от одного предыдущего состояния, а не от целой последовательности: тогда процесс становится марковским. 3. взвешивать loss на коэффициенты, которые зависят от timestamp и наложенного шума. авторы сегодняшнего препринта предлагают ослабить второе условие: добавить зависимость от предыдущих зашумлённых изображений. такую модель они называют dart — denoising autoregressive transformer или диффузионная авторегрессионная модель. саму последовательность при этом можно генерировать по-разному: 1. на каждом шаге генерировать частично расшумлëнное изображение — такой метод называется dart. 2. на каждой стадии расшумления авторегрессионно генерировать изображение по патчам — dart-ar (дарт с авторегрессией). 3. генерировать изображения, последовательно увеличивая их размер — matryoshka-dart. 4. кроме изображения, генерировать ещë и его текстовое описание — kaleydo-dart. в качестве трансформера для генерации на основе текстового промпта используется предобученная модель flan-t5-xl, а для генерации на основе заданного класса — дополнительные слои adaptive layernorm. разбор подготовил ❣ александр шишеня cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-11-15T15:10:34+00:00" href="./posts/35.html">2024-11-15 15:10 UTC</a></div>
      </div>
      <div class="post-body"><strong>DART: Denoising Autoregressive Transformer for Scalable Text-to-Image Generation, часть 1</strong><br><br>Превратить авторегрессионную визуальную генерацию в диффузионную можно, если соблюдать три ограничения: <br><br>1. Фиксировать процесс зашумления. <br>2. Работать в парадигме, где модель зависит только от одного предыдущего состояния, а не от целой последовательности: тогда процесс становится Марковским. <br>3. Взвешивать loss на коэффициенты, которые зависят от timestamp и наложенного шума. <br><br>Авторы сегодняшнего <a href="https://arxiv.org/abs/2410.08159" rel="nofollow noopener noreferrer">препринта</a> предлагают ослабить второе условие: добавить зависимость от предыдущих зашумлённых изображений. Такую модель они называют DART — Denoising Autoregressive Transformer или диффузионная авторегрессионная модель.    <br><br>Саму последовательность при этом можно генерировать по-разному:<br><br>1. На каждом шаге генерировать частично расшумлëнное изображение — такой метод называется DART.<br>2. На каждой стадии расшумления авторегрессионно генерировать изображение по патчам — DART-AR (ДАРТ с авторегрессией). <br>3. Генерировать изображения, последовательно увеличивая их размер — Matryoshka-DART. <br>4. Кроме изображения, генерировать ещë и его текстовое описание — Kaleydo-DART. <br><br>В качестве трансформера для генерации на основе текстового промпта используется предобученная модель Flan-T5-XL, а для генерации на основе заданного класса — дополнительные слои Adaptive LayerNorm.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Александр Шишеня</em><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a></div>
      <div class="actions">
        <span>2 265 просмотров · 20 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/35" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/35.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="28" data-search="впечатления от eccv 2024 мы попросили инженеров яндекса подвести личные итоги конференции eccv и рассказать, чем она запомнилась. о трендах в индустрии, интересных статьях и многом другом — в наших карточках. а остальные посты по следам конференции вы можете найти в канале по тегу #yaeccv. cv time впечатления от eccv 2024 мы попросили инженеров яндекса подвести личные итоги конференции eccv и рассказать, чем она запомнилась. о трендах в индустрии, интересных статьях и многом другом — в наших карточках. а остальные посты по следам конференции вы можете найти в канале по тегу #yaeccv. cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-11-11T11:20:32+00:00" href="./posts/28.html">2024-11-11 11:20 UTC</a></div>
      </div>
      <div class="post-body"><strong>Впечатления от ECCV 2024</strong><br><br>Мы попросили инженеров Яндекса подвести личные итоги конференции ECCV и рассказать, чем она запомнилась. О трендах в индустрии, интересных статьях и многом другом — в наших карточках. <br><br>А остальные посты по следам конференции вы можете найти в канале по тегу #YaECCV.<br><br><a href="https://t.me/+PhJNR_S9gbxmNjhi" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/28_480.webp" srcset="../assets/media/thumbs/28_480.webp 480w, ../assets/media/28.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="28" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/29_480.webp" srcset="../assets/media/thumbs/29_480.webp 480w, ../assets/media/29.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="28" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/30_480.webp" srcset="../assets/media/thumbs/30_480.webp 480w, ../assets/media/30.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="28" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/31_480.webp" srcset="../assets/media/thumbs/31_480.webp 480w, ../assets/media/31.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="28" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/32_480.webp" srcset="../assets/media/thumbs/32_480.webp 480w, ../assets/media/32.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="28" data-image-index="4" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/33_480.webp" srcset="../assets/media/thumbs/33_480.webp 480w, ../assets/media/33.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="28" data-image-index="5" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/34_480.webp" srcset="../assets/media/thumbs/34_480.webp 480w, ../assets/media/34.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="28" data-image-index="6" /></div></div>
      <div class="actions">
        <span>1 985 просмотров · 38 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/28" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/28.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="25" data-search="как llama 3.1 работает с изображениями сегодня расскажем, как llama 3.1 работает с изображениями. как устроена архитектура модели, на чём её обучали и какие результаты она показывает на разных бенчмарках. в основе энкодера изображений в llama 3 выступает модель vit-h/14 с 630 миллионами параметров, обученная на наборе из 2,5 миллиарда пар изображений и текстов. картинку, полученную на вход, делят на патчи размером 16x16, прогоняют через линейное преобразование и трансформерные слои. на выходе получается картиночное представление из 2 308 токенов, что весьма много. эта последовательность с помощью кросс-аттеншен блоков подается в llm. это интересно, потому что, как правило, в мультимодальных llm (visual language models, vlm) картиночные токены подают прямо в промпт, добавляя к текстовым. это сделано для того, чтобы заморозить веса llm и обучать только голубые блоки на схеме. тем самым не забивается исходный контекст llm, а свойства модели на текстовом домене не теряются. веса языковой модели остаются замороженными, чтобы сохранить производительность на текстовых задачах, то есть обновляются только веса визуального энкодера и адаптера изображений. что касается видео, то тут меняется количество кадров на входе: на претрейне их 16, а на файнтюне — 64. далее их сводят к фиксированной размерности и точно так же, как с изображениями, добавляют через кросс-аттеншн в llm. на претрейне используют 6 миллиардов пар картинка-текст. для фильтрации датасета убирают все описания на языках, отличных от английского, и пары с низким clip-скором. дальше происходит дедупликация, чтобы исключить часто повторяющиеся картинки. в ходе ресемплирования текстовых описаний их частотность понижают. кроме того, в каждое описание добавляют распознанный на картинке текст — это нужно для улучшения результатов на задачах, которые требуют использования ocr. существует весьма необычная стадия пост-претрейна — с применением небольшого датасета на 500 миллионов сэмплов. 150 миллионов из них — это изображения вроде скриншотов с html-кодами и таблицами. на файнтюне инженеры прибегают к ещё одному нестандартному трюку — hot-swap. они подменяют веса предварительно обученной llm на веса языковой модели, прошедшей instruction tuning. на sft используют академические датасеты, ответы, написанные людьми, и синтетические данные — например, изображения, созданные из текстовых описаний. далее создают пары, оцененные асессорами по семибалльной шкале. кроме того, есть процедура rejection sampling для итеративного выбора высококачественных ответов, сгенерированных моделью. потом авторы статьи обучают reward-модель и делают dpo, как и для текстовой модели. получившаяся модель на 405 миллиардов параметров показывает 80,2 пункта в бенчмарке vqav2 и 84,8 пункта в textvqa. при этом она уступает claude 3.5 в docvqa, chartqa и ai2 diagram. разбор подготовил ❣ роман исаченко cv time как llama 3.1 работает с изображениями сегодня расскажем, как llama 3.1 работает с изображениями. как устроена архитектура модели, на чём её обучали и какие результаты она показывает на разных бенчмарках. в основе энкодера изображений в llama 3 выступает модель vit-h/14 с 630 миллионами параметров, обученная на наборе из 2,5 миллиарда пар изображений и текстов. картинку, полученную на вход, делят на патчи размером 16x16, прогоняют через линейное преобразование и трансформерные слои. на выходе получается картиночное представление из 2 308 токенов, что весьма много. эта последовательность с помощью кросс-аттеншен блоков подается в llm. это интересно, потому что, как правило, в мультимодальных llm (visual language models, vlm) картиночные токены подают прямо в промпт, добавляя к текстовым. это сделано для того, чтобы заморозить веса llm и обучать только голубые блоки на схеме. тем самым не забивается исходный контекст llm, а свойства модели на текстовом домене не теряются. веса языковой модели остаются замороженными, чтобы сохранить производительность на текстовых задачах, то есть обновляются только веса визуального энкодера и адаптера изображений. что касается видео, то тут меняется количество кадров на входе: на претрейне их 16, а на файнтюне — 64. далее их сводят к фиксированной размерности и точно так же, как с изображениями, добавляют через кросс-аттеншн в llm. на претрейне используют 6 миллиардов пар картинка-текст. для фильтрации датасета убирают все описания на языках, отличных от английского, и пары с низким clip-скором. дальше происходит дедупликация, чтобы исключить часто повторяющиеся картинки. в ходе ресемплирования текстовых описаний их частотность понижают. кроме того, в каждое описание добавляют распознанный на картинке текст — это нужно для улучшения результатов на задачах, которые требуют использования ocr. существует весьма необычная стадия пост-претрейна — с применением небольшого датасета на 500 миллионов сэмплов. 150 миллионов из них — это изображения вроде скриншотов с html-кодами и таблицами. на файнтюне инженеры прибегают к ещё одному нестандартному трюку — hot-swap. они подменяют веса предварительно обученной llm на веса языковой модели, прошедшей instruction tuning. на sft используют академические датасеты, ответы, написанные людьми, и синтетические данные — например, изображения, созданные из текстовых описаний. далее создают пары, оцененные асессорами по семибалльной шкале. кроме того, есть процедура rejection sampling для итеративного выбора высококачественных ответов, сгенерированных моделью. потом авторы статьи обучают reward-модель и делают dpo, как и для текстовой модели. получившаяся модель на 405 миллиардов параметров показывает 80,2 пункта в бенчмарке vqav2 и 84,8 пункта в textvqa. при этом она уступает claude 3.5 в docvqa, chartqa и ai2 diagram. разбор подготовил ❣ роман исаченко cv time">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-11-01T09:24:40+00:00" href="./posts/25.html">2024-11-01 09:24 UTC</a></div>
      </div>
      <div class="post-body"><strong>Как LLaMA 3.1 работает с изображениями</strong><br><br>Сегодня расскажем, <a href="https://arxiv.org/abs/2407.21783" rel="nofollow noopener noreferrer">как LLaMA 3.1 работает с изображениями.</a> Как устроена архитектура модели, на чём её обучали и какие результаты она показывает на разных бенчмарках. <br><br>В основе энкодера изображений в Llama 3 выступает модель ViT-H/14 с 630 миллионами параметров, обученная на наборе из 2,5 миллиарда пар изображений и текстов. Картинку, полученную на вход, делят на патчи размером 16X16, прогоняют через линейное преобразование и трансформерные слои. <br><br>На выходе получается картиночное представление из 2 308 токенов, что весьма много. Эта последовательность с помощью кросс-аттеншен блоков подается в LLM. Это интересно, потому что, как правило, в мультимодальных LLM (visual language models, VLM) картиночные токены подают прямо в промпт, добавляя к текстовым. Это сделано для того, чтобы заморозить веса LLM и обучать только голубые блоки на схеме. <br><br>Тем самым не забивается исходный контекст LLM, а свойства модели на текстовом домене не теряются. Веса языковой модели остаются замороженными, чтобы сохранить производительность на текстовых задачах, то есть обновляются только веса визуального энкодера и адаптера изображений.<br><br>Что касается видео, то тут меняется количество кадров на входе: на претрейне их 16, а на файнтюне — 64. Далее их сводят к фиксированной размерности и точно так же, как с изображениями, добавляют через кросс-аттеншн в LLM. <br><br>На претрейне используют 6 миллиардов пар картинка-текст. Для фильтрации датасета убирают все описания на языках, отличных от английского, и пары с низким CLIP-скором. Дальше происходит дедупликация, чтобы исключить часто повторяющиеся картинки.<br><br>В ходе ресемплирования текстовых описаний их частотность понижают. Кроме того, в каждое описание добавляют распознанный на картинке текст — это нужно для улучшения результатов на задачах, которые требуют использования OCR.<br><br>Существует весьма необычная стадия пост-претрейна — с применением небольшого датасета на 500 миллионов сэмплов. 150 миллионов из них — это изображения вроде скриншотов с HTML-кодами и таблицами. <br><br>На файнтюне инженеры прибегают к ещё одному нестандартному трюку — hot-swap. Они подменяют веса предварительно обученной LLM на веса языковой модели, прошедшей Instruction Tuning. На SFT используют академические датасеты, ответы, написанные людьми, и синтетические данные — например, изображения, созданные из текстовых описаний. <br><br>Далее создают пары, оцененные асессорами по семибалльной шкале. Кроме того, есть процедура Rejection sampling для итеративного выбора высококачественных ответов, сгенерированных моделью. Потом авторы статьи обучают reward-модель и делают DPO, как и для текстовой модели. <br><br>Получившаяся модель на 405 миллиардов параметров показывает 80,2 пункта в бенчмарке VQAv2 и 84,8 пункта в TextVQA. При этом она уступает Claude 3.5 в DocVQA, ChartQA и AI2 Diagram. <br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Роман Исаченко</em><br><br><a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer">CV Time</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/25_480.webp" srcset="../assets/media/thumbs/25_480.webp 480w, ../assets/media/25.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="25" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/26_480.webp" srcset="../assets/media/thumbs/26_480.webp 480w, ../assets/media/26.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="25" data-image-index="1" /></div></div>
      <div class="actions">
        <span>3 415 просмотров · 43 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/25" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/25.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="24" data-search="movie gen: a cast of media foundation models кажется, что после eccv 2024 только ленивый не написал о movie gen. тем не менее далеко не все авторы ссылаются на оригинальный технический отчёт и описывают подробности. movie gen — большая модель на 30 миллиардов параметров, которую исследователи из meta* разработали для генерации видео. она может создавать клипы длительностью до 16 секунд с частотой 16 кадров в секунду и разрешением 1080p. новой модели можно доверить: — cтандартную генерацию видео по текстовому запросу; — персонализированную генерацию по референсному изображению; — редактирование видео по текстовому запросу. movie gen сочетает в себе несколько интересных архитектурных решений. создатели модели: — позаимствовали базовый трансформер у языковой модели llama от той же meta, но дополнили его блоками, специфичными для диффузионных генеративных моделей; — обучили собственную эффективную модель, которая преобразует высокоразмерное пиксельно-временное пространство в куда более компактное латентное; — добавили video-super-resolution-модель, которая повышает разрешение исходной генерации с 768p до 1080p; — скомбинировали несколько моделей для кодирования текстовой информации с разными свойствами; — использовали отдельную модель, чтобы генерировать синтетические промпты специально для видео. на иллюстрации к посту — процедура обучения movie gen. сначала модель предобучали на большом количестве изображений с низким разрешением, потом — тренировали на клипах низкого (256p) и высокого (768p) разрешения. а после всего — файнтюнили на небольшом датасете из вручную отобранных и размеченных данных высокого качества. для того, чтобы оценить качество модели, команда собрала примерно тысячу текстовых запросов из разных областей и сравнила качество их обработки с текущей sota (все closed-source) — runway gen3, kling 1.5 и sora от openai. согласно user preference study, модель от meta превосходит конкурентов (или не уступает им) в большинстве аспектов: например, в следовании текстовому запросу, естественности и плавности движений. в задачах персонализации и редактирования модель тоже показывает хорошее качество. самый существенный недостаток movie gen — большой размер: для работы с ней потребуются значительные вычислительные ресурсы. p. s. а ещё команда из meta обучила модель для генерации звука и музыки — movie gen audio, которая тоже представляет собой большой (на 13 миллиардов параметров) трансформер. но это уже совсем другая история несколько иная архитектура. разбор подготовил ❣ денис кузнеделев cv time ___ meta признана экстремистской организацией, а facebook и instagram запрещены на территории рф movie gen: a cast of media foundation models кажется, что после eccv 2024 только ленивый не написал о movie gen. тем не менее далеко не все авторы ссылаются на оригинальный технический отчёт и описывают подробности. movie gen — большая модель на 30 миллиардов параметров, которую исследователи из meta* разработали для генерации видео. она может создавать клипы длительностью до 16 секунд с частотой 16 кадров в секунду и разрешением 1080p. новой модели можно доверить: — cтандартную генерацию видео по текстовому запросу; — персонализированную генерацию по референсному изображению; — редактирование видео по текстовому запросу. movie gen сочетает в себе несколько интересных архитектурных решений. создатели модели: — позаимствовали базовый трансформер у языковой модели llama от той же meta, но дополнили его блоками, специфичными для диффузионных генеративных моделей; — обучили собственную эффективную модель, которая преобразует высокоразмерное пиксельно-временное пространство в куда более компактное латентное; — добавили video-super-resolution-модель, которая повышает разрешение исходной генерации с 768p до 1080p; — скомбинировали несколько моделей для кодирования текстовой информации с разными свойствами; — использовали отдельную модель, чтобы генерировать синтетические промпты специально для видео. на иллюстрации к посту — процедура обучения movie gen. сначала модель предобучали на большом количестве изображений с низким разрешением, потом — тренировали на клипах низкого (256p) и высокого (768p) разрешения. а после всего — файнтюнили на небольшом датасете из вручную отобранных и размеченных данных высокого качества. для того, чтобы оценить качество модели, команда собрала примерно тысячу текстовых запросов из разных областей и сравнила качество их обработки с текущей sota (все closed-source) — runway gen3, kling 1.5 и sora от openai. согласно user preference study, модель от meta превосходит конкурентов (или не уступает им) в большинстве аспектов: например, в следовании текстовому запросу, естественности и плавности движений. в задачах персонализации и редактирования модель тоже показывает хорошее качество. самый существенный недостаток movie gen — большой размер: для работы с ней потребуются значительные вычислительные ресурсы. p. s. а ещё команда из meta обучила модель для генерации звука и музыки — movie gen audio, которая тоже представляет собой большой (на 13 миллиардов параметров) трансформер. но это уже совсем другая история несколько иная архитектура. разбор подготовил ❣ денис кузнеделев cv time ___ meta признана экстремистской организацией, а facebook и instagram запрещены на территории рф">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-10-25T09:17:32+00:00" href="./posts/24.html">2024-10-25 09:17 UTC</a></div>
      </div>
      <div class="post-body"><strong>Movie Gen: A Cast of Media Foundation Models</strong><br><br>Кажется, что после ECCV 2024 только ленивый не написал о Movie Gen. Тем не менее далеко не все авторы ссылаются на <a href="https://ai.meta.com/static-resource/movie-gen-research-paper" rel="nofollow noopener noreferrer">оригинальный технический отчёт</a> и описывают подробности. <br><br>Movie Gen — большая модель на 30 миллиардов параметров, которую исследователи из Meta* разработали для генерации видео. Она может создавать клипы длительностью до 16 секунд с частотой 16 кадров в секунду и разрешением 1080p. Новой модели можно доверить:<br><br>— cтандартную генерацию видео по текстовому запросу;<br>— персонализированную генерацию по референсному изображению;<br>— редактирование видео по текстовому запросу.<br><br>Movie Gen сочетает в себе несколько интересных архитектурных решений. Создатели модели:<br><br>— позаимствовали базовый трансформер у языковой модели LLaMA от той же Meta, но дополнили его блоками, специфичными для диффузионных генеративных моделей;<br>— обучили собственную эффективную модель, которая преобразует высокоразмерное пиксельно-временное пространство в куда более компактное латентное;<br>— добавили video-super-resolution-модель, которая повышает разрешение исходной генерации с 768p до 1080p;<br>— скомбинировали несколько моделей для кодирования текстовой информации с разными свойствами;<br>— использовали отдельную модель, чтобы генерировать синтетические промпты специально для видео.<br><br>На иллюстрации к посту — процедура обучения Movie Gen. Сначала модель предобучали на большом количестве изображений с низким разрешением, потом — тренировали на клипах низкого (256p) и высокого (768p) разрешения. А после всего — файнтюнили на небольшом датасете из вручную отобранных и размеченных данных высокого качества. <br><br>Для того, чтобы оценить качество модели, команда собрала примерно тысячу текстовых запросов из разных областей и сравнила качество их обработки с текущей SOTA (все closed-source) — Runway Gen3, Kling 1.5 и Sora от OpenAI. Согласно user preference study, модель от Meta превосходит конкурентов (или не уступает им) в большинстве аспектов: например, в следовании текстовому запросу, естественности и плавности движений. <br><br>В задачах персонализации и редактирования модель тоже показывает хорошее качество. Самый существенный недостаток Movie Gen — большой размер: для работы с ней потребуются значительные вычислительные ресурсы.<br><br><strong>P. S.</strong> А ещё команда из Meta обучила модель для генерации звука и музыки — Movie Gen Audio, которая тоже представляет собой большой (на 13 миллиардов параметров) трансформер. Но это уже <del>совсем другая история</del> несколько иная архитектура. <br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Денис Кузнеделев</em><br><br><a href="https://t.me/+jA1a8SVHTxIzNmNi" rel="nofollow noopener noreferrer">CV Time</a><br>___<br><em>Meta признана экстремистской организацией, а Facebook и Instagram запрещены на территории РФ</em><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/24_480.webp" srcset="../assets/media/thumbs/24_480.webp 480w, ../assets/media/24.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="24" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 941 просмотров · 32 реакций</span>
        <span class="action-links"><a href="https://t.me/timeforcv/24" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/24.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    </div>
    
    <div class="pager static-pager" style="justify-content:center">
      <div class="page-links">
        <a class="nav-link" href="page-2.html">←</a>
        <a class="page-link" href="index.html">1</a> <a class="page-link" href="page-2.html">2</a> <a class="page-link current" href="page-3.html">3</a> <a class="page-link" href="page-4.html">4</a>
        <a class="nav-link" href="page-4.html">→</a>
      </div>
    </div>
    
  </main>

  <footer class="footer">
    <div class="container">
      <div class="footer-inner">
        <span>based on <a href="https://github.com/ml-brand/tg-to-gh-pages" target="_blank" rel="noopener">tg-to-gh-pages</a> (created by <a href="https://github.com/ml-brand" target="_blank" rel="noopener">ML Brand</a>)</span>
        <a id="repoLink" href="https://github.com/ml-brand/tg-to-gh-pages" target="_blank" rel="noopener">Do the same with your channel.</a>
        <span class="footer-links">
          static copy ·
          <a href="../feed.xml" target="_blank" rel="noopener">RSS</a> ·
          <a href="../atom.xml" target="_blank" rel="noopener">Atom</a>
        </span>
      </div>
    </div>
  </footer>

  <script>
    window.__STATIC_POSTS = [{"id": 85, "media": [{"kind": "photo", "path": "../assets/media/85.jpg", "thumb": "../assets/media/thumbs/85_480.webp", "size": 102312, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/86.jpg", "thumb": "../assets/media/thumbs/86_480.webp", "size": 65619, "mime": "image/jpeg", "name": null}]}, {"id": 84, "media": [{"kind": "photo", "path": "../assets/media/84.jpg", "thumb": "../assets/media/thumbs/84_480.webp", "size": 38721, "mime": "image/jpeg", "name": null}]}, {"id": 83, "media": [{"kind": "photo", "path": "../assets/media/83.jpg", "thumb": "../assets/media/thumbs/83_480.webp", "size": 142049, "mime": "image/jpeg", "name": null}]}, {"id": 77, "media": [{"kind": "photo", "path": "../assets/media/77.jpg", "thumb": "../assets/media/thumbs/77_480.webp", "size": 118612, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/78.jpg", "thumb": "../assets/media/thumbs/78_480.webp", "size": 84345, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/79.jpg", "thumb": "../assets/media/thumbs/79_480.webp", "size": 117122, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/80.jpg", "thumb": "../assets/media/thumbs/80_480.webp", "size": 186279, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/81.jpg", "thumb": "../assets/media/thumbs/81_480.webp", "size": 97889, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/82.jpg", "thumb": "../assets/media/thumbs/82_480.webp", "size": 113680, "mime": "image/jpeg", "name": null}]}, {"id": 76, "media": []}, {"id": 75, "media": [{"kind": "photo", "path": "../assets/media/75.jpg", "thumb": "../assets/media/thumbs/75_480.webp", "size": 157422, "mime": "image/jpeg", "name": null}]}, {"id": 74, "media": [{"kind": "photo", "path": "../assets/media/74.jpg", "thumb": "../assets/media/thumbs/74_480.webp", "size": 109888, "mime": "image/jpeg", "name": null}]}, {"id": 72, "media": []}, {"id": 65, "media": [{"kind": "photo", "path": "../assets/media/65.jpg", "thumb": "../assets/media/thumbs/65_480.webp", "size": 100886, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/66.jpg", "thumb": "../assets/media/thumbs/66_480.webp", "size": 158484, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/67.jpg", "thumb": "../assets/media/thumbs/67_480.webp", "size": 149374, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/68.jpg", "thumb": "../assets/media/thumbs/68_480.webp", "size": 113751, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/69.jpg", "thumb": "../assets/media/thumbs/69_480.webp", "size": 127374, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/70.jpg", "thumb": "../assets/media/thumbs/70_480.webp", "size": 155623, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/71.jpg", "thumb": "../assets/media/thumbs/71_480.webp", "size": 185719, "mime": "image/jpeg", "name": null}]}, {"id": 64, "media": [{"kind": "photo", "path": "../assets/media/64.jpg", "thumb": "../assets/media/thumbs/64_480.webp", "size": 67317, "mime": "image/jpeg", "name": null}]}, {"id": 60, "media": [{"kind": "photo", "path": "../assets/media/60.jpg", "thumb": "../assets/media/thumbs/60_480.webp", "size": 103100, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/61.jpg", "thumb": "../assets/media/thumbs/61_480.webp", "size": 120453, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/62.jpg", "thumb": "../assets/media/thumbs/62_480.webp", "size": 134536, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/63.jpg", "thumb": "../assets/media/thumbs/63_480.webp", "size": 150448, "mime": "image/jpeg", "name": null}]}, {"id": 59, "media": [{"kind": "photo", "path": "../assets/media/59.jpg", "thumb": "../assets/media/thumbs/59_480.webp", "size": 146053, "mime": "image/jpeg", "name": null}]}, {"id": 58, "media": []}, {"id": 57, "media": []}, {"id": 56, "media": [{"kind": "photo", "path": "../assets/media/56.jpg", "thumb": "../assets/media/thumbs/56_480.webp", "size": 43147, "mime": "image/jpeg", "name": null}]}, {"id": 55, "media": []}, {"id": 54, "media": []}, {"id": 53, "media": [{"kind": "photo", "path": "../assets/media/53.jpg", "thumb": "../assets/media/thumbs/53_480.webp", "size": 92614, "mime": "image/jpeg", "name": null}]}, {"id": 45, "media": [{"kind": "photo", "path": "../assets/media/45.jpg", "thumb": "../assets/media/thumbs/45_480.webp", "size": 166582, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/46.jpg", "thumb": "../assets/media/thumbs/46_480.webp", "size": 211834, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/47.jpg", "thumb": "../assets/media/thumbs/47_480.webp", "size": 189226, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/48.jpg", "thumb": "../assets/media/thumbs/48_480.webp", "size": 201354, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/49.jpg", "thumb": "../assets/media/thumbs/49_480.webp", "size": 239403, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/50.jpg", "thumb": "../assets/media/thumbs/50_480.webp", "size": 207700, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/51.jpg", "thumb": "../assets/media/thumbs/51_480.webp", "size": 226277, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/52.jpg", "thumb": "../assets/media/thumbs/52_480.webp", "size": 204177, "mime": "image/jpeg", "name": null}]}, {"id": 43, "media": []}, {"id": 42, "media": [{"kind": "photo", "path": "../assets/media/42.jpg", "thumb": "../assets/media/thumbs/42_480.webp", "size": 110060, "mime": "image/jpeg", "name": null}]}, {"id": 41, "media": [{"kind": "photo", "path": "../assets/media/41.jpg", "thumb": "../assets/media/thumbs/41_480.webp", "size": 134973, "mime": "image/jpeg", "name": null}]}, {"id": 40, "media": [{"kind": "photo", "path": "../assets/media/40.jpg", "thumb": "../assets/media/thumbs/40_480.webp", "size": 24221, "mime": "image/jpeg", "name": null}]}, {"id": 39, "media": []}, {"id": 37, "media": [{"kind": "photo", "path": "../assets/media/37.jpg", "thumb": "../assets/media/thumbs/37_480.webp", "size": 64662, "mime": "image/jpeg", "name": null}]}, {"id": 36, "media": [{"kind": "photo", "path": "../assets/media/36.jpg", "thumb": "../assets/media/thumbs/36_480.webp", "size": 67016, "mime": "image/jpeg", "name": null}]}, {"id": 35, "media": []}, {"id": 28, "media": [{"kind": "photo", "path": "../assets/media/28.jpg", "thumb": "../assets/media/thumbs/28_480.webp", "size": 189205, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/29.jpg", "thumb": "../assets/media/thumbs/29_480.webp", "size": 193459, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/30.jpg", "thumb": "../assets/media/thumbs/30_480.webp", "size": 170837, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/31.jpg", "thumb": "../assets/media/thumbs/31_480.webp", "size": 167052, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/32.jpg", "thumb": "../assets/media/thumbs/32_480.webp", "size": 188330, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/33.jpg", "thumb": "../assets/media/thumbs/33_480.webp", "size": 205748, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/34.jpg", "thumb": "../assets/media/thumbs/34_480.webp", "size": 166987, "mime": "image/jpeg", "name": null}]}, {"id": 25, "media": [{"kind": "photo", "path": "../assets/media/25.jpg", "thumb": "../assets/media/thumbs/25_480.webp", "size": 78248, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/26.jpg", "thumb": "../assets/media/thumbs/26_480.webp", "size": 48006, "mime": "image/jpeg", "name": null}]}, {"id": 24, "media": [{"kind": "photo", "path": "../assets/media/24.jpg", "thumb": "../assets/media/thumbs/24_480.webp", "size": 38712, "mime": "image/jpeg", "name": null}]}];
    window.__STATIC_META = {"title": "CV Time", "username": "timeforcv", "channel": "timeforcv", "last_sync_utc": "2026-02-14T15:58:11Z", "posts_count": 107, "last_seen_message_id": 239, "stats": {"new": 122, "updated": 1, "media_downloaded": 122}, "avatar": "assets/channel_avatar.jpg", "meta_schema_version": "1.0.0", "posts_schema_version": "1.0.0"};
  </script>
  <script src="../common.js"></script>
  <script src="../static.js"></script>
</body>
</html>
