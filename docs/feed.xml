<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>CV Time</title><link>https://ml-brand.github.io/timeforcv/</link><description>Зеркало Telegram-канала timeforcv</description><atom:link href="https://ml-brand.github.io/timeforcv/feed.xml" rel="self" type="application/rss+xml" /><lastBuildDate>Sat, 14 Feb 2026 19:53:33 +0000</lastBuildDate><item><title>Emu3.5: Native Multimodal Models are World Learners</title><link>https://t.me/timeforcv/238</link><guid>https://t.me/timeforcv/238</guid><pubDate>Tue, 10 Feb 2026 10:16:14 +0000</pubDate><description>&lt;strong&gt;Emu3.5: Native Multimodal Models are World Learners&lt;/strong&gt;&lt;br&gt;&lt;strong&gt;&lt;br&gt;&lt;/strong&gt;Сегодня разбираем &lt;a href="https://arxiv.org/abs/2510.26583v1" rel="nofollow noopener noreferrer"&gt;работу&lt;/a&gt; от команды китайского Института искусственного интеллекта, которая продолжает гнуть свою линию и выкатывает очередную модель семейства Emu. На этот раз — Emu3.5. В отличие от предыдущих работ, здесь авторы прямо говорят, что пытаются построить не просто мультимодальную модель, а некую world model. Ниже разберёмся, что под этим понимают.&lt;br&gt;&lt;br&gt;Сразу о путанице в названиях. Есть &lt;a href="https://arxiv.org/abs/2307.05222" rel="nofollow noopener noreferrer"&gt;Emu от Meta&lt;/a&gt;* — text-image-модель, важная в своё время как ранний пример качественного SFT на небольших датасетах. И есть отдельная серия работ Emu от авторов этой статьи.&lt;br&gt;&lt;br&gt;Например, год назад, у них была работа под названием &lt;a href="https://arxiv.org/abs/2409.18869" rel="nofollow noopener noreferrer"&gt;Emu3: Next-Token Prediction is All You Need&lt;/a&gt;. Тогда идея была довольно простой: свести текст, картинки и видео к единой задаче next-token prediction. Генерации выглядели сочными, но при внимательном рассмотрении страдали от типичных артефактов дискретизации — текстуры «плыли», мелкие детали разваливались. &lt;br&gt;&lt;br&gt;В Emu3.5 амбиции заметно выросли. Архитектурно всё по-прежнему прямолинейно: один decoder-only-трансформер на 34B параметров, обучаемый чисто авторегрессионно. Самое интересное — в данных. Вместо того чтобы опираться в основном на пары картинка-текст, модель обучают преимущественно на чередующихся (interleaved) видео-текстовых последовательностях из интернета. Видео нарезают на ключевые кадры, аудио транскрибируют с помощью ASR с таймстемпами, а затем всё это склеивают в одну длинную последовательность: в сумме — больше 10 триллионов токенов.&lt;br&gt;&lt;br&gt;Так модель учится не отдельным сценам, а событиям во времени: динамике, переходам, причинно-следственным связям. Это и есть их практическое определение «world learning». Кроме видео используют обычные image-text-данные и большой объём text-only-данных.&lt;br&gt;&lt;br&gt;И это ещё не финал: после претрейна модель доучивают — сначала на гигантском SFT (150 млрд сэмплов), а потом через RL-алайнмент, чтобы она вела себя адекватно и по тексту, и по картинкам.&lt;br&gt;&lt;br&gt;Все модальности токенизируются в общее дискретное пространство. Словарь модели — около 280k токенов, из которых ~150k приходятся на текст, а остальная часть — на визуальные токены. Для визуальной части используется собственный токенизатор с REPA-подобной стабилизацией через SigLIP. Авторы честно признают, что дискретизация всё равно даёт артефакты, поэтому опционально добавляют диффузионный декодер поверх авторегрессионной генерации.&lt;br&gt;&lt;br&gt;Отдельная важная часть — DiDA (Discrete Diffusion Adaptation). Так пробуют решить главную боль авторегрессии: медленную генерацию изображений. На этапе инференса модель временно переводится в режим дискретной диффузии: визуальные токены зашумляются и затем восстанавливаются за несколько итераций. За счёт этого генерация картинок ускоряется примерно в 20 раз без заметной потери качества.&lt;br&gt;&lt;br&gt;На выходе Emu3.5 умеет довольно широкий спектр вещей: выдаёт длинные согласованные визуальные нарративы, генерацию историй с картинками, пошаговые визуальные инструкции и даже навигацию по сцене по текстовым командам — как будто внутри есть некоторое представление пространства. В классических задачах text-to-image и image editing модель на уровне сильных закрытых мультимодальных моделей.&lt;br&gt;&lt;br&gt;В итоге, даже если с громким термином world model можно поспорить, сама траектория развития Emu выглядит любопытно — продолжим следить за ними.&lt;br&gt;&lt;br&gt;&lt;em&gt;Разбор подготовил &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt; Сергей Кастрюлин&lt;br&gt;&lt;/em&gt;&lt;a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;&lt;br&gt;&lt;em&gt;___&lt;/em&gt;&lt;br&gt;&lt;em&gt;Компания Meta признана экстремистской; её деятельность в России запрещена.&lt;/em&gt;</description></item><item><title>Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking [2/2</title><link>https://t.me/timeforcv/237</link><guid>https://t.me/timeforcv/237</guid><pubDate>Fri, 06 Feb 2026 11:03:15 +0000</pubDate><description>&lt;strong&gt;Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking [2/2]&lt;br&gt;&lt;/strong&gt;&lt;br&gt;Продолжаем разбирать &lt;a href="https://arxiv.org/abs/2601.04720" rel="nofollow noopener noreferrer"&gt;техрепорт&lt;/a&gt;, описывающий новые модели Qwen.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Обучение моделей и результаты&lt;br&gt;&lt;/strong&gt;&lt;br&gt;Обучение моделей делается в несколько этапов, причём довольно нетривиальным образом: модели с этапа X используются для последующей фильтрации данных для этапа X+1, а Embedding и Reranker на разных этапах выступают учителями друг для друга.&lt;br&gt;&lt;br&gt;— На всех этапах модели обучаются как LoRA к Qwen3-VL, чтобы с большей вероятностью не испортить возможности сильного бэкбона.&lt;br&gt;&lt;br&gt;— На первом этапе (s0) на всём датасете обучается Embedding, используя контрастивный InfoNCE-лосс.&lt;br&gt;&lt;br&gt;— На следующем этапе Embedding:s0 используется для фильтрации датасета — и на этом фильтре обучается Embedder:s1 и Reranker.&lt;br&gt;&lt;br&gt;— На последнем этапе снова фильтруется уже Reranker, и скоры Reranker используются как таргет для дистилляции Embedding:s2.&lt;br&gt;&lt;br&gt;— Наконец, веса полученной модели усредняются (точнее, сферически интерполируются) с Embedding:s1, порождая финальную модель Embedding:s3, которая и пошла в релиз.&lt;br&gt;&lt;br&gt;По замерам авторов, их модели опережают все существующие открытые и закрытые модели на мультимодальных бенчмарках. При этом на текстовых задачах есть и более сильные модели — в основном существенно большего размера.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Использование моделей&lt;br&gt;&lt;/strong&gt; &lt;br&gt;Авторы явно постарались сделать модели production-ready, позаботившись не только о качестве метрик, но и об удобстве использования.&lt;br&gt;&lt;br&gt;Во-первых, в модель заложены несколько очень важных свойств для производительности (помимо инференса в один prefill-этап).&lt;br&gt;&lt;br&gt;Тренировка проводилась в quantization-aware-режиме — при вычислении лоссов для эмбеддингов, авторы одновременно вычисляли их для квантизованных в int8-эмбеддингов. В результате, полученные эмбеддинги можно квантизовать в int8 (отмасштабировать в интервал  [-127, 128] и округлить), хранить и использовать практически потери качества.&lt;br&gt;&lt;br&gt;Также в тренировке эмбеддингов использовался подход матрёшки, при котором лоссы применяются не только к эмбеддингам целиком, но и по частям к их первым 32, 64, 128, 256 и 512 элементам. Благодаря этому каждый кратный степени двойки «подсрез» эмбеддинга — тоже эмбеддинг (хоть и худшего качества). При работе с большой базой документов можно, например, брать только первые 128 элементов эмбеддинга вместо 1024 и хранить только их. Суммарно можно сократить размер эмбеддингов базы документов в 10–50 раз.&lt;br&gt;&lt;br&gt;Во-вторых, в силу архитектуры модель очень гибка в применении. И документ, и запрос могут быть не только одним изображением или текстом, но и их произвольной последовательностью. Довольно большое окно контекста (32К) токенов позволяет обрабатывать 10–20 страниц изображений вместе с текстом. &lt;br&gt;&lt;br&gt;Также интересная фича таких моделей как класса — наличие инструкции. Мультимодальные семантические эмбеддинги доступны всем и каждому как минимум с момента релиза CLIP (5 лет назад!), но способ вычисления эмбеддинга почти всегда был «зашит» в модель. Для эмбеддеров на основе LLM/VLM можно в инструкции указать, что важно в «кодировании» документов и запросов. Например, в случае поиска по картинкам можно инструктировать модель фокусироваться на стиле изображения или, наоборот, на содержимом — и получить эмбеддинги, поиск по которым будет давать разные результаты.&lt;br&gt;&lt;br&gt;В итоге у авторов получилась гибкая и эффективная опенсорсная модель для мультимодального поиска. В отчёте приведено много деталей обучения, а в cookbook — примеров использования. Модели такого класса определённо имеют множество применений как в продуктах, так и в рутинных ML-задачах по работе с данными.&lt;br&gt;&lt;br&gt;&lt;em&gt;Разбор подготовил &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt; Борис Зимка&lt;br&gt;&lt;/em&gt;&lt;a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking [1/2</title><link>https://t.me/timeforcv/236</link><guid>https://t.me/timeforcv/236</guid><pubDate>Tue, 03 Feb 2026 08:42:01 +0000</pubDate><description>&lt;strong&gt;Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking [1/2]&lt;br&gt;&lt;/strong&gt;&lt;br&gt;Ещё летом 2025-го вышли текстовые Qwen3-Embedding/Reranker. А в январе этого года команда Qwen представила новые модели: Qwen3-VL-Embedding и Qwen3-VL-Reranker. В &lt;a href="https://arxiv.org/abs/2601.04720" rel="nofollow noopener noreferrer"&gt;техрепорте&lt;/a&gt; авторы рассказывают, как им удалось адаптировать VLM для решения задач мультимодального поиска и ранжирования — ключевых тем ML с долгой историей развития и огромным количеством применений. Об этом сегодня и поговорим.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Формулировка задачи&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Если кратко, задача поиска по базе документов — по запросу Q среди множества документов D[i] найти подходящие под запрос. В текстовом поиске Q и D — текст, а в мультимодальном варианте — Q и D могут быть картинками, текстом или их комбинацией, причём модальности Q и D могут не совпадать. Например, по запросу «пингвины в Южной Америке» релевантны и статьи Википедии, и соответствующие фотографии.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Модели&lt;br&gt;&lt;/strong&gt;&lt;br&gt;Один из распространённых подходов в решении задачи поиска — разбиение на два этапа: быстрый поиск кандидатов и более сложное ранжирование их между собой для определения лучших. Исходя из такой схемы, команда Qwen подготовила две модели:&lt;br&gt;&lt;br&gt;1. Qwen3-VL-Embedding: модель, предсказывающая для документа или запроса вектор признаков в соответствии с инструкцией. Можно считать, &lt;code&gt;`def embedding(instruction: str, query_or_doc: str | Image) -&amp;gt; list[float]`&lt;/code&gt;.&lt;br&gt;&lt;br&gt;2. Qwen3-VL-Reranker: модель, оценивающая согласно инструкции степень соответствия запроса документу от 0 до 1. Интерфейс примерно: &lt;code&gt;`def reranker(instruction: str, query: str | Image, document: str | Image) -&amp;gt; float`&lt;/code&gt;.&lt;br&gt;&lt;br&gt;Архитектурно модели — почти точные копии VLM: получают на вход токенизированные инструкции и текст, патчи изображений, но имеют модифицированный выход, и инференсятся несколько иначе.&lt;br&gt;&lt;br&gt;Reranker выполняет инференс всей VLM целиком, но на выходе в качестве оценки «релевантен ли документ запросу» берётся соотношение вероятностей токенов “yes” и “no”. Embedding выполняет инференс до последнего слоя (проекции токена в вероятности вокабуляра) — и hidden state перед этой проекцией возвращается как эмбеддинг.&lt;br&gt;&lt;br&gt;В отличие от полноценных VLM, в Embedding и Reranker выполняется только этап prefill (обработка входного контекста), и состояние последнего токена промпта возвращается как ответ. Стадия decoding (предсказания одного токена за другим) отсутствует, что делает инференс многократно быстрее.&lt;br&gt;&lt;br&gt;Обе модели инициализируются Qwen3-VL и доступны в двух вариантах: на 2 и 8 миллиардов параметров.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Данные&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Датасеты для поиска повторяют логику задачи:&lt;br&gt;— одна текстовая инструкция к задаче I;&lt;br&gt;— база мультимодальных документов D[i];&lt;br&gt;— набор мультимодальных запросов Q[j];&lt;br&gt;— матрица меток R[i, j], определяющих D[i] как релевантный или нерелевантный Q[j].&lt;br&gt;&lt;br&gt;На таком датасете можно обучать как Reranker (напрямую классифицировать релевантность пары Q-D), так и Embedding (оценивая релевантность пары по скалярному произведению эмбеддингов).&lt;br&gt;&lt;br&gt;Обучающий корпус Embedding и Reranker состоит из множества таких датасетов. Для каждого из них база документов берётся из реальных данных — эти документы VLM описывает и классифицирует. Некачественные фильтруются, распределение датасетов нормализуется, чтобы избежать сильного перекоса в какой-либо домен.&lt;br&gt;&lt;br&gt;Затем для документов с помощью VLM генерируют запросы разных типов, причём как релевантные документу, так и hard-negative-примеры — запросы, для которых документ похож на релевантный, но не является таковым.&lt;br&gt;&lt;br&gt;После этого датасеты дополнительно фильтруются уже существующими моделями и неудачные элементы датасета отсеиваются.&lt;br&gt;&lt;br&gt;Во второй части разбора поговорим о том, как модели учились, и об их использовании на практике.&lt;br&gt;&lt;br&gt;&lt;em&gt;Разбор подготовил &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt; Борис Зимка&lt;br&gt;&lt;/em&gt;&lt;a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>Ovis-U1 Technical Report</title><link>https://t.me/timeforcv/235</link><guid>https://t.me/timeforcv/235</guid><pubDate>Tue, 27 Jan 2026 07:44:01 +0000</pubDate><description>&lt;strong&gt;Ovis-U1 Technical Report&lt;br&gt;&lt;/strong&gt;&lt;br&gt;Некоторое время назад мы обсуждали &lt;a href="https://t.me/timeforcv/192" rel="nofollow noopener noreferrer"&gt;MLLM&lt;/a&gt;. Сегодня разберём &lt;a href="https://arxiv.org/abs/2506.23044" rel="nofollow noopener noreferrer"&gt;статью&lt;/a&gt; о ещё одной универсальной модели, способной обрабатывать и текст, и изображения. &lt;br&gt;&lt;br&gt;Ovis-U1 — модель-швейцарский-нож. В зависимости от инструкции, она может работать и в режиме image-to-text, и в text-to-image. Например, можно изменить изображение, описать его или сгенерировать совсем новую картинку по текстовому запросу. Архитектуру MLLM можно рассмотреть на первой из трёх схем.&lt;br&gt;&lt;br&gt;Следите за логикой сверху вниз: &lt;br&gt;&lt;br&gt;1. Сначала Ovis-U1 обрабатывает входные данные: токенизирует текст и обрабатывает изображения визуальным энкодером, чтобы составить семантический эмбеддинг, или использует VAE-энкодер для составления детализированного представления. &lt;br&gt;&lt;br&gt;2. Полученная последовательность подаётся в трансформер, инициализируемый с Qwen3-1.7B.&lt;br&gt;&lt;br&gt;3. Для генерации изображения выходные токены текстов и семантических представлений входной картинки комбинируются с помощью пары трансформерных слоев (авторы называют это Refiner’ом, на схеме обозначено как (с)) и, вместе с VAE-эмбеддингами, отправляются в «визуальный декодер» на базе MMDiT. Эта часть инициализируется с нуля.&lt;br&gt;&lt;br&gt;Обучение модели происходит в несколько этапов: &lt;br&gt;&lt;br&gt;— Сначала предобучается визуальный декодер на задачу text-to-image-генерации. Все остальные части при этом заморожены.&lt;br&gt;— Следом предобучается адаптер между LLM и визуальным энкодером на задачи text-to-image-генерации, а также понимание и редактирование изображений.&lt;br&gt;— Потом на тех же данных визуальный энкодер и адаптер обучаются вместе.&lt;br&gt;— На следующей стадии всё, кроме визуального декодера, обучается на задачах понимания изображения.&lt;br&gt;— Далее на задаче генерации изображений обучается refiner и визуальный декодер.&lt;br&gt;— На финальном этапе визуальный декодер файнтюнится для задач text-to-image-генерации и редактирования изображений.&lt;br&gt;&lt;br&gt;Авторы утверждают, что визуальный декодер на основе диффузии в сочетании с Refiner’ом позволяет генерировать изображения почти так же хорошо, как GPT-4o. Интересны ещё несколько замеров:&lt;br&gt;&lt;br&gt;— 69,6 баллов в мультимодальном академическом тесте OpenCompass (что лучше последних современных моделей, такие как Ristretto-3B и SAIL-VL-1.5-2B);&lt;br&gt;— 83,72 балла и 0,89 балла при преобразовании текста в изображение в тестах DPG-Bench и GenEval; &lt;br&gt;— 4,00 и 6,42 для редактирования изображений в ImgEdit-Bench и GEdit-Bench-EN.&lt;br&gt;&lt;br&gt;&lt;em&gt;Разбор подготовил &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt; Сергей Овчаренко&lt;br&gt;&lt;/em&gt;&lt;a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield</title><link>https://t.me/timeforcv/230</link><guid>https://t.me/timeforcv/230</guid><pubDate>Tue, 20 Jan 2026 09:32:02 +0000</pubDate><description>&lt;strong&gt;Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield&lt;br&gt;&lt;/strong&gt;&lt;br&gt;Сегодня разберём &lt;a href="https://arxiv.org/abs/2511.22677" rel="nofollow noopener noreferrer"&gt;статью&lt;/a&gt;, авторы которой возвращаются к идее DMD и пытаются понять, что именно заставляет этот метод работать. Их главное наблюдение — главную роль в обучении играет не distribution matching, как можно было ожидать, а CFG Augmentation. &lt;br&gt;&lt;br&gt;&lt;strong&gt;Что такое DMD &lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2311.18828" rel="nofollow noopener noreferrer"&gt;DMD&lt;/a&gt; относится к ODE-free-дистилляции диффузионных моделей: здесь не важно, по какой траектории происходит сэмплирование, главное — чтобы модель умела выдавать скор-функцию.&lt;br&gt;&lt;br&gt;Идея метода в том, чтобы форсить совпадение распределения генератора с распределением реальных данных, оптимизируя KL-дивергенцию между P_{fake} и P_{real}. Плотность реальных данных напрямую недоступна, но для обучения достаточно градиента этого лосса. После дифференцирования в выражении появляются скор-функции реальных и фейковых данных: фейковую мы учим, а реальную аппроксимируем замороженной моделью-учителем.&lt;br&gt;&lt;br&gt;Поскольку скор-модели плохо работают на незашумлённых изображениях и реальные с фейковыми распределениями часто плохо пересекаются по модам, в DMD скоры считают на зашумлённых данных. Это делает их in-distribution и стабилизирует обучение. В итоге реальный скор остаётся замороженным, а фейковый обучается стандартным diffusion loss — это база для всех модификаций DMD.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Что изменилось в DMD2&lt;br&gt;&lt;/strong&gt;&lt;br&gt;В &lt;a href="https://arxiv.org/abs/2405.14867" rel="nofollow noopener noreferrer"&gt;DMD2&lt;/a&gt; авторы разомкнули обучение генератора и оценщика. Сделали несколько шагов обучения оценщика на один шаг генератора, и за счёт этого отказались от регрессионного лосса. Также был добавлен GAN loss как регуляризация: используют не как основной источник сигнала, а именно для стабилизации обучения.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Основная идея Decoupled DMD&lt;br&gt;&lt;/strong&gt;&lt;br&gt;В новой &lt;a href="https://arxiv.org/abs/2511.22677" rel="nofollow noopener noreferrer"&gt;статье&lt;/a&gt; авторы снова смотрят на градиент KL-дивергенции и замечают, что простая conditional-оценка реального скора работает плохо. Зато на практике гораздо лучше CFG-оценка. Возникает вопрос — это просто удачный трюк или за этим стоит какая-то теория?&lt;br&gt;&lt;br&gt;Оказывается, если подставить CFG прямо в формулу KL-лосса, он раскладывается на две части: классический distribution matching и дополнительный член, соответствующий вектору между real conditional и real unconditional скорами. Именно эту добавку авторы называют CFG Augmentation. Из этого разложения следует ключевой вывод статьи: основной обучающий сигнал в DMD даёт CFG Augmentation, а distribution matching выступает стабилизирующей регуляризацией.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Эксперименты и выводы&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Эксперименты подтверждают этот тезис. Обучение только на distribution matching быстро ломает семантику, обучение только на CFG Augmentation приводит к переобучению. Самый стабильный результат получается при совместном использовании обоих компонент лосса.&lt;br&gt;&lt;br&gt;Авторы также показывают, что CFG Augmentation и distribution matching имеет смысл обучать с разными уровнями шума: больший \tau в CFG-части помогает с высокочастотными деталями, тогда как для distribution matching лучше работает стандартный диапазон шумов.&lt;br&gt;&lt;br&gt;В итоге статья интересна не столько метриками, сколько самим наблюдением: CFG в DMD — это не эвристика, а осмысленный компонент лосса.&lt;br&gt;&lt;br&gt;&lt;em&gt;Разбор подготовил &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt; Михаил Колтаков&lt;br&gt;&lt;/em&gt;&lt;a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>Лучшие статьи 2025 года: выбор авторов СV Time. Часть 2</title><link>https://t.me/timeforcv/229</link><guid>https://t.me/timeforcv/229</guid><pubDate>Fri, 16 Jan 2026 07:48:02 +0000</pubDate><description>&lt;strong&gt;Лучшие статьи 2025 года: выбор авторов СV Time. Часть 2&lt;br&gt;&lt;/strong&gt;&lt;br&gt;Хороших статей в прошлом году оказалось слишком много, чтобы уместить их в один пост. Во второй части мы собрали не менее интересные работы, которые во многом определяют, как будет выглядеть генерация изображений и видео в 2026-м.&lt;br&gt;&lt;br&gt;&lt;a href="https://openreview.net/forum?id=BSZqpqgqM0" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Why Diffusion Models Don’t Memorize: The Role of Implicit Dynamical Regularization in Training &lt;br&gt;&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;Работа, отобранная программным комитетом NeurIPS 2025, как одна из лучших. Авторы исследуют причины, по которым диффузионные модели генерируют новые изображения, а не воспроизводят в точности обучающую выборку. Для модельных экспериментов берут датасет лиц Celeba в низком разрешении и сгенерированный случайной двухслойной сетью. Оказывается, что существуют две временные отметки: t_gen и t_mem, между которыми модель умеет создавать качественные примеры и при этом не в точности копировать данные из обучения. Причём с увеличением количества данных интервал растёт. Вывод: диффузионные модели обладают регуляризацией, которая позволяет им избегать переобучения даже при избыточной параметризации. На практике обучающие выборки очень велики и отметка t_mem недостижима.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2505.13447" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Mean Flows for One-step Generative Modeling &lt;br&gt;&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;Группа исследователей из CMU и MIT этой весной представила работу, где предложила способ обучения генеративных моделей — такой, чтобы они могли делать качественные генерации за один или мало шагов. В отличие от общепринятого сейчас подхода Flow Matching, моделирующего мгновенную скорость в точке, Mean Flow учится воспроизводить усредненную по участку траектории скорость, что даёт более надёжную и точную оценку пути из шума в данные. Авторам удалось достичь лучшего качества одношаговой генерации на ImageNet на момент выхода публикации. Работа получила продолжение в статьях &lt;a href="https://arxiv.org/abs/2510.20771" rel="nofollow noopener noreferrer"&gt;AlphaFlow&lt;/a&gt; и &lt;a href="https://arxiv.org/abs/2512.02012" rel="nofollow noopener noreferrer"&gt;Improved Mean Flows&lt;/a&gt;.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2510.11690" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Diffusion Transformers with Representation Autoencoders&lt;br&gt;&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;Как известно, сейчас в генерации картинок и видео доминирует латентная диффузия: учат VAE, чтобы перевести картинки в более низкоразмерное пространство, и потом — диффузионную модель уже в этом пространстве. Авторы предложили вместо VAE взять сотовый картиночный энкодер (Dino, Siglip), доучить к нему декодер и обучать диффузию в пространстве фичей этого энкодера. Показывают, что диффузия, обученная в этом пространстве, сильно улучшает качество генерации. Вероятно, это будет одно из самых популярных направлений ресёрча на ближайшие полгода-год, как было с &lt;a href="https://arxiv.org/abs/2410.06940" rel="nofollow noopener noreferrer"&gt;REPA&lt;/a&gt;. &lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2511.13720" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Back to Basics: Let Denoising Generative Models Denoise &lt;br&gt;&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;Исторически диффузионные модели чаще всего обучают предсказывать либо шум, который накладывается на картинку, либо разницу между шумом и чистой картинкой. В работе отмечают, что картинки в высоком разрешении, несмотря на большую размерность, лежат в сильно более низкоразмерном пространстве, и поэтому нейронке гораздо проще предсказывать чистую картинку, чем нечто с шумом, который захватывает всё пространство. Исходя из этого, авторы предлагают простейшую диффузионную модель — JiT (Just Image Transformer), которая работает напрямую в пиксель-спейсе (без VAE) и параметризована на предсказание чистой картинки. По архитектуре это обычный ViT с минимальными диффузионными спецификами. Показывают, что такая простая модель отлично работает на больших разрешениях, не требует дополнительных наворотов и внешних моделей. При этом по компьюту они даже эффективнее, чем латетные модели с VAE. &lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2510.21890" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;The Principles of Diffusion Models &lt;br&gt;&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;Классный учебник по диффузионным моделям от их «создателя» Стефано Эрмона. В книге куча пояснений, интуиции и обсуждений, которые помогают получить полную картину о том, что мы сейчас знаем про диффузию. Покрыты почти все ключевые темы — от самой базы и до последних малошаговых моделей, а-ля &lt;a href="https://arxiv.org/abs/2505.13447" rel="nofollow noopener noreferrer"&gt;MeanFlow&lt;/a&gt;. Будет крайне полезным для тех, кто хочет глубоко разобраться с диффузией.&lt;br&gt;&lt;br&gt;&lt;em&gt;Статьи отобрали &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt; Дмитрий Баранчук и Денис Кузнеделев&lt;/em&gt;&lt;br&gt;&lt;a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>Лучшие статьи 2025 года: выбор авторов СV Time. Часть 1</title><link>https://t.me/timeforcv/228</link><guid>https://t.me/timeforcv/228</guid><pubDate>Tue, 13 Jan 2026 08:24:01 +0000</pubDate><description>&lt;strong&gt;Лучшие статьи 2025 года: выбор авторов СV Time. Часть 1&lt;br&gt;&lt;/strong&gt;&lt;br&gt;Прошедший год оказался переломным для AI-рынка: монополия американских моделей пошатнулась, а в фокусе оказались китайские команды. Они выложили в опенсорс большое количество сильных моделей — от ризонинг до мультимодальных. Как заметил один из наших экспертов: «Можно сказать, что весь год был китайским — и есть ощущение, что следующий тоже будет».&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2506.13131" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;AlphaEvolve: A coding agent for scientific and algorithmic discovery&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;Статья, которая описывает способ решения сложных задач путём применения эволюционного алгоритма поверх LLM с большим контекстом. Эта модель нашла более оптимальное решение для ряда открытых математических задач, в том числе обнаружила алгоритм перемножения комплекснозначных матриц размера 4x4, который требует меньше операций (скалярного) перемножения, чем алгоритм Штрассена 1969 года. Этот результат сильнейшие умы человечества не могли получить в течение 56 лет. Открытие позволяет ускорить огромное количество вычислений в самых разных технических отраслях.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2505.14683v1" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Emerging Properties in Unified Multimodal Pretraining&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;Работа о первой унифицированной мультимодальной модели Bagel, выложенной в открытый доступ. Модель умеет принимать на вход и выдавать на выходе любые комбинации текста и картинок. Это позволяет в рамках одной VLM делать генерацию и редактирование картинок по тексту — возможности, которые раньше публично почти не были доступны.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2511.21631" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Qwen3-VL Technical Report&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;Результат развития линейки моделей Qwen-VL. На момент публикации представляет собой state-of-the-art опенсорсную VLM на большинстве мультимодальных бенчмарков. В статье авторы систематизируют ключевые принципы построения современных визуально-языковых моделей и подробно разбирают архитектурные новшества. Среди них — усовершенствованный interleaved-MRoPE для корректного позиционного кодирования пространственно-временных данных, а также интеграция многоуровневых визуальных признаков через механизм DeepStack. С этими решениями модель может эффективно работать с длинными контекстами и сложными визуально-текстовыми зависимостями.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2508.02324" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Qwen-Image Technical Report&lt;br&gt;&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;После VLM для распознавания и рассуждений логично посмотреть на вторую половину мультимодальности — генерацию и редактирование контента. Здесь у Qwen вышла отдельная модель: Qwen-Image, построенная на трансформерной архитектуре с 3D RoPE. Модель отличается улучшенной генерацией текста и точностью редактирования изображений. Также в статье описана структура датасета для мультимодального обучения модели.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2509.02544" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;В 2025 году направление визуальных GUI-агентов стало активно развиваться, и линейка UI-TARS демонстрирует одни из лучших результатов в этом классе задач. В статье основной акцент сделан на тщательном подходе к формированию обучающих данных и на деталях онлайн multi-turn reinforcement learning. Авторы подробно описывают асинхронную генерацию траекторий, дизайн reward-системы и использование специализированных доменных моделей для дальнейшего их объединения. Такой подход позволяет агенту эффективно осваивать сложные многошаговые сценарии взаимодействия с интерфейсами.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2510.26583" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Emu3.5: Native Multimodal Models are World Learners&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;В статье реализован унифицированный подход к обучению предсказания картиночных и текстовых токенов. Он позволяет модели лучше улавливать причинно-следственные связи и переносить знания между модальностями, что улучшает результаты в задачах восприятия, рассуждения и генерации.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2508.10104" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;DINOv3&lt;br&gt;&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;Статья, в которой описано развитие одного из самых сильных визуальных бэкбонов. Такие модели становятся стандартом визуальных бэкбонов; напрямую влияют на качество VLM, OCR, видео- и downstream-задач; масштабируются лучше многих альтернатив; используются как учителя для дистилляции.&lt;br&gt;&lt;br&gt;Продолжение следует.&lt;br&gt;&lt;br&gt;&lt;em&gt;Статьи отобрали &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt; Александр Устюжанин, Данил Кашин и Александр Шишеня&lt;br&gt;&lt;/em&gt;&lt;a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>🎉Итоги года в CV Time: посты, которые читали чаще всего</title><link>https://t.me/timeforcv/227</link><guid>https://t.me/timeforcv/227</guid><pubDate>Mon, 29 Dec 2025 12:15:10 +0000</pubDate><description>🎉&lt;strong&gt;Итоги года в CV Time: посты, которые читали чаще всего&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Пока все постепенно уходят в мандариново-выходной режим, мы решили подвести итоги года, собрав самые популярные публикации в канале за 2025-й. Это уже стало праздничной традицией, которую мы рады разделить с вами, дорогие читатели, и заодно — поздравить вас с наступающим Новым годом! А если считаете, что в топе чего-то не хватает, приходите обсуждать в комментарии.&lt;br&gt;&lt;br&gt;&lt;a href="https://t.me/timeforcv/140" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Yandex Alchemist: открытый датасет для буста text-to-image генерации&lt;br&gt;&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;Пост, в котором исследователи Yandex Research подробно рассказали, как получить датасет уровня Alchemist, имея лишь сырой набор интернет-данных. Интересное (и даже эксклюзивное) дополнение от авторов к &lt;a href="https://arxiv.org/abs/2505.19297v1" rel="nofollow noopener noreferrer"&gt;основной статье&lt;/a&gt;. Кстати, в этом году работа успела &lt;a href="https://t.me/MLunderhood/235" rel="nofollow noopener noreferrer"&gt;съездить&lt;/a&gt; на NeurIPS 2025.&lt;br&gt;&lt;br&gt;&lt;a href="https://t.me/timeforcv/180" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Эволюция Florence: от генеративных моделей к MLLM&lt;br&gt;&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;В этом посте Егор Шестопалов сравнил сразу две статьи о семействе моделей Florence. И пусть по прошествии времени можно сказать, что идея использовать в качестве энкодера в VLM Florence-2 не прижилась, зато разбор получился полезным и собрал свою порцию просмотров.&lt;br&gt;&lt;br&gt;&lt;a href="https://t.me/timeforcv/65" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Главные инсайты CV Week из первых рук&lt;br&gt;&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;Карточки, на которых инженеры из Яндекса рассказывают самое интересное об онлайн-интенсиве по компьютерному зрению, организованном вместе со Школой анализа данных. Рекомендуем полистать, если хотите вспомнить, как это было. А для ностальгии на максималках можно заглянуть ещё и на этот лендинг.&lt;br&gt;&lt;br&gt;&lt;a href="https://t.me/timeforcv/85" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;FoundationStereo: Zero-Shot Stereo Matching&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;Леонид Штанько разобрал статью NVIDIA о восстановлении глубины по стереопаре — двум изображениям, снятым близко расположенными камерами. Камеры смотрят в одном направлении, поэтому каждая 3D-точка оказывается примерно на одной строке в обоих кадрах, но в разных местах. Это упрощает поиск соответствий между пикселями и позволяет восстановить глубину сцены. Ключевые идеи работы вы найдёте в нашем посте.&lt;br&gt;&lt;br&gt;&lt;a href="https://t.me/timeforcv/143" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Improving the Diffusability of Autoencoders&lt;br&gt;&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;Завершаем подборку разбором от Сергея Кастрюлина на тему diffusability латентного пространства. Авторы статьи выясняют, насколько легко диффузионной модели учиться на латентах автоэнкодера. Проблема локальная, но зато в статье есть понятная идея и измеримый эффект. Если ещё не читали, приглашаем ознакомиться.&lt;br&gt;&lt;br&gt;Надеемся, что наступающий год принесёт индустрии, научному сообществу и нам с вами ещё больше вдохновляющих работ на тему компьютерного зрения. А мы будем и дальше держать вас в курсе самого полезного и интересного! &lt;br&gt;&lt;br&gt;&lt;a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>Современные нейросетевые модели для глобального прогноза погоды</title><link>https://t.me/timeforcv/226</link><guid>https://t.me/timeforcv/226</guid><pubDate>Wed, 24 Dec 2025 11:17:33 +0000</pubDate><description>&lt;strong&gt;Современные нейросетевые модели для глобального прогноза погоды&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Недавно вышла модель &lt;a href="https://deepmind.google/science/weathernext/" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;WeatherNext-2&lt;/strong&gt;&lt;/a&gt; от Google, и мы решили рассказать не только о ней, но и в целом о задаче и моделях глобального прогноза погоды.&lt;br&gt;&lt;br&gt;Глобальный прогноз погоды — это задача прогноза эволюции всей земной атмосферы на несколько дней вперёд. Классический подход — численный прогноз погоды (NWP), в котором численно решается система связанных дифференциальных уравнений гидродинамики. Такие расчёты выполняются на суперкомпьютерах более часа, поэтому прогноз на ближайшие часы нельзя получить мгновенно.&lt;br&gt;&lt;br&gt;Из-за хаотичной природы атмосферных процессов применяется ансамблирование: прогноз запускают с немного различающихся начальных условий, получая десятки возможных сценариев. Ансамблевый прогноз — наиболее точный, он позволяет оценить вероятности событий.&lt;br&gt;&lt;br&gt;К 2025 году сформировались базовые требования к DL-моделям глобального прогноза:&lt;br&gt;&lt;br&gt;— пространственное разрешение не грубее 0,25°по широтам и долготам (~28×28 км);&lt;br&gt;— соответствие спектров (проверка физичности);&lt;br&gt;— наличие осадков и желательно метрик, отличных от MAE/RMSE;&lt;br&gt;— поддержка ансамблей.&lt;br&gt;&lt;br&gt;Ключевым фактором развития DL-подходов стало усвоение данных. Современные техники ассимиляции позволили пересобрать архив наблюдений с 1940 года, получив ERA5 — самый полный и согласованный датасет состояния атмосферы на сетке 0,25°. Доступность большого числа качественных данных — благодатная почва для DL-подхода. Стандартный вход DL-моделей — около 72 карт (приземные переменные, переменные по уровням давления и статические поля).&lt;br&gt;&lt;br&gt;&lt;strong&gt;Обзор основных моделей&lt;/strong&gt;&lt;br&gt;&lt;br&gt;За последние годы появились DL-модели глобального прогноза: Pangu Weather, GraphCast, Aurora, GenCast. Все они используют ERA5 и авторегрессионно транслируют состояние атмосферы в будущее.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2211.02556" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Pangu Weather&lt;/strong&gt;&lt;/a&gt; показала, что «картиночная» модель может воспроизводить крупномасштабную динамику, но ансамбли через шум в начальных условиях оказались некачественными.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2212.12794" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;GraphCast&lt;/strong&gt;&lt;/a&gt; использует графовую архитектуру на икосаэдрической сетке и задаёт планку качества для детерминистских моделей. &lt;a href="https://arxiv.org/abs/2312.15796v2" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;GenCast&lt;/strong&gt;&lt;/a&gt; расширил этот подход, применив диффузию для получения ансамблей, что позволило уменьшить «мыло» и лучше моделировать экстремумы, но ценой более медленного инференса.&lt;br&gt;&lt;br&gt;При этом выяснилось, что стандартных метрик (LW-RMSE и ACC) недостаточно: многие модели не проходят проверку на физичность по спектрам. Несоответствие спектров означает, что модель не улавливает вариации энергии на мелких масштабах, и неэффективно использует высокое разрешение.&lt;br&gt;&lt;br&gt;&lt;a href="https://deepmind.google/science/weathernext/" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;WeatherNext-2 &lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;WeatherNext-2 — третья итерация модели Google. Это вероятностная модель, которая напрямую оптимизируется по CRPS и строит ансамбли без диффузии.&lt;br&gt;&lt;br&gt;Ключевая идея — декомпозиция неопределённости:&lt;br&gt;&lt;br&gt;— эпистемическая неопределённость моделируется deep-ансамблем (четыре модели с разными сидами);&lt;br&gt;&lt;br&gt;— алеаторическая неопределённость моделируется через функциональные возмущения: для каждого члена ансамбля и шага сэмплируется один глобальный 32-мерный шумовой вектор, который через conditional layer norm подаётся во все слои модели.&lt;br&gt;&lt;br&gt;Архитектура сохраняет подход GraphCast: переход grid→mesh, граф-трансформер на mesh и обратное отображение. Глобальный низкоразмерный шум, применяемый ко всем слоям и пространственным точкам, задаёт согласованную пространственную вариативность.&lt;br&gt;&lt;br&gt;Модель работает с шагом шесть часов и делает полный 15-дневный прогноз ансамбля менее чем за минуту на одном TPU, что значительно быстрее GenCast. По метрикам CRPS и RMSE среднего ансамбля WeatherNext-2 превосходит GenCast и приближается к численным ансамблям. Про осадки в статье сообщается скупо, спектры лучше, чем у GenCast, но хуже, чем у FourCastNetV3.&lt;br&gt;&lt;br&gt;В целом WeatherNext-2 показывает, что можно получить быстрый ансамбль без диффузии и существенно улучшить качество по сравнению с предыдущими нейромоделями.&lt;br&gt;&lt;br&gt;При этом ключевые вопросы о соответствии спектров и корректной работе с осадками остаются. &lt;br&gt;&lt;br&gt;&lt;em&gt;Разбор подготовил &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt; Павел Анисимов &lt;br&gt;&lt;/em&gt;&lt;a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>SANA-Sprint: One-Step Diffusion with Continuous-Time Consistency Distillation</title><link>https://t.me/timeforcv/225</link><guid>https://t.me/timeforcv/225</guid><pubDate>Mon, 15 Dec 2025 11:58:01 +0000</pubDate><description>&lt;strong&gt;SANA-Sprint: One-Step Diffusion with Continuous-Time Consistency Distillation&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Сегодня разбираем &lt;a href="https://arxiv.org/abs/2503.09641v3" rel="nofollow noopener noreferrer"&gt;статью&lt;/a&gt; от NVIDIA, в которой высокая скорость достигается в первую очередь за счёт генерации изображений в малое число шагов с приемлемым качеством. Прошлые версии SANA быстро генерировали благодаря VAE с большим downsampling-фактором, а в SANA Sprint добились ещё большего ускорения с помощью дистилляции по шагам.&lt;br&gt;&lt;br&gt;Основа работы — идея continuous-time consistency моделей, о которой ещё осенью прошлого года &lt;a href="https://arxiv.org/abs/2410.11081" rel="nofollow noopener noreferrer"&gt;говорил&lt;/a&gt; Yang Song. По сути, она описывает движение от шума к сигналу через временную производную, превращая дискретный диффузионный процесс в непрерывный поток динамики.&lt;br&gt;&lt;br&gt;Сontinuous-time consistency позволяет достигать качественных генераций в малое число шагов, но есть и нюанс. Модель должна быть обучена со специальной TrigFlow-параметризацией, а имеющиеся диффузионные модели обычно используют стандартную flow-matching-постановку. Поэтому следующая задача — правильно «перевести» предобученную модель в нужное представление.&lt;br&gt;&lt;br&gt;SANA-Sprint решает это с помощью серии преобразований:&lt;br&gt;— переноса временной шкалы в тригонометрические координаты (cos / sin),&lt;br&gt;— масштабирования латентов, чтобы шум совпадал по дисперсии с данными,&lt;br&gt;— трансформации выходной head-функции, чтобы предсказания соответствовали формуле consistency-динамики.&lt;br&gt;&lt;br&gt;Но перенести диффузионку в новую параметризацию — это только половина дела. Вторая часть — заставить всё это стабильно учиться. И вот здесь начинаются инженерные приключения. Стабильность «улетает в космос» из-за того, что временной эмбеддинг использует слишком большой масштаб шума — из-за этого производные становятся огромными. Лечится это просто: нужно изменить масштаб частот эмбеддинга и немного дообучить модель, буквально несколько тысяч итераций.&lt;br&gt;&lt;br&gt;Вторая проблема — большие нормы градиентов в механизме внимания. Решение довольно стандартное: добавить RMSNorm на Q/K (QK-Normalization) в self- и cross-attention, после чего обучение стабилизируется.&lt;br&gt;&lt;br&gt;Теперь самое главное — скорость. В разрешении 1024×1024 SANA-Sprint выдаёт картинку за ~0,1–0,18 секунды при одношаговой генерации. Из них на сам трансформер уходит ≈0,03 секунды, остальное — VAE-декодер, который становится основным бутылочным горлышком. По времени работы диффузионной модели SANA-Sprint быстрее FLUX-schnell примерно в 65 раз, а по end-to-end-задержке — примерно в 10 раз. То есть «быстро» тут — не просто эпитет.&lt;br&gt;&lt;br&gt;Итоговое качество вполне пристойное: на 1–4 шагах она даёт FID и GenEval на уровне или лучше, чем у других быстрых моделей. Например, не уступает FLUX-schnell по метрикам (7,59 против 7,94 по FID и 0,74 против 0,71 по GenEval), будучи заметно быстрее.&lt;br&gt;&lt;br&gt;&lt;em&gt;Разбор подготовил &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt; Денис Кузнеделев&lt;br&gt;&lt;/em&gt;&lt;a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>Байки из склепа прода Alice AI VLM</title><link>https://t.me/timeforcv/221</link><guid>https://t.me/timeforcv/221</guid><pubDate>Thu, 11 Dec 2025 10:16:40 +0000</pubDate><description>&lt;strong&gt;Байки из &lt;del&gt;склепа&lt;/del&gt; прода Alice AI VLM&lt;br&gt;&lt;/strong&gt;&lt;br&gt;Сегодня делимся двумя &lt;del&gt;скримерами&lt;/del&gt; историями из первых рук о том, с какими сложностями столкнулись разработчики новой &lt;a href="https://alice.yandex.ru/" rel="nofollow noopener noreferrer"&gt;Алисы AI&lt;/a&gt; в продакшне.  &lt;br&gt;&lt;br&gt;Популярный сценарий использования нейросети — когда пользователь отправляет в чат картинку и просит помочь с тем, что на ней изображено. За этот навык отвечают Alice AI VLM и команда компьютерного зрения Яндекса, которая её развивает. Слово руководителю подгруппы распознавания текста в VLM Антону Клочкову @blog_toxa.&lt;br&gt;&lt;br&gt;&lt;blockquote&gt;&lt;strong&gt;Проблема первая: пережатие картинок&lt;/strong&gt;&lt;br&gt;&lt;strong&gt;&lt;br&gt;&lt;/strong&gt;Те, кто имел дело с сервисами, где есть работа с картинками, не дадут соврать: найти баланс между качеством и скоростью загрузки изображений — сложная задача. Иногда баланс перевешивает в одну из сторон, и в нашем случае была проблема качества.&lt;br&gt;&lt;br&gt;Как-то во время тестирования Алисы AI прилетает баг-репорт: фотография из учебника и комментарий: «Формулы выписываются неверно!» &lt;em&gt;(см. картинку 1)&lt;/em&gt;. &lt;br&gt;&lt;br&gt;Проверяем в тестинге — есть ошибка. Прогоняем офлайн через модель — ошибки нет. Странно? Очень!&lt;br&gt;&lt;br&gt;Оказалось, что в продакшене сильно пережимаются изображения &lt;em&gt;(см картинку 2)&lt;/em&gt;. Из-за этого путаются мелкие обозначения, вроде знаков неравенства, и иногда теряется весь смысл. Фикс был простой: мы ослабили правила на пережатие картинок.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Проблема вторая: парсинг LaTeX&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Наши первые шаги к тому, чтобы сделать Алису AI действительно умной, проходили в Поиске по картинкам — там уже была готовая инфраструктура, а в чате ещё требовалась донастройка.&lt;br&gt;&lt;br&gt;Однажды пришла пора тестировать решение в сервисе. И в целом, всё было хорошо, кроме одной детали. Оказалось, что на разных поверхностях (в нашем случае — Поиска и Алисы AI) по-разному работают правила парсинга LaTeX-вставок в Markdown. Например, в Поиске по картинкам формулы отображались одним образом &lt;em&gt;(см. картинку 3)&lt;/em&gt;, а в Алиса AI — другим &lt;em&gt;(см. картинку 4)&lt;/em&gt;. И это было не единственное различие в парсинге. &lt;br&gt;&lt;br&gt;Решили мы это в одних случаях дообучением VLM на форматы, в других — правками во фронтенде.&lt;/blockquote&gt;&lt;br&gt;&lt;br&gt;Алиса AI — это не только Alice AI VLM, о которой мы пишем в этом посте, но и Alice AI LLM, Alice AI LLM Search, Alice AI ART, а ещё много крутых инженерных решений. Если хотите больше технических деталей, советуем почитать свежий &lt;a href="https://habr.com/ru/companies/yandex/articles/974594/" rel="nofollow noopener noreferrer"&gt;техрепорт&lt;/a&gt;. А ознакомиться с главными фичами можно на &lt;a href="https://alice.yandex.ru/about" rel="nofollow noopener noreferrer"&gt;лендинге&lt;/a&gt;. &lt;br&gt;&lt;br&gt;&lt;a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning and Efficiency</title><link>https://t.me/timeforcv/220</link><guid>https://t.me/timeforcv/220</guid><pubDate>Tue, 09 Dec 2025 09:53:01 +0000</pubDate><description>&lt;strong&gt;InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning and Efficiency&lt;br&gt;&lt;/strong&gt;&lt;br&gt;Авторы опенсорс-семейства InternVL постоянно выпускают всё новые и новые улучшения своих мультимодальных моделей, которые опережают SoTA-результаты в первую очередь по бенчмаркам. Сегодня разберём &lt;a href="https://arxiv.org/abs/2508.18265" rel="nofollow noopener noreferrer"&gt;статью&lt;/a&gt; о свежей версии InternVL3.5.&lt;br&gt;&lt;br&gt;В основе улучшений — три основных нововведения.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Cascade Reinforcement Learning&lt;br&gt;&lt;/strong&gt;&lt;br&gt;Раньше модели InternVL использовали MPO в качестве offline RL. В новой версии 3.5 авторы добавили ещё и online RL: принято считать, что на LLM/VLM он гораздо лучше, чем offline. Но offline RL значительно легче по вычислениям (в основном из-за того, что во время обучения не нужно генерировать ответы на инструкции).&lt;br&gt;&lt;br&gt;Авторы показали, что offline RL не так уж сильно отстаёт от online RL, но при этом обучается в 20 раз быстрее. А лучшее качество модели достигается при совместном каскадном обучении: результаты лучше, чем у online RL, даже на двух эпохах. Так offline RL превратился в warmup для online RL.&lt;br&gt;&lt;br&gt;В качестве online RL используется GSPO — модификация GRPO, которая решает проблему нестабильности обучения и «коллапса модели», особенно при тренировке Mixture-of-Experts-моделей. GRPO работает на уровне отдельных токенов, создавая шумные градиенты, а GSPO применяет оптимизацию на уровне всей последовательности целиком, что важно для длинных цепочек рассуждений.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Visual Resolution Router (ViR)&lt;br&gt;&lt;/strong&gt;&lt;br&gt;Основная цель этого нововведения — снизить вычислительную нагрузку на модель во время инференса. Этого удалось добиться за счёт уменьшения количества визуальных токенов в представлении каждого кропа картинки. Сколько токенов нужно выделить на кроп, решает роутер. Среднее количество визуальных токенов, поступающих в LLM, при таком подходе сокращается на 50%. &lt;br&gt;&lt;br&gt;Стандартный процесс кодирования картинки выглядит так: &lt;br&gt;&lt;br&gt;— изображение делится на кропы, &lt;br&gt;— каждый патч преобразуется в 1024 токена для ViT, &lt;br&gt;— после обработки ViT количество токенов уменьшается адаптером до 256 и передаются в LLM. &lt;br&gt;&lt;br&gt;Роутер может направить токены в более агрессивный адаптер и сжать до 64 токенов. Обучение происходит в два этапа. На первом этапе модель тренируется решать задачу с меньшим количеством токенов за счёт минимизации KL-дивергенции между распределениями выходных данных изначального сжатия и более агрессивного сжатия.&lt;br&gt;&lt;br&gt;Цель второго этапа — научить сам роутер ViR принимать правильные решения о степени сжатия для каждого кропа. ViR обучается как стандартный бинарный классификатор, где label кропа определяется по значению loss из первого этапа.&lt;br&gt;&lt;br&gt;Итог — flash-модель практически без потери качества с ускорением до 4 раз (точная цифра зависит от разрешения картинки и размера модели).&lt;br&gt;&lt;br&gt;&lt;strong&gt;DvD (Decoupled Vision-Language Deployment)&lt;/strong&gt;&lt;br&gt;&lt;br&gt;В этой системе модель для обработки изображений (ViT) и языковая модель (LLM) разворачиваются на отдельных серверах или GPU.&lt;br&gt;&lt;br&gt;Они работают не последовательно (сначала картинка, потом текст), а параллельно. Пока языковая модель генерирует ответ на предыдущий запрос, визуальный энкодер уже обрабатывает следующее изображение. Это даёт ускорение до 2 раз для базовых моделей, а в комбинации с ViR — до 4 раз на высоких разрешениях. &lt;br&gt;&lt;br&gt;По словам авторов, новая InternVL3.5 рассуждает на +16,0% эффективнее и в 4,05 раз быстрее, чем её предшественники. &lt;br&gt;&lt;br&gt;&lt;em&gt;Разбор подготовил &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt; Антон Астахов&lt;br&gt;&lt;/em&gt;&lt;a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>NeurIPS в Мехико: продолжаем делиться интересным</title><link>https://t.me/timeforcv/213</link><guid>https://t.me/timeforcv/213</guid><pubDate>Fri, 05 Dec 2025 13:31:42 +0000</pubDate><description>&lt;strong&gt;NeurIPS в Мехико: продолжаем делиться интересным&lt;br&gt;&lt;br&gt;&lt;/strong&gt;Червёртый день конференции в Мексике получился насыщенным. Было выступление Ричарда Саттона о его видении SuperIntelligence, две сессии со статьями и две — с постерами.&lt;br&gt;&lt;br&gt;Самая интересная статья дня, по мнению Владислава Фахретдинова, — &lt;a href="https://arxiv.org/abs/2504.13181" rel="nofollow noopener noreferrer"&gt;Perception Encoder: The best visual embeddings are not at the output of the network&lt;/a&gt; от Meta*. Мы уже &lt;a href="https://t.me/c/2494702394/249" rel="nofollow noopener noreferrer"&gt;разбирали&lt;/a&gt; работу в канале, а теперь делимся тем, что о ней говорят сами авторы.&lt;br&gt;&lt;br&gt;&lt;blockquote&gt;Исследователи рассказывают, что поставили перед собой цель создать лучший визуальный энкодер для многих downstream-задач. Для этого двухстадийно обучались контрастив-лоссом на парах «изображение-текст» и потом — на парах «видео–текст», используя свою модель как кадровый энкодер.&lt;br&gt;&lt;br&gt;Начав с CLIP-бейзлайна, добавили ряд улучшений и сравнили их по качеству и устойчивости. Уже на этом этапе модель достигла SOTA в zero-shot retrieval и классификации; назвали её PE_core.&lt;br&gt;&lt;br&gt;Затем авторы протестировали модель как энкодер на разных downstream-задачах: детекции, трекинге, предсказании глубин. Увидели, что перфоманс оказался ниже ожидаемого.&lt;br&gt;&lt;br&gt;В ходе исследования с помощью аттеншен-карт заметили появление глобальных токенов на определённом слое. Чтобы проверить гипотезу, стали брать эмбеддинги не с последнего слоя, а с предыдущих. Построив график качества по слоям для разных downstream-задач и моделей, увидели, что качество растёт к эмбеддингам средних слоёв, а к последним слоям — резко падает.&lt;br&gt;&lt;br&gt;Для решения этой проблемы использовали два метода после обучения:&lt;br&gt;&lt;br&gt;1. Чтобы сохранить глобальную информацию, провели файнтьюн на 41-м слое (который показывает близкие к лучшим значениям по всем задачам) с минимизацией косинусного расстояния между ним и последним слоем.&lt;br&gt;&lt;br&gt;2. Чтобы сохранить локальную информацию, добавили файнтьюн на MSE попарного косинусного расстояния между эмбеддингами последнего слоя (H×W×1024 -&amp;gt; HW×HW) и попарного косинусного расстояния между логитами SAM для 1024 точек из равномерной сетки исходного изображения.&lt;br&gt;&lt;br&gt;Эту модель авторы назвали PE_spatial и показали, что она достигает SOTA по многим downstream-задачам. Хотя вышедший позже DinoV3 достиг более высоких результатов, подход остаётся интересным.&lt;/blockquote&gt;&lt;br&gt;&lt;br&gt;#YaNeurIPS25&lt;br&gt;&lt;br&gt;&lt;a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;&lt;br&gt;___&lt;br&gt;&lt;em&gt;Meta признана экстремистской организацией, а Facebook и Instagram запрещены на территории РФ&lt;/em&gt;</description></item><item><title>NeurIPS в Мехико: туториал о геопространственных foundation-моделях</title><link>https://t.me/timeforcv/203</link><guid>https://t.me/timeforcv/203</guid><pubDate>Wed, 03 Dec 2025 13:09:43 +0000</pubDate><description>&lt;strong&gt;NeurIPS в Мехико: туториал о геопространственных foundation-моделях&lt;br&gt;&lt;/strong&gt;&lt;br&gt;В третий день конференции прошло большое количество туториалов. Один из них — &lt;a href="https://neurips.cc/virtual/2025/loc/mexico-city/128793" rel="nofollow noopener noreferrer"&gt;Geospatial Foundation Models: Overview, Application and Benchmarking&lt;/a&gt; — посетил Владислав Фахретдинов из команды восприятия робота доставки. Делимся его заметками!&lt;br&gt;&lt;br&gt;&lt;blockquote&gt;Выступали докладчики из бразильского подразделения IBM Research. Начали с рассказа о задаче remote sensing — дистанционного зондирования по спутниковым данным. Основное отличие от классических задач компьютерного зрения в том, что кроме RGB-сигналов необходимо использовать и другие спектральные каналы, у каждого из которых есть своё физическое назначение.&lt;br&gt;&lt;br&gt;На основе этих данных можно решать множество задач, таких как сегментация земного покрова, пожарных шрамов и наводнений, предсказание глубины для водного покрова и процента покрытия деревьями.&lt;br&gt;&lt;br&gt;Затем был базовый экскурс в развитие компьютерного зрения: от свёрточных моделей и трансформеров до автоэнкодеров, а после — рассказ о foundation-моделях в этой сфере. &lt;br&gt;&lt;br&gt;Докладчики представили множество работ, в которых главный архитектурный вопрос состоит в том, как правильно объединять данные из разных каналов (модальностей). Отчасти это связано с тем, что нельзя просто склеить все каналы из-за отличий в разрешении, поэтому используются разные подходы:&lt;br&gt;&lt;br&gt;— отдельные энкодер и декодер для каждой модальности, но общий аттеншн; &lt;br&gt;— динамический подбор размеров патчей для каждой модальности на основе длины волны и общий энкодер;&lt;br&gt;— либо разные энкодеры, но совместный семплинг патчей со всех модальностей на этапе претрейна.&lt;br&gt;&lt;br&gt;После этого исследователи рассказали о своём фреймворке для обучения геопространственных моделей TerraTorch. На практике — собрали ноутбук с обучением двум разным задачам: land segmentation и burn scars.&lt;br&gt;&lt;br&gt;Также авторы представили свой новый бенчмарк GeoBenchV2, который сгруппировали из 19 существующих датасетов. Взяли множество популярных в CV моделей для сравнения и дофайнтюнили их на разные задачи только на основе RGB.&lt;br&gt;&lt;br&gt;В итоге оказалось, что общие модели, такие как DinoV3, дают гораздо лучшие предсказания на основе RGB-изображений, но на задачах с мультиспектральными данными более маленькие, но узкоспециализированные модели всё ещё побеждают.&lt;/blockquote&gt;&lt;br&gt;&lt;br&gt;#YaNeurIPS25&lt;br&gt;&lt;br&gt;&lt;a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>NeurIPS 2025 в Мехико идёт полным ходом</title><link>https://t.me/timeforcv/195</link><guid>https://t.me/timeforcv/195</guid><pubDate>Tue, 02 Dec 2025 13:00:12 +0000</pubDate><description>&lt;strong&gt;NeurIPS 2025 в Мехико идёт полным ходом &lt;/strong&gt;&lt;br&gt;&lt;br&gt;Конференция продолжается, а наш коллега Владислав Фахретдинов делится заметками о воркшопе второго дня — &lt;a href="https://neurips.cc/virtual/2025/loc/mexico-city/workshop/127828" rel="nofollow noopener noreferrer"&gt;7th International Workshop on Large Scale Holistic Video Understanding: Toward Video Foundation Models&lt;/a&gt;. &lt;br&gt;&lt;br&gt;&lt;blockquote&gt;Было немного спикеров, но почти каждый привёз по две-три статьи или исследования, поэтому день получился насыщенным. Основной мотив воркшопа — большинство моделей для работы с видео недостаточно хорошо ориентируются «во времени». Участники разбирались, что с этим можно сделать.&lt;br&gt;&lt;br&gt;Первым выступил профессор университета Амстердама. Он заметил, что многие VideoLLM не справляются даже с простым синтетическим бенчмарком: какой из двух объектов в видео появляется раньше. Это показывает, что мы до конца не понимаем, как правильно оценивать такие способности модели.&lt;br&gt;&lt;br&gt;Затем последовал рассказ о работе Bench of Time с более подробными исследованиями — оказалось, что большинство примеров в популярном бенчмарке (MVBench) решается либо знанием всего об одном кадре, либо вообще исключительно по тексту. Чтобы исправить эту ситуацию, авторы сделали свой бенчмарк TVBench. В нём все вопросы были сформулированы так, что без понимания объектов и процессов в кадре нельзя дать правильный ответ. &lt;br&gt;&lt;br&gt;Сравнение моделей на новом бенчмарке показало, что большинство языковых, картиночных и даже видеомоделей выдают результаты немногим лучше случайного предсказания. При этом все же нашлись несколько моделей, которые были достаточно хороши на обоих бенчмарках, например Gemini-1.5.&lt;br&gt;&lt;br&gt;Следом было выступление о генерации 3D-представления из изображения. По сути, это продолжение работы &lt;a href="https://arxiv.org/abs/2312.14132" rel="nofollow noopener noreferrer"&gt;DUSt3R&lt;/a&gt;, в которой научились по любым входным изображениям без параметров камер и поз делать матчинг и генерировать плотное облако точек 3D-представления сцены. &lt;br&gt;&lt;br&gt;Авторы сделали уточнение, что матчинг изображений по случайному видео с движением — вычислительно сложная задача. Поэтому они собрали датасет 360-1M, где происходит движение и вращение вокруг оси, из-за чего матчить изображения стало гораздо проще. На основе своего датасета они обучили генеративную модель ODIN, которая по изображению и смещению позиции камеры генерирует новое изображение. Подробностей было мало, никаких сравнений с DUSt3R или NeRF не показали, но зато рассказали, что модель хорошо обобщается вне домена — например, на картины.&lt;br&gt;&lt;br&gt;Самый интересный доклад за день — о том, что визуальные модели знают о нашем мире. Авторы выделили и проверили три свойства: базовое представление о физическом устройстве мира, визуальное предсказание, а также обобщение — понимание аналогий.&lt;br&gt;&lt;br&gt;Для первого свойства взяли часовые видео с прогулками по городам и с помощью сервиса визуальной локализации, а также небольшого объёма человеческой проверки, разметили эти видео. В частности, для каждого видео сгенерировали маршрут на карте.&lt;br&gt;&lt;br&gt;Далее видео нарезали и собрали бенчмарк, в котором модели задавали вопросы по содержанию ролика, например: о евклидовом расстоянии от начальной до конечной точки на полученном маршруте; направлении; зацикленность маршрута; выборе правильного трека на карте среди нескольких вариантов (с текстом на карте и без текста); распознавании окружающей архитектуры. По всем этим вопросам модели уступают человеку — за исключением проверки на зацикленность маршрута.&lt;br&gt;&lt;br&gt;Авторы также показали, что на самом деле модели не понимали, был цикл в маршруте или нет. Вместо этого они просто смотрели на разметку на карте и сопоставляли её с текстовыми названиями улиц, которые видны в видео.&lt;br&gt;&lt;br&gt;Напоследок был доклад из трёх частей, из которых я бы выделил как самую интересную — SSL-обучение мультимодальной модели видео+аудио CAV-MAE Sync. Из того, что мне кажется важным: авторы совместно используют аудио- и видеопатчи и добавляют регистровый токен, чтобы переносить накопленную информацию в следующие слои. Больше всего мне понравилось, что новая модель позволяет локализовать на видео источники звука.&lt;/blockquote&gt;&lt;br&gt;&lt;br&gt;#YaNeurIPS25&lt;br&gt;&lt;br&gt;&lt;a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>DeepSeek-OCR: Contexts Optical Compression [2/2]</title><link>https://t.me/timeforcv/194</link><guid>https://t.me/timeforcv/194</guid><pubDate>Fri, 28 Nov 2025 08:33:01 +0000</pubDate><description>&lt;strong&gt;DeepSeek-OCR: Contexts Optical Compression [2/2]&lt;/strong&gt;&lt;br&gt;&lt;br&gt;В &lt;a href="https://t.me/timeforcv/193" rel="nofollow noopener noreferrer"&gt;первой части&lt;/a&gt; разбора мы рассказали об особенностях архитектуры &lt;a href="https://arxiv.org/abs/2510.18234" rel="nofollow noopener noreferrer"&gt;DeepSeek-OCR&lt;/a&gt; и ключевых задачах, которые решали авторы. А теперь посмотрим на нюансы обучения и на озвученные результаты.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Обучение модели&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Процесс упрощён и включает только две стадии: тренировку энкодера и обучение модели целиком.&lt;br&gt;&lt;br&gt;Важный момент: во время тренировки энкодера DeepEncoder учится работать и в режиме native-resolution, и в режиме tile-based-resolution. То есть модель видит как большие картинки, так и маленькие в разных представлениях. &lt;br&gt;&lt;br&gt;Энкодер тренируется на парах картинок и текстовых описаний по схеме, описанной в статье Vary: к нему приделывается маленький текстовый декодер, и они вместе обучаются авторегрессионно.&lt;br&gt;&lt;br&gt;Второй этап с обучением всей VLM повторяет обычный претрейн/SFT во множестве других VLM. &lt;br&gt;&lt;br&gt;&lt;strong&gt;Результаты&lt;/strong&gt;&lt;br&gt; &lt;br&gt;Авторы представляют небольшую мультиязычную модель, которая может обрабатывать изображения в разном размере и даже в разных режимах динамического разрешения (tile-based, native-resolution). &lt;br&gt;&lt;br&gt;Замеры точности распознавания в зависимости от размера изображения (и числа токенов) на OCR-бенчмарке Fox показывают, что для надёжного чтения текста можно использовать примерно &lt;strong&gt;в 10 раз меньше картиночных токенов, чем необходимо текстовых токенов для представления текста на изображении&lt;/strong&gt;. При уменьшении этого соотношения качество чтения быстро падает.&lt;br&gt;&lt;br&gt;DeepSeek-OCR показывает отличное качество на OmniDocBench, опережая в зависимости от разрешения не только сильные опенсорсные бэйзлайны, вроде Qwen-2.5VL, но и Gemini2.5-Pro. При этом скорость обработки на GPU сопоставима с пайплайновыми OCR-пакетами, такими как Miner, обрабатывая около двух изображений в секунду на А100.&lt;br&gt;&lt;br&gt;В заключение можно заметить, что хотя результаты вышли довольно впечатляющими, в работе использованы только бенчмарки с фокусом на PDF-подобных картинках, а другие, более разнообразные OCR-бенчи для VLM (OCRBench_v2, CC-OCR), не замеряны. Также в статье нет аблейтов влияния на результаты ни выбранной архитектуры, ни этапов обучения, поэтому авторы сами называют свои результаты proof-of-concept.&lt;br&gt;&lt;br&gt;&lt;em&gt;Разбор подготовил &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt; Борис Зимка&lt;br&gt;&lt;/em&gt;&lt;a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>DeepSeek-OCR: Contexts Optical Compression [1/2]</title><link>https://t.me/timeforcv/193</link><guid>https://t.me/timeforcv/193</guid><pubDate>Tue, 25 Nov 2025 10:06:12 +0000</pubDate><description>&lt;strong&gt;DeepSeek-OCR: Contexts Optical Compression [1/2]&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Сегодня начинаем разбирать недавнюю &lt;a href="https://arxiv.org/abs/2510.18234" rel="nofollow noopener noreferrer"&gt;статью DeepSeek-OCR&lt;/a&gt;. Авторы работы сфокусировались на двух аспектах:&lt;br&gt;&lt;br&gt;1. обучении эффективной VLM-модели, заточенной именно под OCR-задачи;&lt;br&gt;2. изучении влияния размера входного изображения на качество работы VLM (и компрессии визуальной информации в целом).&lt;br&gt;&lt;br&gt;Сначала небольшое интро по каждому из этих аспектов.&lt;br&gt;&lt;br&gt;&lt;strong&gt;OCR-специфичные VLM-модели&lt;br&gt;&lt;/strong&gt;&lt;br&gt;Задачи, связанные с чтением текста, встречаются довольно часто и у простых пользователей, и в бизнес-процессах компаний. Такие задачи не требуют знания фактов, агентности, рассуждений, и тратить много GPU на них жалко. За последний год вышло несколько статей по OCR-специализированным легковесным VLM (GOT, Dolphin, UMiner, dots.ocr).&lt;br&gt;&lt;br&gt;&lt;strong&gt;Динамическое разрешение в VLM&lt;br&gt;&lt;/strong&gt;&lt;br&gt;Первые VLM, вроде LLaVA, использовали статический размер изображения: любая картинка для обработки ресайзилась к фиксированному квадрату, прогонялась через энкодер (например CLIP), готовя картиночные токены на вход LLM. Так как изображение на входе может быть и пиксельной строкой текста 128 х 16, и большим фото со смартфона 1500 х 4500 пикселей — статический размер работает не оптимально. Сегодня для VLM есть два основных способа сделать разрешение динамическим:&lt;br&gt;&lt;br&gt;1. Tile-based-resolution (Intern-VL2) — изображение разрезается на квадраты, например 512х512 пикселей, и каждое прогоняется через картиночный энкодер. Все выходные токены (чем больше размер — тем больше тайлов и токенов) подаются на вход LLM. &lt;br&gt;&lt;br&gt;2. Native-resolution (Qwen-VL2) — картиночный энкодер обучается принимать на вход изображение любого размера, используя подходящие для этого позицинные эмбеддинги типа RoPE.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Модель и данные&lt;/strong&gt;&lt;br&gt;&lt;br&gt;DeepSeek-OCR архитектурно повторяет стандартную для VLM схему: картиночный энкодер, присоединенный к предобученной LLM (в этом случае DeepSeek-3B). &lt;br&gt;&lt;br&gt;Однако вместо стандартного CLIP/SigLIP в качестве энкодера используется пайплайн из SegmentAnything (SAM-ViT-Det), свёрточного адаптера и CLIP (CLIP-ViT), который в статье называют DeepEncoder. Авторы хотели, чтоб энкодер был эффективным и быстрым, и чтобы в уже обученном энкодере можно было легко «на лету» менять количество картиночных токенов.&lt;br&gt;&lt;br&gt;SAM-ViT-Det может принимать на вход изображение любого размера; токенизированные патчи обрабатываются независимо друг от друга благодаря window attention — поэтому количество вычислений уменьшается. Затем адаптер снижает количество токенов в 16 раз, а после глобальный аттеншн в CLIP-ViT агрегирует их вместе. &lt;br&gt;&lt;br&gt;Для обучении используется типичная смесь пар (картинка-описание) и только текстовых данных с упором на OCR: печатный текст, графики и таблицы, формулы. В отличие от других OCR-специализированных VLM (обычно обучаемых только на английском и китайском), датасеты содержат более 100 языков.&lt;br&gt;&lt;br&gt;Во второй части подробнее разберём, как обучали DeepSeek-OCR и к каким результатам пришли авторы.&lt;br&gt;&lt;br&gt;&lt;em&gt;Разбор подготовил &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt; Борис Зимка&lt;br&gt;&lt;/em&gt;&lt;a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>X-Fusion: Introducing New Modality to Frozen Large Language Models</title><link>https://t.me/timeforcv/192</link><guid>https://t.me/timeforcv/192</guid><pubDate>Tue, 18 Nov 2025 11:16:18 +0000</pubDate><description>&lt;strong&gt;X-Fusion: Introducing New Modality to Frozen Large Language Models&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Сейчас индустрия унифицирует подходы к обработке различных видов данных. Существенную часть задач компьютерного зрения решают VLM: генерируют текст на основе изображений и запросов, которые получают на вход. Следующий шаг — наделить модели возможностью генерировать изображения. &lt;br&gt;&lt;br&gt;Изображения, в отличие от текстов, недискретные, поэтому для них лучше применять вариации диффузионных лоссов, а не next-token prediction. Сегодня рассмотрим &lt;a href="https://arxiv.org/abs/2504.20996" rel="nofollow noopener noreferrer"&gt;статью&lt;/a&gt;, где предлагается объединить в одной системе два лосса.&lt;br&gt;&lt;br&gt;Суперверхнеуровневая схема нового фреймворка X-Fusion — на иллюстрации к посту. Авторы предлагают использовать две одинаковых предобученных LLM: первую заморозить, чтобы она стабильно хорошо справлялась с текстовыми задачами. А её копию — назвать визуальной башней и дообучить для работы с изображениями. &lt;br&gt;&lt;br&gt;Если нужно обработать изображение, то закодируем его VAE от SD-1,5 и подадим на вход визуальной башне. Таким образом, генерация текста происходит через предсказание следующего токена. А для создания изображений выберем токены, расшумим их диффузией и декодируем VAE.&lt;br&gt;&lt;br&gt;Авторы сравнили четыре базовые архитектуры: &lt;br&gt;— Единообразно обрабатывать текстовые и картиночные входы одним трансформером.&lt;br&gt;— Дублировать каждый слой LLM-gated-слоем. Обучать только визуальные слои, результаты складывать, а визуальный выход домножать на обучаемый скаляр.&lt;br&gt;— Схема с двойной проекцией: копировать и добучать QKV-матрицы и MLP для визуальной модальности. &lt;br&gt;— Финальный вариант: две башни, одна из которых применяется для текстовой модальности, а вторая — для визуальной. А потом либо использовать (в целях экономии вычислений) выходы из соответствующих башен, либо суммировать их с некоторыми весами.&lt;br&gt;&lt;br&gt;X-Fusion обучали на синтетике: caption сгенерировали InternVL-2.0 26B. А для text-to-image взяли свой inhouse-датасет. Хотя по словам авторов, подход с двумя башнями превосходит другие базовые решения в задачах создания изображений, в обратную сторону это не работает: задача генерации текста не помогает получать хорошие caption для изображений. Авторы также изучают, стоит ли зашумлять входные латенты для задач распознавания изображений. Их вывод — нет, это приводит к деградации качества. &lt;br&gt;&lt;br&gt;&lt;em&gt;Разбор подготовил &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt; Сергей Овчаренко &lt;/em&gt;&lt;br&gt;&lt;a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>ERNIE 4.5 Technical Report [2/2]</title><link>https://t.me/timeforcv/191</link><guid>https://t.me/timeforcv/191</guid><pubDate>Fri, 14 Nov 2025 10:09:46 +0000</pubDate><description>&lt;strong&gt;ERNIE 4.5 Technical Report [2/2]&lt;br&gt;&lt;/strong&gt;&lt;br&gt;Продолжаем разбирать &lt;a href="https://yiyan.baidu.com/blog/publication/" rel="nofollow noopener noreferrer"&gt;технический репорт&lt;/a&gt; от Baidu. В работе фактически выделены два независимых пайплайна алаймента: LLM и VLM. После посттрейна в мультимодальном семействе модели получаются гибридными: они работают и в режиме с ризонингом, и без него. При этом авторы не объясняют, как эти два направления соотносятся между собой и как VLM-компонента влияет на метрики LLM (и наоборот).&lt;br&gt;&lt;br&gt;&lt;strong&gt;LLM-линия: SFT и RL с множеством ревордов&lt;/strong&gt;&lt;br&gt;&lt;br&gt;На этапе SFT всё довольно просто — собрали и сбалансировали нужные срезы под задачи. Дальше идёт многостадийный RL с разными сигналами. Есть rule-based-реворды для ризонинг-задач, есть «верифицируемые» — когда можно проверить ответ прямо в среде, например, запустить код. Также используется LLM-as-a-judge, где отдельная модель оценивает ответы, и стандартный Bradley-Terry-реворд, в котором на вход подаётся ещё и ground truth, что не очень типично для таких моделей.&lt;br&gt;&lt;br&gt;Вместо классического GRPO в работе используют UPO (Unified Preference Optimization) — смесь онлайн-RL и офлайн-обучения на парах (DPO-подобный лосс). Мотивация — не переобучаться на потенциально шумных сигналах reward-моделей и держать устойчивый сигнал на аккуратно подобранных офлайн-парах. Инструкции отбирают так, чтобы дисперсия ревордов по ним была высокой — это даёт полезный сигнал в RL.&lt;br&gt;&lt;br&gt;&lt;strong&gt;VLM-линия: три SFT-этапа и свой RL&lt;br&gt;&lt;/strong&gt;&lt;br&gt;В сложных мультимодальных задачах часто «провисает» не сам ризонинг, а перцепция: модель плохо считывает сложные структуры и объекты на картинке. Проблема — дефицит плотных, подробных пар «картинка-кэпшен». Синтетика тут помогает ограниченно. Поэтому авторы делают детальные кэпшены на реальных картинках в срезе STEM так, чтобы текст-only-модель могла отвечать на вопрос по исходной картинке, имея только кэпшен. Если это работает для множества моделей — кэпшен считается годным и идёт в обучение.&lt;br&gt;&lt;br&gt;&lt;strong&gt;SFT включает три шага:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;strong&gt;1. Text-only Reasoning Cold Start.&lt;/strong&gt; Сначала учат чисто текстовый ризонинг (визуальные эксперты и энкодер тут не участвуют). Интересно, что VLM-способности при этом не разрушаются и даже появляется генерализация reasoning-паттернов на мультимодальные задачи в срезе STEM.&lt;br&gt;&lt;br&gt;&lt;strong&gt;2. Reject Sampling for Multimodal Enhancement.&lt;/strong&gt; Берут мультимодальные сэмплы, генерят много гипотез, ранжируют мультимодальными reward-моделями, отбирают лучшие — получается датасет для мультимодального reasoning-SFT.&lt;br&gt;&lt;br&gt;&lt;strong&gt;3. Thinking / Non-Thinking Fusion.&lt;/strong&gt; Обучение на смеси thinking- и non-thinking-данных; дополнительно описывают идею мёрджа экспертов между ризонинг- и неризонинг-моделью, чтобы перенести полезных мультимодальных экспертов.&lt;br&gt;&lt;br&gt;&lt;strong&gt;RL для VLM&lt;/strong&gt;&lt;br&gt;Авторы используют как model-based-сигналы вознаграждения, так и верифицируемые задачи, где можно проверить правильность ответа. К таким задачам относятся STEM-примеры (переписывание коротких тестовых вопросов в развёрнутые ответы), визуальные пазлы и генерация HTML по скриншоту интерфейса с автоматической проверкой через сравнение изображений (рендер против эталона). &lt;br&gt;&lt;br&gt;&lt;strong&gt;Результаты&lt;br&gt;&lt;/strong&gt;&lt;br&gt;Текстовые модели ERNIE 4.5 чаще выигрывают у DeepSeek V3 на основных бенчмарках. После пост-трейна они держатся на уровне проприетарных моделей, вроде GPT-4, особенно хорошо справляясь с instruction-following и длинным контекстом.&lt;br&gt;&lt;br&gt;В мультимодальных задачах ERNIE 4.5 показывает результаты примерно на уровне Qwen 2.5-VL — где-то чуть выше, где-то сопоставимо, особенно в reasoning-режиме.&lt;br&gt;&lt;br&gt;&lt;em&gt;Разбор подготовил &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt; Алексей Григорьев&lt;/em&gt; &lt;br&gt;&lt;a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>ERNIE 4.5 Technical Report [1/2]</title><link>https://t.me/timeforcv/190</link><guid>https://t.me/timeforcv/190</guid><pubDate>Fri, 07 Nov 2025 08:26:08 +0000</pubDate><description>&lt;strong&gt;ERNIE 4.5 Technical Report [1/2]&lt;/strong&gt;&lt;br&gt;&lt;strong&gt;&lt;br&gt;&lt;/strong&gt;Сегодня начинаем разбирать большой &lt;a href="https://yiyan.baidu.com/blog/publication/" rel="nofollow noopener noreferrer"&gt;технический репорт&lt;/a&gt; (около 40 страниц без аппендикса) от Baidu о том, как они обучали мультимодальные Mixture-of-Experts (MoE)-модели. Авторы предлагают целую линейку моделей: MoE- и dense-версии, с ризонингом и без, варианты под LLM- и VLM-задачи. &lt;br&gt;&lt;br&gt;В этой части разбираем интересные решения в архитектуре и претрейне.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Архитектура &lt;/strong&gt;&lt;br&gt;&lt;br&gt;Авторы предлагают мультимодальную гетерогенную MoE-архитектуру. Поддерживаются текст, изображения и видео, на выходе — текст. Внутри блока трансформера два роутера: один маршрутизирует текстовые токены, второй — визуальные. &lt;br&gt;&lt;br&gt;Кроме специализированных экспертов, есть shared-эксперты, которые всегда активны для обеих модальностей. Это нужно, чтобы не было сдвигов при совместном обучении и модальности не разбегались в эмбеддинговом пространстве (авторы ссылаются на работу &lt;a href="https://arxiv.org/abs/2411.04996" rel="nofollow noopener noreferrer"&gt;Mixture-of-Transformers&lt;/a&gt;). Для роутинга используется привычный top-k-подход, знакомый нам по DeepSeek.&lt;br&gt;&lt;br&gt;Визуальный энкодер реализован аналогично Qwen. С помощью адаптивного 2D RoPE  картинку приводят к подходящему разрешению по соотношению сторон, разбивают на патчи и кодируют. Для видео применяют тот же принцип, только с 3D RoPE и таймстемпами.&lt;br&gt;&lt;br&gt;Если ролик не влезает в контекст, выбираются кадры с нужным шагом (adaptive frame-resolution sampling strategy) и при необходимости уменьшают разрешение. Потом идёт pixel shuffle и темпоральная компрессия — пространственные размеры урезаются, а временная часть остаётся.&lt;br&gt;В итоге визуальные токены из картинок и видео отправляются в мультимодальный self-attention.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Претрейн&lt;/strong&gt;&lt;br&gt;&lt;br&gt;В работе описан стандартный пайплайн с дедупликацией, удалением мусора и quality-фильтрами. Но есть и особенности:&lt;br&gt;&lt;br&gt;— &lt;strong&gt;Data Map&lt;/strong&gt;: с её помощью данные организуют по языку, домену знаний, сценарию, качеству.&lt;br&gt;— &lt;strong&gt;Human-Model-in-the-Loop Data Refinement&lt;/strong&gt;: асессоры помогают улучшать качество и разметку, результаты возвращаются в обучение классификаторов.&lt;br&gt;— &lt;strong&gt;Text-only-данные&lt;/strong&gt;: делятся на пять типов по DIKW-фреймворку; отдельный акцент делается на фактические знания и программирование.&lt;br&gt;— &lt;strong&gt;Interleaved-данные&lt;/strong&gt; (текст + картинка из веба): аккуратная каталогизация источников, аугментации, чистка, генерация и фильтрация кэпшенов, дедупликация по хешам изображений и текстов; категоризация типов картинок (натуральные сцены, таблицы, скриншоты, чаты, документы и др.).&lt;br&gt;— &lt;strong&gt;Видео&lt;/strong&gt;: авторы парсили ролики с богатым контекстом, прогоняли через ASR и использовали транскрипты.&lt;br&gt;— &lt;strong&gt;Domain-specific data&lt;/strong&gt;: здесь используют прогрессивный рефильтринг данных — примерно так же, как это делалось в DeepSeekMath. Собирают пул URL по нужному домену, фильтруют, отправляют на оценку асессором, парсят содержимое, обучают новый классификатор и повторяют цикл. &lt;br&gt;&lt;br&gt;Интересная находка: авторы собирали сетки из нескольких картинок в один кадр — так модель лучше учится работать с несколькими изображениями сразу и точнее понимает, о каком объекте речь.&lt;br&gt;&lt;br&gt;Также исследователи пишут о применении REEAO (Record Everything Everywhere All at Once) — способе упаковывать сэмплы так, чтобы максимально заполнять контекст, не теряя остатки, и при этом быть робастными к смене data-parallel-группы. &lt;br&gt;&lt;br&gt;В следующей части разберём интересное из посттрейна.&lt;br&gt;&lt;br&gt;&lt;em&gt;Разбор подготовил &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt; Данил Кашин&lt;/em&gt; &lt;br&gt;&lt;a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>Loong: Generating Minute-level Long Videos with Autoregressive Language Models</title><link>https://t.me/timeforcv/189</link><guid>https://t.me/timeforcv/189</guid><pubDate>Tue, 28 Oct 2025 12:18:01 +0000</pubDate><description>&lt;strong&gt;Loong: Generating Minute-level Long Videos with Autoregressive Language Models&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Сегодня разберём &lt;a href="https://arxiv.org/abs/2410.02757" rel="nofollow noopener noreferrer"&gt;статью&lt;/a&gt; о Loong — авторегрессионной модели для генерации видео на основе LLM. Архитектура у неё типичная:&lt;br&gt;&lt;br&gt;1. Видео токенизируют. В качестве энкодера использует MAGViT2. Это 3D CNN свёрточная модель, которая обрабатывает темпоральную часть кадров видео, токенизированную с помощью Clustering Vector Quantization. Размер токенайзера — 246M параметров. &lt;br&gt;&lt;br&gt;2. Вектора видео подают на вход LLM. Авторы учат с нуля LLaMa от 700M до 7B параметров: 32 000 токенов для текста, 8 192 — для видео и 10 специальных — скорее всего, для разделителей между кадрами. &lt;br&gt;&lt;br&gt;3. LLM возвращает другие вектора, на основе которых модель-декодер VQGAN предсказывает изображения — кадры видео. &lt;br&gt;&lt;br&gt;Лосс в конце длинной последовательности кадров оказывается меньше, так как видеотокены в одном видео похожи между собой, а модели проще предсказывать похожие токены последовательно. Текстовые токены сильно отличаются от видео: для того чтобы качественно генерировать первые кадры, авторы предлагают перевзвешивать их лосс.&lt;br&gt;&lt;br&gt;Обучение делят на три стадии:&lt;br&gt;&lt;br&gt;&lt;strong&gt;1-я стадия.&lt;/strong&gt; Модель предсказывает только одно изображение.&lt;br&gt;&lt;strong&gt;2-я стадия.&lt;/strong&gt; Генерируется 1 секунда видео и 17 фреймов.&lt;br&gt;&lt;strong&gt;3-я стадия.&lt;/strong&gt; Самое длинное видео — 10 секунд.&lt;br&gt;&lt;br&gt;Модель обучают на десятисекундных видео. Этого мало, если на выходе должно получиться качественное длинное видео. Чтобы повысить качество генерации, авторы предлагают так называемый реинкодинг. То есть, генерировать первые кадры по исходному промпту пользователя. А потом брать в качестве следующего промпта несколько последних кадров получившегося видео и генерировать новое. &lt;br&gt;&lt;br&gt;Такой подход замедляет инференс, но снижает требования к обучающему датасету. Loong тренировали на 100M пар «текст + изображение». Для первой стадии использовали датасеты LAION-2B и CC12M. Обучающие видео — 5,5M клипов, отфильтрованных из HDVG.&lt;br&gt;&lt;br&gt;Пример Loong подтверждает: генерировать качественные длинные видео можно, даже если обучать модель только на коротких примерах.&lt;br&gt;&lt;br&gt;Посмотреть результаты генераций можно на &lt;a href="https://yuqingwang1029.github.io/Loong-video" rel="nofollow noopener noreferrer"&gt;GitHub&lt;/a&gt;.&lt;br&gt;&lt;br&gt;&lt;em&gt;Разбор подготовил &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt; Андрей Чернов&lt;/em&gt;&lt;br&gt;&lt;a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>Что читает команда стримингового зрения: подборка актуальных статей</title><link>https://t.me/timeforcv/188</link><guid>https://t.me/timeforcv/188</guid><pubDate>Fri, 24 Oct 2025 09:04:51 +0000</pubDate><description>&lt;strong&gt;Что читает команда стримингового зрения: подборка актуальных статей &lt;br&gt;&lt;/strong&gt;&lt;br&gt;Заглянули к инженерам команды стримингового зрения в Яндексе — узнали, что они читают и обсуждают в последнее время. В сегодняшней подборке: новый мультивидовый датасет для устойчивого отслеживания объектов, трекинг мяча под окклюзией в спортивных видео и рекурсивное рассуждение маленьких нейросетей, которые обгоняют крупные LLM на логических задачах.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/html/2502.20111v1" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;MITracker: Multi-View Integration for Visual Object Tracking&lt;br&gt;&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;Авторы собрали и разметили крупный мультивидовый датасет (260 видео, около 234 тысяч кадров) с калибровкой камер, BEV-аннотациями и девятью атрибутами (occlusion, motion blur, low-res и др.). С одной стороны, этот датасет отличается разнообразием классов, с другой — ограничен только сценами в помещениях, что снижает переносимость в уличные условия.&lt;br&gt;&lt;br&gt;Как устроен MITracker: &lt;br&gt;&lt;br&gt;— View-specific feature extraction: для каждой камеры используется отдельный Vision Transformer, который извлекает представления целевого объекта в поточном кадре; объект задаётся эталонным изображением.&lt;br&gt;&lt;br&gt;— Multi-view integration: 2D-признаки всех ракурсов проецируются и объединяются в 3D-feature volume с использованием BEV-информации; этот объём применяется в spatial-enhanced attention, который корректирует представления и улучшает локализацию и ассоциацию.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/html/2508.09650v1" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;TOTNet: Occlusion-Aware Temporal Tracking for Robust Ball Detection in Sports Videos&lt;/strong&gt;&lt;/a&gt;&lt;a href="https://arxiv.org/pdf/2508.09650" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;&lt;br&gt;&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;TOTNet вводит архитектуру для трекинга мяча в спортивных видео, специально сфокусированную на работе в условиях частичной и полной окклюзии. Модель сохраняет временную структуру данных за счёт применения 3D-свёрток. Это позволяет извлекать динамические признаки движения, а не статические из пачки кадров.&lt;br&gt;&lt;br&gt;Ключевые компоненты TOTNet:&lt;br&gt;&lt;br&gt;— Occlusion Augmentation: специальная аугментация, которая имитирует скрытие мяча, чтобы модель училась восстанавливать позицию по контексту.&lt;br&gt;&lt;br&gt;— Visibility-weighted BCE loss: взвешенная функция потерь, которая усиливает вклад случаев с окклюзией при обучении.&lt;br&gt;&lt;br&gt;— Интеграция оптического потока (RAFT): используется для более точного захвата движения мяча в быстрых сценах.&lt;br&gt;&lt;br&gt;В результате модель устойчиво отслеживает мяч, даже когда он временно исчезает из кадра, и превосходит предыдущие методы на всех спортивных датасетах, включая новый датасет TTA (Table Tennis under Occlusion).&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2510.04871" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Less is More: Recursive Reasoning with Tiny Networks&lt;br&gt;&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;В статье авторыпредставляют Tiny Recursive Model (TRM) — простой и эффективный подход к решению сложных логических задач. Суть метода в использовании маленькой нейросети (всего 7 млн параметров), которая рекурсивно, шаг за шагом «размышляет» над решением и улучшает свои ответы с помощью механизма deep supervision.&lt;br&gt;&lt;br&gt;По результатам экспериментов TRM превосходит современные LLM на бенчмарках Sudoku и ARC-AGI, используя при этом в тысячи раз меньше вычислительных ресурсов. Авторы отмечают, что для некоторых типов задач, особенно при ограниченном количестве обучающих данных, глубокая рекурсия компактной сети помогает избежать переобучения и оказывается намного эффективнее простого увеличения размера модели.&lt;br&gt;&lt;em&gt;&lt;br&gt;&lt;/em&gt;&lt;a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language Models</title><link>https://t.me/timeforcv/187</link><guid>https://t.me/timeforcv/187</guid><pubDate>Fri, 17 Oct 2025 12:03:08 +0000</pubDate><description>&lt;strong&gt;Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language Models&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Сегодня разбираем &lt;a href="https://arxiv.org/abs/2509.12132" rel="nofollow noopener noreferrer"&gt;статью&lt;/a&gt; с любопытным методом разметки данных, который возвращает внимание модели к картинке, а не только к тексту.&lt;br&gt;&lt;br&gt;При обучении на синтетике визуально-языковые модели быстро перестают смотреть на изображение и уходят в чисто текстовый ризонинг. Пример из статьи: нужно вычислить площадь под графиком. Текстовая модель пересчитывает шаги правильно, но не учитывает, что площадь под осью идёт с минусом. А модель с «визуальным рефлекшеном» может повторно взглянуть на картинку и заметить этот нюанс.&lt;br&gt;&lt;br&gt;Чтобы показать проблему, в статье приводят несколько метрик. Первая — attention score между токенами рассуждения и визуальными токенами. Чем длиннее ризонинг, тем меньше внимания остаётся на картинку. &lt;br&gt;&lt;br&gt;Вторая метрика — расстояние Хеллингера. Сначала запускают генерацию с картинкой, а затем убирают визуальные токены и продолжают без них. График показывает, что расстояние со временем уменьшается. Это значит, что итоговые генерации с убранной картинкой (после нескольких токенов, сгенерированных с изображением) почти не отличаются от генераций, где картинка присутствует. Иначе говоря, начиная с какого-то шага модель просто перестаёт использовать изображение и игнорирует его.&lt;br&gt;&lt;br&gt;Авторы предлагают модель Reflection-V, которая умеет делать рефлекшн именно по изображению. &lt;br&gt;&lt;br&gt;Решением становится новая разметка. Сначала составляется максимально подробный кэпшн, затем сильная текстовая модель (например, DeepSeek) выполняет задачу только по описанию. &lt;br&gt;&lt;br&gt;Но ключевая идея статьи — агентский пайплайн. LLM-агент получает задачу: &lt;em&gt;«На что похожа фигура — на телевизор, телефон, компьютер или часы?»&lt;/em&gt;. Он вызывает VLM и уточняет: &lt;em&gt;«Похоже ли это на часы?»&lt;/em&gt;. VLM отвечает: «&lt;em&gt;Есть треугольники и квадраты, ничего круглого — не часы»&lt;/em&gt;. Агент делает вывод: &lt;em&gt;«Значит, может быть телефон — у него кнопки сеткой, как клавиатура»&lt;/em&gt;, и снова уточняет. Так формируется диалог, который суммаризатор превращает в связный reasoning trace. В итоге рассуждение действительно опирается на картинку, а не на текст.&lt;br&gt;&lt;br&gt;Дополнительно используются фильтрации: если агент ответил без обращения к VLM, пример удаляется. На собранных данных модель обучается с GRPO. К обычной награде за правильный ответ добавляется ещё одна — по attention. Она измеряет, насколько во второй половине ризонинга модель продолжает опираться на изображение. Идея в том, чтобы не дать ей «забыть» картинку в середине рассуждения.&lt;br&gt;&lt;br&gt;Тесты проводили на MathVision, MathVista, MMMU, IMMU-Pro, M3CoT и HallBench. Обучали две версии — Reflection-V-3B и Reflection-V-7B на базе Qwen2.5-VL. Они уверенно обгоняют опенсорсные ризонёры на синтетике и даже внутренние модели Qwen. &lt;br&gt;&lt;br&gt;В агентской системе «мозгом» выступает QWQ-32B (LLM-reasoner), визуальным экспертом — Qwen-2.5-VL-72B. Обучение идёт в два этапа: сначала SFT (три эпохи на двух H100), затем GRPO (двенадцать эпох на восьми H100 через vLLM). Всего — около 16 тысяч ризонинг-семплов. Сетап скромный, особенно по объёму данных.&lt;br&gt;Аблейшны показывают, что полная модель (3B и 7B) даёт лучшие результаты. &lt;br&gt;&lt;br&gt;Убираем reward по attention — метрики падают. Без SFT — ещё хуже. Убираем и то, и другое — совсем провал. Вывод авторов очевиден: все элементы нужны и каждый вносит свой вклад.&lt;br&gt;&lt;br&gt;&lt;em&gt;Разбор подготовил &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt; Илья Димов&lt;br&gt;&lt;/em&gt;&lt;a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>Работы по сбору датасетов для задачи instruction-based editing</title><link>https://t.me/timeforcv/186</link><guid>https://t.me/timeforcv/186</guid><pubDate>Mon, 13 Oct 2025 08:52:06 +0000</pubDate><description>&lt;strong&gt;Работы по сбору датасетов для задачи instruction-based editing&lt;br&gt;&lt;/strong&gt;&lt;br&gt;Вместе с ростом популярности T2I-генерации стала активно развиваться и задача редактирования изображений. Несмотря на очевидные сходства, между ними есть как минимум одно ключевое отличие: редактирование — не одна задача, а целое семейство, и с точки зрения ML, и с точки зрения данных.&lt;br&gt;&lt;br&gt;Сергей Кастрюлин, исследователь Yandex Research, разобрал основные работы по сбору датасетов для задачи instruction-based editing. &lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2404.18212" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Paint by Inpaint: Learning to Add Image Objects by Removing Them First&lt;/strong&gt;&lt;/a&gt; [&lt;a href="https://huggingface.co/datasets/paint-by-inpaint/PIPE" rel="nofollow noopener noreferrer"&gt;датасет на HF&lt;/a&gt;, без лицензии]&lt;br&gt;&lt;br&gt;Крупный (1,8M сэмплов) датасет, полностью посвящённый задаче добавления/удаления объектов. Авторы стартуют с картинок из COCO и OpenImages, для которых уже просчитаны маски (датасет LVIS). По этим маскам делают Remove через SD-Inpainting.&lt;br&gt;&lt;br&gt;Основная часть работы посвящена фильтрациям:&lt;br&gt;— Исходные пары картинка-маска фильтруют по размеру и положению маски (слишком мелкая, слишком близка к краю картинки).&lt;br&gt;— После инпейнтинга проверяют, что объект действительно удалён, что удалён именно важный объект и что в целом картинка не испортилась, вычисляя набор эвристических метрик на основе локальных CLIP-эмбеддингов.&lt;br&gt;&lt;br&gt;В статье указано соотношение source- и target-картинок: из ~800К исходных получили 1,800К таргетов. Это довольно сбалансированное распределение.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2405.04007" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;SEED-Data-Edit Technical Report: A Hybrid Dataset for Instructional Image Editing&lt;/strong&gt;&lt;/a&gt; [&lt;a href="https://huggingface.co/datasets/AILab-CVC/SEED-Data-Edit" rel="nofollow noopener noreferrer"&gt;датасет на HF&lt;/a&gt;, некоммерческий]&lt;br&gt;&lt;br&gt;Ещё один большой (1,5М сэмплов) датасет, состоящий из трёх частей.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Часть 1: синтетические данные&lt;br&gt;&lt;/strong&gt;&lt;br&gt; 1) Добавление и удаление объектов:&lt;br&gt;— Берут изображения из Unsplash и OpenImages.&lt;br&gt;— С помощью моделей LLAVA-1.5, GroundingDINO и SAM сегментируют объекты, подходящие для удаления.&lt;br&gt;— Делают удаление с помощью модели инпейнтинга LaMa.&lt;br&gt;— Для получения данных на задачу добавления объектов инвертируют триплеты.&lt;br&gt;&lt;br&gt;2) Изменение объектов:&lt;br&gt;— Берут реальную картинку, кепшенят её.&lt;br&gt;— С помощью ChatGPT изменяют часть исходного инстракта. &lt;br&gt;— Берут image-guided T2I-модель PnP, подают в неё исходную картинку и измененный инстракт, получают результат.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Части 2 и 3: реальные данные&lt;br&gt;&lt;/strong&gt;&lt;br&gt;— Парсят сайты, где пользователи просят отфотошопить картинки. Получают 52К триплетов.&lt;br&gt;— Просят асессоров в фотошопе последовательно внести простые изменения и описать их кепшенами. Получают 21К последовательностей разной длины (до пяти редактирований на картинку).&lt;br&gt;&lt;br&gt;На смеси данных учат LoRA для модели SEED-X. Минусы:&lt;br&gt;— В отличие от Qwen-Image авторы не перераспределяют данные по стадиям (было бы логично начать с плохой синетики, а закончить обучение на чистых реальных данных).&lt;br&gt;— Информация о последовательных редактированиях никак не используется — её просто перегруппируют в триплеты.&lt;br&gt;— О фильтрации не сказано ни слова, так что датасет почти наверняка шумный.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2411.15738" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;AnyEdit: Mastering Unified High-Quality Image Editing for Any Idea&lt;/strong&gt;&lt;/a&gt; [&lt;a href="https://huggingface.co/datasets/Bin1117/AnyEdit" rel="nofollow noopener noreferrer"&gt;датасет на HF&lt;/a&gt;, без лицензии]&lt;br&gt;&lt;br&gt;2,5М сэмплов, разбитых на 5 категорий для увеличения разнообразия данных:&lt;br&gt;&lt;br&gt;— Локальное редактирование: добавление, удаление или замена объектов, изменение цвета и действий;&lt;br&gt;— Глобальное редактирование: изменение тона, стиля или фона изображения;&lt;br&gt;— Редактирование, связанное с движением камеры: расширение кадра, поворот, изменение размера;&lt;br&gt;— Визуальное редактирование: перенос материалов, работа со скетчами и масками;&lt;br&gt;— Неявное редактирование (Implicit Editing).&lt;br&gt;&lt;br&gt;Авторы стартуют с 680К из нескольких открытых датасетов. В данных отсутствуют «редкие концепты», поэтому генерят синтетические исходные картинки:&lt;br&gt;&lt;br&gt;— Определяют редкие концепты.&lt;br&gt;— Просят LM сгенерить промпты для T2I-модели, чтоб они включали эти концепты.&lt;br&gt;— Генерят еще 700К картинок, доливают к исходным реальным.&lt;br&gt;&lt;br&gt;Затем берутся промпты к исходным синтетическим картинкам и кепшены к реальным и — из них с помощью Llama3-8b генерятся editing-инстракты. &lt;br&gt;&lt;br&gt;В статье описаны 9 пайплайнов генерации данных для покрытия пяти категорий задач указанных выше (Figure 7, appendix). После генерации есть фильтрация на основе CLIP-based эвристик.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Продолжение читайте в авторском канале Сергея Кастрюлина &lt;/strong&gt;&lt;strong&gt;@c_research&lt;/strong&gt;&lt;strong&gt;.&lt;br&gt;&lt;/strong&gt;&lt;br&gt;&lt;a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning</title><link>https://t.me/timeforcv/184</link><guid>https://t.me/timeforcv/184</guid><pubDate>Wed, 08 Oct 2025 09:02:44 +0000</pubDate><description>&lt;strong&gt;GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Сегодня разберём &lt;a href="https://arxiv.org/abs/2507.01006" rel="nofollow noopener noreferrer"&gt;статью&lt;/a&gt; о том, как с помощью reinforcement learning (RL) и curriculum sampling обучить сильную визуально-языковую модель (VLM), опережающую аналоги в опенсорс-бенчмарках. Именно такой подход помог создать GLM-4.5V.&lt;br&gt;&lt;br&gt;Авторы позиционируют GLM‑4.5V как одну из лучших открытых моделей для широкого круга задач: работа с длинными документами, агентный режим, видеоанализ, OCR и графика, генерация кода, STEM и VQA.&lt;br&gt;&lt;br&gt;Архитектурно GLM‑4.5V близка к современным VLM и во многом напоминает Qwen2‑VL. Модель состоит из трёх ключевых компонентов: визуального энкодера, MLP‑адаптера и LLM‑декодера (MoE 12А109B). Для кодирования визуальных токенов в ViT применяются 2D‑RoPE и интерполяция абсолютных позиций для произвольных разрешений и экстремальных аспект‑ratios. А в LLM используются 3D‑RoPE и временные индексы для видео, что улучшает моделирование темпоральных зависимостей.&lt;br&gt;&lt;br&gt;Модель предобучали с нуля на академических текстовых корпусах и больших, разнообразных наборах изображений. Для этого понадобилось свыше 10B пар «изображение + текст», отфильтрованных при помощи CLIP‑подобной модели. Чтобы минимизировать смещения, все операции с данными сопровождались сбором статистик: нормировали частоты в корпусе, следили за распределениями и итеративно улучшали собственный captioning‑пайплайн. Итоговый объём претренировочного датасета составил около 2T токенов.&lt;br&gt;&lt;br&gt;Крупный претрейн и аккуратно собранный корпус для SFT с чётко заданным форматом ответов создали прочную основу для RL‑стадии. Качество модели оценивали через многократное сэмплирование предсказаний и подсчёт PASS@k на разных бенчмарках — это позволило заранее понимать, как система проявит себя после RL.&lt;br&gt;&lt;br&gt;Главное новшество — мультидоменный онлайн‑RL с продуманной reward‑системой на базе GRPO. Авторы валидировали отдельные критерии оценки для каждого домена, контролировали риск reward hacking и балансировали сложность примеров. Такой подход позволил получить хорошее межпредметное обобщение: обучение в одном домене повышало качество в других, а совместное обучение сразу в нескольких — приводило к ещё большим улучшениям в каждом из них.&lt;br&gt;&lt;br&gt;Второе важное нововведение — curriculum sampling: отбор наиболее полезных примеров для обучения. Подготовка выборки (RLCS) и её динамическое расширение реализованы с помощью экспоненциальной скользящей средней (EMA), что стабилизирует траекторию обучения и ускоряет сходимость модели.&lt;br&gt;&lt;br&gt;По итогам проверки на 42 публичных бенчмарках GLM‑4.5V обеспечивает высокие результаты почти во всех задачах среди открытых моделей сопоставимого размера и демонстрирует конкурентоспособность по отношению к закрытым решениям.&lt;br&gt;&lt;br&gt;Познакомиться с GLM-4.5V можно на &lt;a href="https://github.com/zai-org/" rel="nofollow noopener noreferrer"&gt;github&lt;/a&gt;.&lt;br&gt;&lt;br&gt;&lt;em&gt;Разбор подготовил &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt; Данил Кашин&lt;br&gt;&lt;/em&gt;&lt;a href="https://t.me/+SSca5c9pEyszN2Uy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>Что читает команда алайнмента VLM: подборка актуальных статей</title><link>https://t.me/timeforcv/183</link><guid>https://t.me/timeforcv/183</guid><pubDate>Tue, 30 Sep 2025 13:05:13 +0000</pubDate><description>&lt;strong&gt;Что читает команда алайнмента VLM: подборка актуальных статей &lt;br&gt;&lt;/strong&gt;&lt;br&gt;Узнали у инженеров Яндекса из команды алайнмента визуально-языковых моделей, какие статьи они читали и обсуждали в последнее время. В сегодняшней подборке: новый способ обучения MAE с прогрессивным замораживанием слоёв для видеолатентов без коллапса, как именно теряется сигнал в коннекторах VLM, объединение текста, картинки и звука в одной модели с сильным алайнментом и другое.&lt;br&gt;&lt;br&gt;&lt;a href="http://arxiv.org/abs/2509.10156v1" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;LayerLock: Non-collapsing Representation Learning with Progressive Freezing&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;В статье предлагается новый способ обучения MAE (Masked AutoEncoder) моделей для сжатия видео в латентные векторы на неразмеченных данных. Авторы заметили, что слои ViT на разной глубине сходятся с разной скоростью, и придумали прогрессивно замораживать по ходу обучения ранние слои, одновременно меняя таргет от восстановления пикселей к всё более глубоким латентным признакам. Это решает проблемы с representation collapse, и модель учится хорошо извлекать высокоуровневые фичи из видео.&lt;br&gt;&lt;br&gt;&lt;a href="http://arxiv.org/abs/2509.11986v1" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Lost in Embeddings: Information Loss in Vision-Language Models&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;Авторы исследуют потерю информации в коннекторе — модуле, связывающем модальности в архитектуре современных VLM. В статье предлагают довольно интересные методы выявления этой потери, вплоть до определения конкретных участков изображения. Готовых решений нет, но работа помогает лучше понять, как сигнал передаётся от изображения к языковой модели внутри VLM, и подсвечивает информационный bottleneck современных архитектур.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/pdf/2509.17765" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Qwen3-Omni Technical Report&lt;br&gt;&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;Это инженерное чудо и второй подход к объединению всех модальностей (текста, картинки и звука) в семействе Qwen. На этот раз модель не уступает эквивалентным по размеру моделям-экспертам в каждой из модальностей. В работе описан пайплайн обучения и процесс объединения модальностей на разных стадиях. &lt;br&gt;&lt;br&gt;Примечательно, что стадия алайнмента включает дистилляцию более сильных тестовых моделей из семейства Qwen, возможно, с использованием моделей-экспертов в других модальностях. А вот об RL доподлинно известно, что часть ревордов в нём относятся к картиночной модальности, причём в обучении фигурируют, как model-based-, так и verifiable-реворды.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/pdf/2509.07969" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;В работе предлагают систему, способную решать сложные задачи визуального поиска с помощью многошаговых рассуждений на основе tool calling в виде зума изображения. В отличие от существующих подходов, ограниченных короткими цепочками действий, Mini-o3 может выполнять десятки взаимодействий методом проб и ошибок. Предложенная стратегия обучения на разнообразных траекториях рассуждений позволяет получить модель, генерирующую длинные цепочки рассуждений и повышающую свою точность с каждым шагом. Интересно, что схожая особенность появилась в передовой модели Qwen3-VL.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/pdf/2509.16127" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;BaseReward: A Strong Baseline for Multimodal Reward Model&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;В работе исследуется рецепт создания мультимодальных моделей вознаграждения (MRM). Путём обширных экспериментов авторы определили оптимальную парадигму обучения, архитектуру, состав и баланс данных, обнаружив, что добавление текстовой информации значительно улучшает оценку мультимодальных задач. В результате исследователи получили модель вознаграждения, превосходящую прочие подходы по ключевым бенчмаркам.&lt;br&gt;&lt;br&gt;&lt;a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>Should VLMs be Pre-trained with Image Data?</title><link>https://t.me/timeforcv/182</link><guid>https://t.me/timeforcv/182</guid><pubDate>Thu, 25 Sep 2025 17:00:01 +0000</pubDate><description>&lt;strong&gt;Should VLMs be Pre-trained with Image Data?&lt;br&gt;&lt;/strong&gt;&lt;br&gt;Сегодня разбираем &lt;a href="https://arxiv.org/abs/2503.07603" rel="nofollow noopener noreferrer"&gt;статью&lt;/a&gt; о том, как лучше организовать претрейн для VLM. Архитектурных новшеств здесь нет: модель напоминает стандартные опенсорсные VLM вроде LLaVA. Картинка кодируется вижн-энкодером, эмбеддинги прогоняются через несколько MLP-слоёв и подаются вместе с текстовыми эмбеддингами в LLM-декодер.&lt;br&gt;&lt;br&gt;Главный вопрос статьи: на каком этапе и в каких пропорциях подключать мультимодальные данные, чтобы итоговая модель была сильной и в text-only, и в мультимодальном режимах.&lt;br&gt;&lt;br&gt;Разберём три интересных аблейшна, представленных в работе.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Когда останавливать LLM-претрейн&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Обычно берут полностью обученную LLM (например, на 3–4T токенов) и затем добавляют мультимодальный претрейн со своим LR-шедулером, который часто начинается с warmup. Авторы считают это неэффективным: мы сначала «убиваем» learning rate, а потом снова разгоняем его на мультимодальных данных.&lt;br&gt;&lt;br&gt;Исследователи пробуют прервать обучение LLM не в самом конце, а на определённом проценте (например, ~80% от шага). Дальше продолжают обучение уже на смеси текстовых и мультимодальных данных, сохраняя текущий learning rate. По представленным VLM метрикам и отдельно text-only-числам, такой вариант даёт лучше результаты, чем стратегия «сначала — до конца LLM, потом — мультимодальность».&lt;br&gt;&lt;br&gt;&lt;strong&gt;Соотношение текстовых и мультимодальных данных&lt;br&gt;&lt;br&gt;&lt;/strong&gt;Во многих открытых моделях текстовые и мультимодальные данные миксуют на претрейне VLM, однако аблейшенов не дают. В статье показано, что оптимально брать в претрейн 10–20% мультимодальных данных.&lt;br&gt;&lt;br&gt;Это можно объяснить качеством датасета: картинки проще, но сами мультимодальные пары нередко «грязные», особенно в опенсорсе. Исходя из практики, мы тоже видим необходимость подбирать соотношение, однако это сильно зависит от качества данных и представленных в них доменов.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Инструктивность и SFT-эпохи&lt;br&gt;&lt;/strong&gt;&lt;br&gt;В классическом VLM-pretrain нет инструктивности — модели просто описывают картинки. В последнее время часть инструктивных примеров добавляется уже на претрейне, и это работает. У авторов эффект почти незаметен, скорее всего, из-за слабого датасета (устаревшие LLaVA-данные) и малого количества инструктивных данных.&lt;br&gt;&lt;br&gt;Ещё одно наблюдение связано с количеством эпох на SFT. Авторы пишут, что в их случае оптимальны четыре эпохи. При данных среднего качества выводы ограниченные и вряд ли могут быть перенесены на любую модель, однако результат полезный. По нашему же опыту — если данные хорошие, дополнительные эпохи действительно помогают.&lt;br&gt;&lt;br&gt;В целом статья скорее систематизирует наблюдения, чем открывает новое, но её результаты подтверждают, как важно грамотно комбинировать текст и мультимодальность и где именно стоит искать улучшения.&lt;br&gt;&lt;br&gt;&lt;em&gt;Разбор подготовил &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt; Владислав Смирнов&lt;/em&gt;&lt;br&gt;&lt;a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>Эволюция Florence: от генеративных моделей к MLLM</title><link>https://t.me/timeforcv/180</link><guid>https://t.me/timeforcv/180</guid><pubDate>Thu, 18 Sep 2025 09:03:44 +0000</pubDate><description>&lt;strong&gt;Эволюция Florence: от генеративных моделей к MLLM&lt;br&gt;&lt;/strong&gt;&lt;br&gt;Сегодня разберём сразу две статьи о семействе моделей Florence: что такое Florence-2 и как авторы использовали её в VLM.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2311.06242" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;br&gt;Это cемейство VLM-моделей появилось в 2023 году. По сути, это и была VLM, хотя сам термин тогда ещё не вошёл в широкое употребление. Показательно, что в Florence-2 авторы сделали ставку не на архитектуру, а на огромный и качественно собранный датасет FLD-5B.&lt;br&gt;&lt;br&gt;В основе архитектуры — обычная схема энкодер-декодер-трансформер. Разве что схему VLM авторы нарисовали не так, как принято в 2025-м.  &lt;br&gt;&lt;br&gt;Вся суть статьи в пайплайне обработки данных. Авторы сформулировали множество разных задач в формате «текст на входе — текст на выходе». Так всю разметку можно условно поделить на три группы: &lt;br&gt;&lt;br&gt;— понимание картинки в целом (classification, captioning, VQA) — семантика;&lt;br&gt;— умение локализовать объект (object detection, segmentation, referring expression comprehension) — геометрия; &lt;br&gt;— поиск и детекция объектов по набору признаков (text grounding) — семантика + геометрия.&lt;br&gt;&lt;br&gt;Пайплайн обработки данных, с помощью которого получили обучающий датасет — на первой иллюстрации к посту: &lt;br&gt;&lt;br&gt;1. первичная аннотация с помощью специализированных моделей (детекторы, OCR, сегментаторы);&lt;br&gt;2. фильтрация данных той же нейросетью: исправляют ошибки, удаляют ненужные аннотации;&lt;br&gt;3. итеративный процесс уточнения данных всё той же нейросетью.&lt;br&gt;&lt;br&gt;FLD-5B состоит из 5 млн аннотаций, 126 млн изображений, 500 млн текстовых аннотаций, 1,3 млн текстовых аннотаций для локализации объекта на изображении и 3,6 млн текстовых аннотаций для поиска и детекции объектов по набору признаков. &lt;br&gt;&lt;br&gt;Как итог, Florence-2 умеет делать 10+ задач (OCR, detection, segmentation, Caption to Phrase Grounding и др.) и довольно редко галлюцинирует. Однако, в отличие от современных VLM, она не справляется со сложными инстрактами, потому что не училась этому. Да и инстракты может принимать небольшие.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2412.04424" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion&lt;br&gt;&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;Во второй статье авторы предлагают простую идею — использовать в качестве энкодера в VLM Florence-2. Причина проста: эта модель явно училась на OCR, детекцию и сегментацию, в отличие от CLIP/SigLIP (хотя SigLIP2 уже училась с next token prediction).&lt;br&gt;&lt;br&gt;Заменить Image Encoder на Florence несложно. Нужно трижды инферить Image Encoder — по одному разу для получения признаков с прицелом на OCR, детекцию и сегментацию. Дальше фичи конкатенируются и пропускаются через projection (DBFusion), чтобы получить желаемое число каналов. Так появилось семейство Florence-VL. Подробнее — на второй иллюстрации к посту. &lt;br&gt;&lt;br&gt;В результате Florence-VL демонстрирует высокую согласованность визуального энкодера и LLM, превосходя другие модели по 25 критериям. В том числе в задачах распознавания объектов, понимания семантики, распознавания текста и построения диаграмм. &lt;br&gt;&lt;br&gt;Идея интересная, но, как показало время, не прижилась. Видимо, из-за того, что при таком подходе растёт число операций для получения фичей.&lt;br&gt;&lt;br&gt;&lt;em&gt;Разбор подготовил &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt; &lt;em&gt;Егор Шестопалов&lt;/em&gt;&lt;br&gt;&lt;a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>Scale-wise Distillation of Diffusion Models</title><link>https://t.me/timeforcv/179</link><guid>https://t.me/timeforcv/179</guid><pubDate>Thu, 11 Sep 2025 09:04:32 +0000</pubDate><description>&lt;strong&gt;Scale-wise Distillation of Diffusion Models&lt;br&gt;&lt;/strong&gt;&lt;br&gt;Сегодня разбираем &lt;a href="https://arxiv.org/html/2503.16397v1" rel="nofollow noopener noreferrer"&gt;статью&lt;/a&gt; от исследователей из Yandex Research, появившуюся на &lt;a href="http://arXiv.org/" rel="nofollow noopener noreferrer"&gt;arXiv.org&lt;/a&gt; в марте 2025 года. Авторы предложили метод дистилляции Scale-wise Distillation (SwD), при котором диффузионная модель не сразу генерирует изображение в полном разрешении, а постепенно повышает его на каждом шаге. Такой подход позволяет ускорить процесс генерации более чем в два раза по сравнению с обычной дистилляцией. &lt;br&gt;&lt;br&gt;Диффузия на данный момент — ведущая парадигма в области генерации изображений. Но, к сожалению, генерация даже одной картинки может быть довольно долгой. Причина: нужно делать много шагов, каждый из которых считается в фиксированном конечном разрешении и вычислительно затратен.&lt;br&gt;&lt;br&gt;Проблему попытались решить с помощью scale-wise-генерации: стартовать с одного пикселя и постепенно повышать разрешение, приходя к результату за несколько шагов. Тогда первые шаги идут в низком разрешении и стоят очень дёшево — затраты растут по мере увеличения размера изображения.&lt;br&gt;&lt;br&gt;Эта парадигма реализована в &lt;a href="https://arxiv.org/html/2502.06167v1" rel="nofollow noopener noreferrer"&gt;VAR&lt;/a&gt; (Visual Autoregressive Transformer), но кроме scale-wise-генерации, там используется представление изображения в виде дискретных токенов и авторегрессия. Однако дискретное представление изображений приводит к неустранимым ошибкам в представлении картинок и ограничивает максимально достижимое качество. &lt;br&gt;&lt;br&gt;Отсюда возникла идея вытащить из VAR scale-wise-генерацию и поместить её во фреймворк, сочетающий лучшие стороны обеих парадигм (VAR и диффузии). Метод обучения SwD-подхода основан на известных процедурах дистилляции диффузионных моделей. Но дистилляция в этом случае позволяет не только уменьшить число шагов генерации, но ещё и генерировать при меньших разрешениях.&lt;br&gt;&lt;br&gt;Интуиция авторов исходит из анализа диффузионного процесса в фурье-пространстве. У естественных картинок амплитуды частот убывают с ростом частоты, а у гауссова шума спектр плоский. Когда мы добавляем шум, высокочастотные компоненты изображения маскируются — сначала самые тонкие, потом всё больше. В итоге на ранних шагах модели остаются только низкие частоты, а детали всё равно «съедаются» шумом.&lt;br&gt;&lt;br&gt;Это объясняет, почему диффузия хорошо подходит для генерации изображений: она восстанавливает сигнал от грубых низкочастотных структур к высоким частотам и деталям. Однако становится очевидно, что на начальных этапах нет смысла использовать полное разрешение — всё, что модель посчитает, будет уничтожено шумом.&lt;br&gt;&lt;br&gt;Есть важные нюансы:&lt;br&gt;&lt;br&gt;— если напрямую увеличивать разрешение шумных латентных представлений, возникает много артефактов, и качество изображения значительно ухудшается. Поэтому лучше сначала увеличить разрешение чистой картинки в низком разрешении, а затем добавить шум;&lt;br&gt;&lt;br&gt;— важно подобрать такие шаги, чтобы уровень шума подавлял артефакты увеличения разрешения. Расписание шумов имеет критическое значение: в отличие от базовой дистилляции с равномерным расписанием, здесь его следует сдвинуть в сторону более высокого уровня шума, чтобы «погасить» дефекты увеличения разрешения;&lt;br&gt;&lt;br&gt;— «перезашумить» — не так страшно, как «недозашумить». Если шума будет меньше, чем требует текущий шаг, качество сильно упадёт, и на финальных картинках появятся артефакты.&lt;br&gt;&lt;br&gt;Обучение строится на парах соседних разрешений. Исходное изображение уменьшают до меньшего и до целевого размера. Малоразмерное изображение увеличивают, добавляют шум в соответствии с шагом t и подают в генератор, который предсказывает изображение в целевом разрешении. Функция потерь основана на сопоставлении распределения между предсказанием и целевым изображением (distribution matching).&lt;br&gt;&lt;br&gt;Отдельно важно, что модель учится на синтетике учителя. Предобученной диффузией генерируют много картинок на основе некоторой выборки пользовательских запросов. Такой подход даёт заметный прирост качества по сравнению с обучением на реальных картинках.&lt;br&gt;&lt;br&gt;&lt;em&gt;Разбор подготовил &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt; &lt;em&gt;Денис Кузнеделев&lt;/em&gt;&lt;br&gt;&lt;a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>Подборка статей о PEFT в VLM</title><link>https://t.me/timeforcv/176</link><guid>https://t.me/timeforcv/176</guid><pubDate>Wed, 03 Sep 2025 09:06:07 +0000</pubDate><description>&lt;strong&gt;Подборка статей о PEFT в VLM &lt;br&gt;&lt;/strong&gt;&lt;br&gt;Сегодня у нас краткий обзор PEFT (Parameter-Efficient Fine-Tuning) в визуальных моделях. Разберём три подхода и ключевые статьи в каждом из них. &lt;br&gt;&lt;br&gt;&lt;strong&gt;Аддитивные методы&lt;/strong&gt;&lt;br&gt;&lt;strong&gt;&lt;br&gt;&lt;/strong&gt;﻿﻿&lt;a href="https://arxiv.org/pdf/2205.13535" rel="nofollow noopener noreferrer"&gt;AdaptFormer&lt;/a&gt;&lt;br&gt;&lt;a href="https://arxiv.org/pdf/2205.13535" rel="nofollow noopener noreferrer"&gt;&lt;br&gt;&lt;/a&gt;Базовый метод в этом классе, который фактически копирует адаптер-тюнинг из LLM. Подразумевает добавление адаптер-блока с понижением, нелинейным преобразованием и повышением размерности. &lt;br&gt;&lt;br&gt;Обычно адаптер-блоки последовательно добавляют к feed-forward-слоям, а авторы подключают их параллельно — при этом адаптер складывается с результатом feed-forward-слоя с некоторым весом. Этот вес задаётся как гиперпараметр. В LLM его обычно берут больше единицы (например, 4), а для ViT у авторов лучший результат получился при 0,1. &lt;br&gt;&lt;br&gt;В статье утверждают, что этот метод, применённый к VLM, даёт более высокие результаты по сравнению с prompt tuning, а иногда и с full tuning.&lt;br&gt;&lt;br&gt;﻿﻿&lt;a href="https://arxiv.org/abs/2205.08534" rel="nofollow noopener noreferrer"&gt;ViT-Adapter&lt;/a&gt;&lt;br&gt;&lt;br&gt;Авторы исходят из того, что CNN лучше извлекают пространственные признаки, поэтому добавляют в ViT адаптер, который объединяет CNN и ViT. Основные компоненты адаптера:&lt;br&gt;&lt;br&gt;— Spatial prior module — CNN на основе Stem из ResNet (свёртки 3×3 со stride=2 и свёртка 1×1), которая проецирует карты признаков в размерность D. На выходе получается пирамида {F1, F2, F3} из D-мерных карт с разрешениями 1/8, 1/16 и 1/32 от исходного. Эти карты разворачиваются и конкатенируются в один вектор.&lt;br&gt;&lt;br&gt;— Spatial Feature Injector — компонент, состоящий из n блоков, где i-й блок добавляет пространственную информацию в i-й блок ViT с помощью слоя cross-attention.&lt;br&gt;&lt;br&gt;— Spatial Feature Extractor — компонент, состоящий из n блоков, где в i-й блок добавляют многоуровневые  признаки из i-го блок ViT с помощью: слоя cross-attention, FFN-слоя и skip connection с результатом i-го блока инъектора.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Side Tuning&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2206.06522" rel="nofollow noopener noreferrer"&gt;LST: Ladder Side-Tuning &lt;/a&gt;&lt;br&gt;&lt;br&gt;Side-tuning впервые предложили в LST. Идея в том, что адаптеры и prompt-tuning уменьшают число обучаемых параметров, но не решают проблему памяти, так как требуют полного распространения градиента. В side-tuning выходы адаптеров в исходную архитектуру не попадают напрямую, что экономит ресурсы.&lt;br&gt;&lt;br&gt;Реализация:&lt;br&gt;— добавляют несколько блоков-адаптеров, которые представляют собой маленькие трансформеры;&lt;br&gt;— с каждого трансформерного блока основной модели выход подают на соответствующий адаптер через линейное сжатие размерности. При такой подаче выход трансформерного блока суммируется с результатом предыдущего блока адаптера;&lt;br&gt;— суммирование происходит с помощью gate-механизма (обычный обучаемый гейт);&lt;br&gt;— метод можно применять как к декодеру, так и к энкодер-декодер-архитектурам. В ViLT-5 авторы использовали его только на уровне энкодеров-декодеров LLM, но не в самом ViT, так как там выход напрямую передаётся в адаптер для перевода визуальных токенов в языковые.&lt;br&gt;&lt;br&gt;Эксперименты показали, что использование классических адаптеров вместо трансформерных блоков ухудшает качество, как и замена gate на cross-attention. Для инициализации маленьких трансформеров применяли pruning с матрицей информации Фишера.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Prompt-like-методы&lt;br&gt;&lt;/strong&gt;&lt;br&gt;&lt;a href="https://arxiv.org/pdf/2203.12119" rel="nofollow noopener noreferrer"&gt;﻿﻿Visual prompt tuning&lt;br&gt;&lt;/a&gt;&lt;br&gt;Метод — буквально обычный Ptune, добавленный в сам ViT. Сравнивали, куда именно добавлять промпты: базовый вариант даёт результат не хуже остальных. Аналогично проверяли, куда подключать «классификационную голову» на выходе ViT, и снова базовый вариант оказался не хуже. Есть несколько вариаций: добавление промптов только в первый слой или deep visual prompt tuning — обучаемые векторы для каждого блока. &lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2109.01134" rel="nofollow noopener noreferrer"&gt;CoOp: Context Optimization&lt;/a&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2109.01134" rel="nofollow noopener noreferrer"&gt;&lt;br&gt;&lt;/a&gt;Метод, сделанный для CLIP в задачах классификации. Вместо ручного промпта используют обучаемые векторы. В отличие от Ptune, текстовый промпт тут убирается полностью. Метод сам по себе тривиальный, но стал базой для других подходов (например, CLIP-Adapter). &lt;br&gt;&lt;br&gt;&lt;em&gt;Разбор подготовил &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt;  Александр Мандров&lt;br&gt;&lt;/em&gt;&lt;a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>Emerging Properties in Unified Multimodal Pretraining</title><link>https://t.me/timeforcv/175</link><guid>https://t.me/timeforcv/175</guid><pubDate>Tue, 26 Aug 2025 09:05:15 +0000</pubDate><description>&lt;strong&gt;Emerging Properties in Unified Multimodal Pretraining&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Сегодня разбираем &lt;a href="https://arxiv.org/abs/2505.14683" rel="nofollow noopener noreferrer"&gt;работу&lt;/a&gt; о модели Bagel, способной генерировать и редактировать изображения, а также работать с последовательностями кадров. Авторы заявляют результаты, местами превосходящие Flux.1-dev, и позиционируют Bagel как одну из сильнейших открытых VLM. В своё время команда Bytedance занимала топ-1 на Text-to-Image Arena, сейчас уступают GPT, но остаются в числе лидеров.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Свойства мультимодальных моделей&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Понятие VLM постепенно меняется: от простых связок «текст-картинка» к системам, где на вход и выход можно подавать любые комбинации текста и изображений. Ключевые свойства таких моделей:&lt;br&gt;&lt;br&gt;— Дискретное vs непрерывное представление. Дискретные токенизаторы (например, VQ) ограничены размером словаря, из-за чего страдает качество. Bagel использует непрерывные представления.&lt;br&gt;&lt;br&gt;— Количество энкодеров. Эксперименты показывают, что для понимания и генерации нужны разные свойства эмбеддингов. Поэтому лучше использовать отдельные энкодеры: один для understanding-задач, другой для генерации.&lt;br&gt;&lt;br&gt;— Авторегрессивность. В Bagel отдельные патчи каждого изображения предсказываются параллельно, а не последовательно.&lt;br&gt;&lt;br&gt;— Интегрированный или внешний генератор. Возможны два подхода: всё в едином трансформере или через адаптер + внешнюю диффузионную модель. Bagel реализует первый вариант.&lt;br&gt;&lt;br&gt;— Open vs closed source. Отличительная черта Bagel — это открытый код, редкость среди моделей с непрерывными токенами.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Архитектура&lt;/strong&gt;&lt;br&gt;&lt;br&gt;В основе Bagel — крупный трансформер с двумя башнями для задач понимания и генерации. Для понимания используется SigLIP2, а для генерации — Flux VAE. Чтобы согласовать размеры представлений, добавлены MLP-адаптеры.&lt;br&gt;&lt;br&gt;Архитектура реализует принцип Mixture of Transformers: параллельно работают два трансформера (каждый на ~7B параметров). Токены разделяются между ними, а на отдельных шагах self-attention их представления смешиваются.&lt;br&gt;&lt;br&gt;Ключевой момент: вместо дискретного next-token prediction используется flow matching, где модель предсказывает векторы скорости в непрерывном пространстве. Эксперименты показывают, что эта стратегия даёт ощутимое преимущество.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Обучающие данные&lt;/strong&gt;&lt;br&gt;&lt;br&gt;В основе обучения триплет-схема данных: чистый текст, пары «текст-картинка» для задач понимания и мультимодальные примеры, где текст и изображения перемешаны. Основные источники данных — видео и веб-контент. Большая часть разметки сгенерирована синтетически с помощью Qwen-моделей (до 14B параметров) и DeepSeek для reasoning-трейсов. &lt;br&gt;&lt;br&gt;​​Для задачи редактирования авторы собирают данные за счёт аннотации различий между кадрами видео. Также берут связанные по смыслу последовательности изображений из веба, например из step-by-step-инструкций.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Обучение&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Обучение проходило в четыре стадии. Сначала проводился алайнмент энкодера. Обучался небольшой MLP-адаптер на выходах SigLIP2, тогда как остальные компоненты оставались замороженными.&lt;br&gt;&lt;br&gt;Затем претрейн: почти все части модели размораживались (кроме VAE), задачи понимания и генерации смешивались —причём оптимальным оказалось соотношение 4:1 в пользу генеративных задач. &lt;br&gt;&lt;br&gt;На стадии Continued Training разрешения увеличивались, а набор задач становился  разнообразнее. &lt;br&gt;&lt;br&gt;Завершающий шаг — SFT и дообучение. Здесь использовали промты, переформулированные с помощью DeepSeek, и внедряли reasoning-трейсы.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Результаты&lt;/strong&gt;&lt;br&gt;&lt;strong&gt;&lt;br&gt;&lt;/strong&gt;В задачах на понимание изображений Bagel показывает топовые результаты почти во всех бенчмарках, уступая лишь Qwen-2.5-VL на MMMU. В генерации модель на GenEval превосходит Flux и делит второе место со своей облегчённой версией, а в более сложном бенчмарке WICE занимает второе место сразу после GPT-Image.&lt;br&gt;&lt;br&gt;&lt;em&gt;Разбор подготовил &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt; Александр Устюжанин&lt;/em&gt;&lt;br&gt;&lt;a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>Nexus-Gen: A Unified Model for Image Understanding, Generation, and Editing</title><link>https://t.me/timeforcv/173</link><guid>https://t.me/timeforcv/173</guid><pubDate>Thu, 21 Aug 2025 08:02:29 +0000</pubDate><description>&lt;strong&gt;Nexus-Gen: A Unified Model for Image Understanding, Generation, and Editing&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Сегодня разбираем &lt;a href="https://arxiv.org/pdf/2504.21356" rel="nofollow noopener noreferrer"&gt;статью&lt;/a&gt; о Nexus-Gen — мультимодальной модели от Alibaba, которая задумывалась как полностью открытая: авторы выложили не только код и веса, но и датасет. Модель умеет генерировать и редактировать изображения по текстовым запросам. &lt;br&gt;&lt;br&gt;Качество картинок в целом достойное, хотя не всегда удаётся сохранить идентичность объектов при редактировании: при простых изменениях могут искажаться второстепенные детали — например, у человека слегка меняются черты лица, а в интерьере исчезают или трансформируются объекты, которые трогать не просили.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Архитектура&lt;/strong&gt;&lt;br&gt;&lt;br&gt;В основе модели авторегрессор (Qwen-2.5-VL) в связке с визуальным энкодером и декодером на базе Flux. Архитектура вдохновлена &lt;a href="https://arxiv.org/html/2503.13436v1" rel="nofollow noopener noreferrer"&gt;UniFLUID&lt;/a&gt;: текст и изображение проходят через общий авторегрессор, а для визуальной части используется отдельный визуальный декодер. В новой версии также добавлен декодер для редактирования изображений, который работает вместе с генеративным.&lt;br&gt;&lt;br&gt;Главное улучшение модели связано с проблемой накопления ошибок на непрерывных визуальных токенах. В отличие от текста, где токены дискретны и ошибки не накапливаются, изображения страдают от смещения при последовательной генерации патчей. Авторы предложили решение: ввести специальный обучаемый токен, который обозначает места для генерации визуальных патчей. При обучении он вставляется в последовательность, а при инференсе автоматически генерируется и подаётся в диффузионную голову. Таким образом, модель всегда работает с фиксированным токеном, не накапливая ошибок с предыдущих шагов.&lt;br&gt;&lt;br&gt;Для обучения используется комбинация лоссов: кросс-энтропия для текстовых токенов, MSE и косинусная близость — для визуальных. Это позволяет согласовать пространство визуального энкодера и выходы авторегрессора, сохраняя совместимость с диффузионной частью.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Этапы обучения&lt;br&gt;&lt;/strong&gt;&lt;br&gt;Сначала модель училась на задачах image understanding и image generation без учёта редактирования. На втором этапе задачи редактирования добавлялись в небольшом количестве. На третьем — к обучению подключили новый декодер для задач редактирования, а баланс сместился в сторону таких задач. На заключительном шаге проводили элайнмент между визуальными представлениями на входе и выходе авторегрессора, чтобы стабилизировать работу с диффузией и улучшить согласованность между генеративным и редактирующим декодерами.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Результаты &lt;/strong&gt;&lt;br&gt;&lt;br&gt;В новой версии Nexus-Gen авторы, наконец, показали количественные результаты: модель на 7B параметров занимает первое место на ряде бенчмарков по пониманию изображений, включая MME-P (1602,3) и TextVQA (75,5). Также она показывает высокий уровень на VQAv2 (79,3) и SEED (77,1), сопоставимый или превосходящий конкурентов ощутимо больших размеров. При этом она сохраняет баланс между пониманием, генерацией и редактированием.&lt;br&gt;&lt;br&gt;&lt;em&gt;Разбор подготовил &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt;  Михаил Колтаков&lt;br&gt;&lt;/em&gt;&lt;a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>Тематическая подборка статей: генерация с эдитингом и VLM с генерацией</title><link>https://t.me/timeforcv/172</link><guid>https://t.me/timeforcv/172</guid><pubDate>Wed, 13 Aug 2025 08:03:22 +0000</pubDate><description>&lt;strong&gt;Тематическая подборка статей: генерация с эдитингом и VLM с генерацией&lt;br&gt;&lt;/strong&gt;&lt;br&gt;Сегодня подборка объединяет два актуальных направления в CV: развитие генеративных моделей с возможностью редактирования изображений и интеграцию генерации в VLM. &lt;br&gt;&lt;br&gt;&lt;strong&gt;Генерация со встроенным эдитингом&lt;br&gt;&lt;/strong&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2505.22705" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;HiDream-I1: A High-Efficient Image Generative Foundation Model with Sparse Diffusion Transformer&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;Неплохая модель по меркам опенсорса. Авторы используют трансформер с mixture-of-experts-блоками и гибридной архитектурой MM-DiT: текстовые и картиночные токены сначала процессятся отдельными слоями, затем — общими. В решении применяются четыре разных текстовых энкодера — выглядит как рекорд. Также авторы делают дообучение модели под задачи эдитинга — самое горячее направление в генерации картинок, которому посвящены и следующие работы.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2505.20275" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;ImgEdit: A Unified Image Editing Dataset and Benchmark&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;В статье предлагают полный набор для задачи эдитинга: датасет для обучения (автоматический пайплайн, которым сгенерировали 1,2 млн сэмплов, в том числе с многошаговым сценарием); обученную на нём модель (соединили VLM и DiT, переиспользовав Qwen и Flux) и бенчмарк для оценки качества (также обучили Qwen-as-a-judge, чтобы избежать разметки людьми).&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2505.17768" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;R-Genie: Reasoning-Guided Generative Image Editing&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;Модель для редактирования изображений с упором на задачи, требующие рассуждений (пример: «Замени самого сонного человека на изображении на кота»). Авторы предлагают свой бенчмарк под такую задачу. Архитектурно соединяют VLM и DiT, но с хитрыми блоками-перемычками между ними.&lt;br&gt;&lt;br&gt;&lt;strong&gt;VLM со встроенной генерацией&lt;br&gt;&lt;/strong&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2505.19616" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Diagnosing and Mitigating Modality Interference in Multimodal Large Language Models&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;Предлагают набор регуляризаций, чтобы VLM лучше связывала текстовую и картиночную модальности. В частности, при обучении в текстовых задачах авторы подают случайную (мусорную) картинку и требуют, чтобы предсказание модели не изменилось; добавляют adversarial-шум к картиночным токенам.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2505.23661" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;Соединяют VLM (InternVL3, веса заморожены) и диффузионный генератор (SANA, дообучается на второй стадии на 60 тыс. изображениях) через шестислойный трансформер (обучается на первой и второй стадиях). Пайплайн выглядит просто, качество сравнимо с другими открытыми аналогами.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2505.23606" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model&lt;br&gt;&lt;/strong&gt;&lt;/a&gt;Особенность работы в том, что для генерации изображений и текстов авторы используют дискретную диффузию. В качестве бэкбона берут предобученный MM-DiT, и добавляют энкодер/декодер для картинок и текстов. Качество не топовое, работа имеет скорее концептуальную ценность.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2505.23043" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Are Unified Vision-Language Models Necessary: Generalization Across Understanding and Generation&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;Авторы показывают, что задачи генерации и дискриминации могут обогащать друг друга при совместном обучении. Особенно хорошо работает, когда вход и выход имеют схожую природу: «SigLIP in / SigLIP out» или «VQA in / VQA out».&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2505.17534" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Co-Reinforcement Learning for Unified Multimodal Understanding and Generation&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;Статья о том, как делать RL для архитектуры вроде Janus-Pro. Интересная идея — использовать GRPO с Cycle Consistency Reward: модель учится и на дискриминации, и на генерации, проверяя, насколько хорошо восстановленный ввод совпадает с исходным.&lt;br&gt;&lt;br&gt;&lt;em&gt;Подборку подготовил &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt; Артём Конев&lt;br&gt;&lt;/em&gt;&lt;a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion Models</title><link>https://t.me/timeforcv/170</link><guid>https://t.me/timeforcv/170</guid><pubDate>Thu, 07 Aug 2025 09:29:46 +0000</pubDate><description>&lt;strong&gt;Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion Models&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Сегодня речь пойдёт об улучшении генерации видео. Разберём &lt;a href="https://arxiv.org/abs/2506.09229" rel="nofollow noopener noreferrer"&gt;статью&lt;/a&gt; о Cross-frame Representation Alignment (CREPA) — адаптированной версии REPA.&lt;br&gt;&lt;br&gt;Метод REPA разработан для генерации изображений. Он считает similarity-score между промежуточным представлением диффузионной модели и предподсчитанными визуальными фичами (например, DINO). Чтобы приблизить фичи, в модели similarity-score добавляется к диффузионному лоссу. Именно в этом кроется потенциал REPA для тонкой настройки диффузионной модели.&lt;br&gt;&lt;br&gt;Авторы предлагают два способа обобщения картиночного REPA на видео: &lt;br&gt;&lt;br&gt;1. Применять REPA для каждого из кадров. Но REPA-составляющая никак не учитывает темпоральную связь между кадрами, что может порождать неконсистентные генерации.&lt;br&gt;&lt;br&gt;2. CREPA. В лосс для каждого кадра добавляются similarity-score соседних представлений (с некоторым коэффициентом) — темпоральная связь появляется, проблема решена! &lt;br&gt;&lt;br&gt;Для апробации CREPA авторы использовали две модели CogVideoX-5B и Hunyuan Video. Результаты их работы можно оценить на иллюстрациях (первая генерация — от CogVideoX-5B). Визуально консистентность растёт. А авторы отмечают динамику FVD 305-291-281 для Vanilla-REPA-CREPA.&lt;br&gt; &lt;br&gt;&lt;em&gt;Разбор подготовил &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt; Андрей Чернов&lt;/em&gt;&lt;br&gt;&lt;a href="https://t.me/timeforcv" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>Forte: Finding Outliers with Representation Typicality Estimation</title><link>https://t.me/timeforcv/169</link><guid>https://t.me/timeforcv/169</guid><pubDate>Thu, 31 Jul 2025 11:59:18 +0000</pubDate><description>&lt;strong&gt;Forte: Finding Outliers with Representation Typicality Estimation&lt;br&gt;&lt;/strong&gt;&lt;br&gt;Сегодня разбираем &lt;a href="https://arxiv.org/abs/2410.01322" rel="nofollow noopener noreferrer"&gt;статью&lt;/a&gt;, в которой авторы представляют новый метод обнаружения выбросов (out-of-distribution) для картиночных датасетов. Метод показал лучшие результаты в задаче &lt;a href="https://paperswithcode.com/sota/out-of-distribution-detection-on-imagenet-1k-13" rel="nofollow noopener noreferrer"&gt;Out-of-Distribution Detection on ImageNet-1k vs NINCO&lt;/a&gt; (AUROC = 98.34, FPR@95 = 5.18).&lt;br&gt;&lt;br&gt;В работе утверждается, что низкое значение likelihood не всегда эффективно для обнаружения аутлаеров в пространствах высокой размерности. Вместо likelihood предлагается использовать оценку typicality, по аналогии с подходом из &lt;a href="https://arxiv.org/abs/2006.09273" rel="nofollow noopener noreferrer"&gt;Density of States Estimator (DoSE)&lt;/a&gt;: для каждого изображения собираются статистики эмбеддинга, после чего на этих признаках обучается модель оценки плотности. Авторы тестируют One-Class SVM, Gaussian Kernel Density Estimation и Gaussian Mixture Model. Полученные оценки плотности используются для вычисления typicality каждого изображения. При этом для обучения используются только in-distribution-данные. Для получения статистик применяются локальные геометрические признаки из работ по manifold estimation (например, Recall per point — доля in-distribution-семплов в радиусе, равном расстоянию до ближайшего соседа).&lt;br&gt;&lt;br&gt;Авторы показывают, что метод позволяет успешно обнаруживать сгенерированные изображения. Например, при модификации изображений с помощью Stable Diffusion 2.0 при strength=0.5 (умеренное изменение оригинала) достигаются AUROC = 82.93 и FPR@95 = 46.80.&lt;br&gt;&lt;br&gt;Этот алгоритм оказался интересен ML-разработке Яндекс Карт в задаче поиска фотографий, которые пользователи по ошибке загрузили в неправильную организацию. Его применили для нахождения аутлаеров на двух датасетах: один разметили вручную, второй — автоматически. Для автоматической разметки использовали косинус между изображением и строкой, состоящей из {название организации} + {рубрика организации}.&lt;br&gt;&lt;br&gt;На размеченном датасете Forte показал AUROC = 91.68 и FPR@95TPR = 20.95, а на синтетическом — AUROC = 85.24 и FPR@95TPR = 93.24. При этом текущий бейзлайн, который фильтрует аутлайеры по значению косинуса, набирает AUROC = 81.02 и FPR@95TPR = 82.87.&lt;br&gt;&lt;br&gt;Пока преимущество Forte над нашим бейзлайном не выглядит значительным, но идея использования методов из manifold estimation кажется перспективной.&lt;br&gt;&lt;br&gt;&lt;em&gt;Разбор подготовил &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt;  Иван Балашов&lt;br&gt;&lt;/em&gt;&lt;a href="https://t.me/timeforcv" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>Perception Encoder: The best visual embeddings are not at the output of the network</title><link>https://t.me/timeforcv/168</link><guid>https://t.me/timeforcv/168</guid><pubDate>Tue, 22 Jul 2025 10:05:45 +0000</pubDate><description>&lt;strong&gt;Perception Encoder: The best visual embeddings are not at the output of the network &lt;/strong&gt;&lt;br&gt;&lt;br&gt;Сегодня разбираем &lt;a href="https://arxiv.org/abs/2504.13181" rel="nofollow noopener noreferrer"&gt;статью&lt;/a&gt;, авторы которой предлагают простой визуальный энкодер, обученный только на открытых данных, без сложных архитектур и языковых моделей. Всё обучение — это contrastive learning между изображениями и подписями. Исследователи показывают, что даже в таком режиме можно получить эмбеддинги, которые превосходят существующие модели на стандартных бенчмарках. Главная идея: сильные визуальные представления появляются не обязательно в последнем слое модели, а где-то внутри.&lt;br&gt;&lt;br&gt;В архитектуре используется базовая ViT-модель с разрешением 224. При обучении применяются стандартные аугментации, attention pooling через CLS-токен и несколько инженерных приёмов: прогрессивное увеличение разрешения, обучение с большим batch size, оптимизатор LAMB вместо AdamW, маскирование части изображений с регуляризацией (maskfit), RoPE вместе с позиционными эмбеддингами. Вся модель обучается на contrastive loss — пары «изображение-текст» берут из общедоступных коллекций. Чтобы сэкономить вычисления, сначала обучают на низком разрешении, потом повышают до 336. Такой подход не только ускоряет обучение, но и, как утверждают авторы, помогает избежать переобучения позиционных эмбеддингов.&lt;br&gt;&lt;br&gt;После обучения на изображениях авторы подключают видео. Они берут небольшой датасет с роликами и описаниями, прогоняют по 8 кадров через perception encoder, усредняют эмбеддинги и обучают contrastive loss на парах «видео-текст». Часть описаний взяли из открытых источников, часть — сгенерировали своей моделью. Для этого они собрали отдельную VLM (PLM), в которую встроили perception encoder и дообучили на видео и картинках с подписями. Модель даёт черновой текст, который потом правят вручную и добавляют метаинформацию — действия, объекты, временные сегменты. Эти описания идут в обучение. Авторы пишут, что это помогает даже в задачах классификации изображений. &lt;br&gt;&lt;br&gt;На бенчмарках perception encoder показывает хорошие результаты. Авторы замечают: если взять не последний слой, а, например, 47-й, то на многих задачах это даёт лучший результат. У других моделей эмбеддинги либо слабее в середине, либо не меняются от увеличения модели. У perception encoder эффект усиления заметен.&lt;br&gt;&lt;br&gt;Чтобы подключить этот энкодер к языковой модели, обучают projection head на выбранном слое — с температурой и двухслойным MLP. Такой подход даёт выигрыш по качеству по сравнению с head&amp;#x27;ами на других слоях. Чем больше языковая модель — тем выше метрики.&lt;br&gt;&lt;br&gt;Однако есть несколько моментов, которые вызывают вопросы. Во-первых, сравнение с конкурентами неполное: в основной статье нет упоминания Qwen, хотя в другом материале от тех же авторов сравнение с ней есть — и Qwen выигрывает по ряду задач. Во-вторых, идея, что видеоданные помогают классификации изображений, не объяснена, авторы не предлагают гипотезу, почему так происходит. В-третьих, подход с выбором «лучшего» слоя работает у их модели, но неясно, насколько он универсален. Отдельно хочется понять, насколько perception encoder стабилен вне тех задач, которые выбрали для оценки.&lt;br&gt;&lt;br&gt;В целом статья показывает, что простая архитектура с грамотной инженерией и небольшим дообучением может дать представления, которые хорошо работают на downstream-задачах. Авторы не предлагают революции, но аккуратно исследуют поведение модели и дают полезные практические выводы — особенно про выбор слоя и влияние видеоданных.&lt;br&gt;&lt;br&gt;&lt;em&gt;Разбор подготовил &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt; Малик Газизуллин&lt;/em&gt;&lt;br&gt;&lt;a href="https://t.me/timeforcv" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>Тематическая подборка статей: дискриминативные модели</title><link>https://t.me/timeforcv/167</link><guid>https://t.me/timeforcv/167</guid><pubDate>Thu, 17 Jul 2025 07:48:44 +0000</pubDate><description>&lt;strong&gt;Тематическая подборка статей: дискриминативные модели&lt;br&gt;&lt;/strong&gt;&lt;br&gt;Свежая подборка статей о методах улучшения взаимодействия текста и изображений в мультимодальных моделях. В центре внимания — файнтюн CLIP для понимания отрицаний, новые подходы к retrieval, оптимизации архитектур Vision Transformer и многое другое.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Дообучение CLIP-моделей&lt;br&gt;&lt;/strong&gt; &lt;br&gt;&lt;a href="https://arxiv.org/abs/2505.18434" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;TNG-CLIP: Training-Time Negation Data Generation for Negation Awareness of CLIP&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;&lt;br&gt;&lt;/strong&gt;&lt;br&gt;Предлагают пайплайн файнтюна текстовой части CLIP на понимание отрицаний: на лету для батча генерируют новые тексты, содержащие отрицания, используя тексты с похожих картинок для усложнения задачи. Также показывают, что можно подменить текстовый энкодер в предобученной диффузионной модели, и генерации с отрицаниями в промпте тоже станут лучше.&lt;br&gt;&lt;br&gt;&lt;a href="https://www.arxiv.org/abs/2505.20291" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Visualized Text-to-Image Retrieval&lt;/strong&gt;&lt;br&gt;&lt;/a&gt;&lt;br&gt;Авторы говорят, что вместо text-to-image retrieval можно сначала сгенерировать картинку по текстовому запросу, а потом уже делать image-to-image retrieval чисто по картиночным фичам. Тестируются на специфических постановках задач типа RAG, но идея интересная.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2505.20152" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;&lt;br&gt;&lt;/strong&gt;&lt;br&gt;Доливают в обучение CLIP датасет с геометрией и используют полученную модель как энкодер в VLM. Геометрические датасеты добавляют и в другие стадии обучения VLM, но основная новизна в том, как сделать файнтюн на геометрию в CLIP-постановке.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2505.21549" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal Transformer Distillation&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;Для дообучения CLIP собирают модель-учитель, которая извлекает картиночные фичи по выделенным через YOLO областям и агрегирует их через cross-attention с текстовыми фичами; затем этот учитель используется для дистилляции. С ростом качества на retrieval-задачах метод просаживает точность zero-shot-классификации.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2505.21501" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Vision Transformers with Self-Distilled Registers&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;&lt;br&gt;&lt;/strong&gt;&lt;br&gt;Изучают проблему токенов-аутлаеров в трансформерных моделях, описанную в статье &lt;a href="https://arxiv.org/html/2309.16588v2" rel="nofollow noopener noreferrer"&gt;Vision Transformers Need Registers&lt;/a&gt;. В ней предложили на вход модели подавать токены-регистры. Также авторы пишут, что такие токены можно добавлять в уже обученную модель и файнтюнить её так, чтобы аутлаеры «перетекали» в добавленные токены.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Архитектура дискриминативных моделей&lt;/strong&gt;&lt;br&gt; &lt;br&gt;&lt;a href="https://arxiv.org/abs/2505.21910" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Taming Transformer Without Using Learning Rate Warmup&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2505.21910" rel="nofollow noopener noreferrer"&gt;&lt;br&gt;&lt;/a&gt;Связывают нестабильность в обучении трансформеров с тем, что матрица аттеншена становится низкоранговой и разреженной. Предлагают добавить в Adam ограничение на learning rate для апдейтов, которые имеют высокую спектральную норму по сравнению с текущей матрицей. Показывают, что в этом случае возможно обучение без lr-warmup&amp;#x27;а.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2505.21847" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;RePaViT: Scalable Vision Transformer Acceleration via Structural Reparameterization on Feedforward Network Layers&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;Применяют идеи из ShuffleNet к ViT: в FFN-блоке делают нелинейность только для части нейронов промежуточного слоя — вторую часть можно после обучения вмерджить в одну линейную операцию. Также заменяют LayerNorm на BatchNorm и его тоже вмердживают после обучения. Но тестируют всё это только на ImageNet, есть подозрение, что на более сложных датасетах профита не будет.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2505.23769" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;TextRegion: Text-Aligned Region Tokens from Frozen Image-Text Models&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;Решают zero-shot-сегментацию и смежные задачи, предлагают пайплайн, в котором объединяют SAM и CLIP-модель: через SAM находят области с объектами, и в CLIP-модели модифицируют аттеншен последнего слоя, чтобы он смотрел на каждую область по отдельности — таким образом получают токены для областей, которые уже можно сопоставлять с текстовыми представлениями класса и делать сегментацию.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2505.18153" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;REN: Fast and Efficient Region Encodings from Patch-Based Image Encoders&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;&lt;br&gt;&lt;/strong&gt;&lt;br&gt;Отличие от предыдущей статьи в том, что теперь область интереса на картинке кодируем координатами точки. Имея предобученный бэкбон, добавляем к нему голову, которая по координатам точки смотрит на карту признаков и возвращает эмбеддинг соответствующего ей объекта; SAM теперь используется только на этапе обучения.&lt;br&gt;&lt;br&gt;&lt;em&gt;Подборку подготовил &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt; Артём Конев&lt;br&gt;&lt;/em&gt;&lt;a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>Впечатления от конференции ICLR 2025</title><link>https://t.me/timeforcv/157</link><guid>https://t.me/timeforcv/157</guid><pubDate>Fri, 11 Jul 2025 11:59:15 +0000</pubDate><description>&lt;strong&gt;Впечатления от конференции ICLR 2025&lt;/strong&gt;&lt;br&gt;&lt;br&gt;ICLR 2025 принесла много полезных работ на тему CV. Мы попросили инженеров Яндекса подвести личные итоги конференции и рассказать, чем она запомнилась. О трендах в индустрии, интересных статьях и многом другом — в наших карточках.&lt;br&gt;&lt;br&gt;Работы, которые упоминаются в посте:&lt;br&gt;— &lt;a href="https://iclr.cc/virtual/2025/invited-talk/36782" rel="nofollow noopener noreferrer"&gt;Building Safe and Robust AI Systems &lt;/a&gt;&lt;br&gt;— &lt;a href="https://iclr.cc/virtual/2025/invited-talk/36785" rel="nofollow noopener noreferrer"&gt;Pursue the Nature of Intelligence&lt;/a&gt;&lt;br&gt;— &lt;a href="https://arxiv.org/abs/1412.6980" rel="nofollow noopener noreferrer"&gt;Adam: A Method for Stochastic Optimization&lt;/a&gt;&lt;br&gt;— &lt;a href="https://arxiv.org/abs/1409.0473" rel="nofollow noopener noreferrer"&gt;Neural Machine Translation by Jointly Learning to Align and Translate&lt;/a&gt;&lt;br&gt;— &lt;a href="https://arxiv.org/abs/2410.01322" rel="nofollow noopener noreferrer"&gt;Finding Outliers Using Representations Typicality Estimation&lt;/a&gt;&lt;br&gt;— &lt;a href="https://arxiv.org/pdf/2410.08182" rel="nofollow noopener noreferrer"&gt;MRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented Multimodal Models&lt;/a&gt;&lt;br&gt;— &lt;a href="https://arxiv.org/pdf/2411.02937" rel="nofollow noopener noreferrer"&gt;Benchmarking Multimodal Retrieval Augmented Generation with Dynamic VQA Dataset and Self-adaptive Planning Agent&lt;/a&gt;&lt;br&gt;— &lt;a href="https://arxiv.org/pdf/2409.12959" rel="nofollow noopener noreferrer"&gt;MMSEARCH: Unveiling the Potential of Large Models as Multi-modal Search Engines&lt;/a&gt;&lt;br&gt;— &lt;a href="https://arxiv.org/abs/2411.02571" rel="nofollow noopener noreferrer"&gt;MM-Embed: Universal Multimodal Retrieval with Multimodal LLMs&lt;/a&gt;&lt;br&gt;— &lt;a href="https://habr.com/ru/companies/yandex/articles/847706/" rel="nofollow noopener noreferrer"&gt;VLM в Нейро: как мы создавали мультимодальную нейросеть для поиска по картинкам&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;&lt;br&gt;&lt;br&gt;#YaICLR</description></item><item><title>ICLR 2025: полезные статьи на тему CV</title><link>https://t.me/timeforcv/150</link><guid>https://t.me/timeforcv/150</guid><pubDate>Tue, 08 Jul 2025 08:02:44 +0000</pubDate><description>&lt;strong&gt;ICLR 2025: полезные статьи на тему CV&lt;br&gt;&lt;/strong&gt;&lt;br&gt;Конференция прошла, а интересные статьи, которые мы не успели упомянуть в наших подборках, — остались. Александр Шишеня, ведущий разработчик службы компьютерного зрения, отобрал и прокомментировал несколько работ, заслуживающих внимания.&lt;br&gt;&lt;br&gt;&lt;a href="https://web.cs.ucla.edu/~guyvdb/slides/LLMCP24.pdf" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Symbolic reasoning about LLMs&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&lt;a href="https://web.cs.ucla.edu/~guyvdb/slides/LLMCP24.pdf" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;&lt;br&gt;&lt;/strong&gt;&lt;/a&gt;- Подход &lt;a href="https://arxiv.org/pdf/2406.13892" rel="nofollow noopener noreferrer"&gt;Ctrl-G&lt;/a&gt; позволяет модели генерировать ответ, который подчиняется жёстким условиям (например, валидный JSON). Основан на использовании детерминистического конечного автомата и скрытой марковской цепи в дополнение к обученной LLM.&lt;br&gt;- Можно навешивать мягкие ограничения в виде дополнительной LLM, заточенной на сдвиг генерации в нужное направление (например, убирать токсичность).&lt;br&gt;&lt;br&gt;&lt;a href="https://iclr.cc/virtual/2025/10000308" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Neural Networks as Graphs&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;Используют графовую нейросеть для генерации апдейтов весов сети при обучении. Лучший результат получается, если чередовать такие нейросетевые апдейты с итерациями Adam. Один из авторов работы — Борис Князев. &lt;br&gt;&lt;br&gt;&lt;a href="https://iclr.cc/virtual/2025/invited-talk/10000532" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Training Language Models in Academia: Challenge or Calling?&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;У академии на несколько порядков меньше ресурсов, чем у индустрии. Какую же роль в таком случае может играть академия в современном DL? Автор даёт свой ответ: возможностей академии хватает, чтобы делать полезный ресерч, а жёсткие ограничения диктуют направление развития — это оптимизация ресурсов и поиск подходов по ускорению обучения. В качестве доказательства приводится список работ Best Paper Awards ICML 2025, где большинство работ выполнено академией. Сомнительное доказательство — ведь можно предположить, что индустрии просто не так важно публиковаться, да и коммерческую тайну никто не отменял.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2411.02780" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;How much is a noisy image worth? Data Scaling Laws for Ambient Diffusion&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;Эффективно используются шумные данные для обучения диффузии. Выведен специальный лосс, который применяется к шумным сэмплам, а для чистых данных используется обычный лосс.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/html/2410.10812v1" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;HART: Efficient Visual Generation with Hybrid Autoregressive Transformer&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;Статья от MIT и NVIDIA. Предлагается картиночный токенизатор, который генерирует дискретные токены и непрерывные поправки к ним. Далее дискретные токены предсказываются авторегрессионной моделью, а непрерывные — легковесной диффузионной моделью.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2412.10891" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Zigzag Diffusion Sampling: Diffusion Models Can Self-Improve via Self-Reflection&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;Улучшают качество генерации изображений, чередуя прямую генерацию с высоким гайденсом и обратную генерацию с низким гайденсом.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2404.07206" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;GoodDrag: Towards Good Practices for Drag Editing with Diffusion Models&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;Редактирование изображений с помощью варпа. Фишка в том, что итерации варпа и денойзинга применяются попеременно — это позволяет достичь лучшего качества, чем последовательное применение сначала полного варпа, а потом расшумления.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2501.05803" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Test-time Alignment of Diffusion Models without Reward Over-optimization&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;Элайнмент диффузионной модели на этапе сэмплирования. RL-Objective можно явно оптимизировать и выразить целевую плотность вероятности через плотность вероятности претренированной модели и реворд-функцию. Дальше сэмплируются сразу несколько траекторий, попутно отсеивая траектории с низким ревордом, добавляя новые и постепенно уменьшая силу гайденса.&lt;br&gt;&lt;br&gt;&lt;a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;&lt;br&gt;&lt;br&gt;#YaICLR</description></item><item><title>Что читает команда распознавания текста в VLM: подборка актуальных статей</title><link>https://t.me/timeforcv/149</link><guid>https://t.me/timeforcv/149</guid><pubDate>Tue, 01 Jul 2025 08:35:01 +0000</pubDate><description>&lt;strong&gt;Что читает команда распознавания текста в VLM: подборка актуальных статей &lt;/strong&gt;&lt;br&gt;&lt;br&gt;Инженеры VLM-команды Яндекса поделились статьями, которые они в последнее время читали и обсуждали. В сегодняшней подборке: новые подходы к генерации инфографики, свежие бенчмарки для мультимодальных моделей, работающие пайплайны генерации кода по графику и попытки добавить зрение в диффузионки. &lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2505.18668" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;ChartGalaxy: A Dataset for Infographic Chart Understanding and Generation&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;Статья о том, как сгенерировать около миллиона инфографик. Авторы подробно описали каждую стадию процесса: сбор шаблонов, индексирование описаний, иконок и других элементов для заполнения шаблонов, фильтрацию и проверку качества.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2505.19028" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;InfoChartQA: A Benchmark for Multimodal Question Answering on Infographic Charts&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;Авторы собрали новый бенчмарк позволяющий проверить, как VLM-модели понимают инфографику. Для каждой инфографики сделали упрощённую версию в виде обычного графика с теми же данными — модели справляются с таким заметно лучше, чем с визуально перегруженным оригиналом. Также добавили новый тип вопросов по отдельным кропам из изображения инфографики — на понимание мелких визуальных деталей.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2501.06598" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;ChartCoder: Advancing Multimodal Large Language Model for Chart-to-Code Generation&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;Авторы обучили модель понимать графики: она получает изображение и возвращает код на Python (Matplotlib), чтобы построить такой же график. Для этого использовали стратегию Snippet-of-Thoughts (SoT) — пошаговое рассуждение перед финальной генерацией кода. Взяли LLM, способную писать код, собрали датасет под задачу (160 тысяч картинок, на каждую — один вопрос и ответ). Кратко описали пайплайн его создания. Модель показывает лучшие результаты среди аналогов такого же размера (включая почти самые свежие Qwen и InternVL). В ablation-экспериментах дообучили Qwen на своём датасете — получили прирост; 384 px + Anyres почти хватает для большинства графиков.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/pdf/2504.10659" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Relation-Rich Visual Document Generator for Visual Information Extraction &lt;/strong&gt;&lt;/a&gt;&lt;br&gt;Статья с CVPR 2025 о генерации синтетических text-rich-документов с логической структурой (таких, как формы). Пайплайн генерации любопытен тем, что в нём сначала генерируют текст с помощью ChatGPT, а уже потом — структуру документа (laytout). Чаще встречается обратный вариант, когда структуру документа заполняют текстом. Авторы показывают, что обучение Qwen2-VL и Llava-NexT-mistral на таких данных улучшает метрики распознавания текста и извлечения информации на публичных бенчмарках.&lt;br&gt;&lt;br&gt;&lt;a href="https://www.arxiv.org/abs/2505.16933" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;Авторы попытались расширить предобученную текстовую диффузию LLaDA на мультимодальность, добавив визуальный вход через SigLIP2 и MLP-проекцию в языковое пространство. Итоговая модель зафайнтюнена на визуальных и reasoning-focused-инструкциях MAmmoTH-VL и VisualWebInstruct и бьёт автогрессионные и диффузионные бейзлайны по ряду мультидисциплинарных и визуально-математических бенчмарков.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2501.17161" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;Интересная статья, авторы которой подтверждают тезис из названия: SFT хорошо запоминает жёсткие форматы и правила, но плохо справляется с out-of-distribution-задачами. В то же время RL реально улучшает генерализацию и показывает заметный прирост на OOD-случаях. Но SFT всё равно нужен, чтобы RL вообще завёлся. В противном случае модель не умеет нормально реагировать на инструкции или генерирует неконтролируемый выход. RL-обучение не получает положительного сигнала. Это справедливо как для LLM, так и для VLM.&lt;br&gt;&lt;br&gt;&lt;em&gt;Подборку подготовила &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt; Команда распознавания текста в VLM&lt;/em&gt;&lt;br&gt;&lt;a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>Scaling Vision Pre-Training to 4K Resolution</title><link>https://t.me/timeforcv/148</link><guid>https://t.me/timeforcv/148</guid><pubDate>Thu, 26 Jun 2025 08:04:55 +0000</pubDate><description>&lt;strong&gt;Scaling Vision Pre-Training to 4K Resolution&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Большинство доступных визуальных энкодеров предобучено на изображениях низкого разрешения: например, на 378✕378, как SigLIP. Это становится серьёзной проблемой, если вы хотите обрабатывать изображения высокого разрешения с мелкими деталями. Дорожный знак STOP будет неразличим, если сжать кадры записи видеорегистратора до 378✕378. То же касается распознавания текста, где много мелких деталей.&lt;br&gt;&lt;br&gt;Авторы сегодняшней &lt;a href="https://arxiv.org/abs/2503.19903v1" rel="nofollow noopener noreferrer"&gt;статьи&lt;/a&gt; отмечают, что в индустрии уже борются с этой проблемой. Методом &lt;a href="https://arxiv.org/pdf/2404.16821" rel="nofollow noopener noreferrer"&gt;AnyRes&lt;/a&gt; режут большое изображение на части поменьше — тайлы без пересечений. Или, как в &lt;a href="https://arxiv.org/pdf/2403.13043v2" rel="nofollow noopener noreferrer"&gt;S2&lt;/a&gt;, одновременно ресайзят изображение до нужного размера и делят его оригинал на тайлы, чтобы добавить каналы для описания одних и тех же участков изображения в более высоком разрешении. Но эти методы кодируют картинку заранее — не учитывая запрос пользователя. Логично предположить, что для вопроса, например, про одежду человека, не нужно кодировать автомобили и здания.&lt;br&gt;&lt;br&gt;Новое решение, которое предлагают авторы сегодняшней статьи, учитывает промпты пользователя. Они предлагают подбирать куски изображения, которые подходят под запрос, и подмешивать их в инпут. Сделать это можно в два шага:&lt;br&gt;&lt;br&gt;1. предобучить энкодер PS3, который сможет угадывать подходящие области изображения;&lt;br&gt;2. обучить VLM отвечать на запросы пользователя вместе c энкодером PS3.&lt;br&gt;&lt;br&gt;А если промпта нет и top-down-selection невозможен, можно подключить bottom-up-selection: попросить нейросеть самостоятельно выбрать интересные области. «Интересность» при этом определяется данными, на которых обучалась модель.&lt;br&gt;&lt;br&gt;Архитектура PS3 изображена на схеме. На входе — предобученный SigLIP. Энкодим им изображение и получаем low-res-фичи. Из-за ресайза теряются все высокоуровневые фичи. Авторы предлагают исправить это с помощью дополнительного так называемого light-weight-high-res-энкодера (обучаемая урезанная CNN). Третьей фичой будет либо эмбеддинг текста, чтобы выбрать интересный образ, либо обучаемый эмбеддинг, который заменит промпт. По этой тройке для каждой позиции предсказывается вероятность её релевантности: вырезают топ-K областей и энкодят через SigLIP (несколько раз в разных разрешениях). &lt;br&gt;&lt;br&gt;Итоговые фичи картинки — исходные low-res и вырезанные топ-K областей. Чтобы подключить PS3 к VLM, понадобится LLM: достаточно передать последний токен из запроса к ней в PS3. Отобрав топ выученных с энкодом позиционных эмбеддов, можно переходить к тренировке language modeling.&lt;br&gt;&lt;br&gt;Для эффективного обучения VLM вместе с PS3 нужно дотюнить выбор региона, чтобы подмена не ощущалась. А дальше можно тренировать модель как обычно. &lt;br&gt;&lt;br&gt;Модель, которая получилась после подключения PS3 к мультимодальной LLM, авторы назвали VILA-HD. По их замерам, она значительно превосходит по качеству AnyRes и S2, используя при этом в 4,3 раза меньше токенов.&lt;br&gt;&lt;br&gt;&lt;em&gt;Разбор подготовил &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt; Егор Шестопалов&lt;/em&gt;&lt;br&gt;&lt;em&gt;&lt;br&gt;&lt;/em&gt;&lt;a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>Seedream 3.0 Technical Report</title><link>https://t.me/timeforcv/147</link><guid>https://t.me/timeforcv/147</guid><pubDate>Wed, 18 Jun 2025 08:02:58 +0000</pubDate><description>&lt;strong&gt;Seedream 3.0 Technical Report&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Сегодняшняя &lt;a href="https://arxiv.org/abs/2504.11346" rel="nofollow noopener noreferrer"&gt;статья&lt;/a&gt; — описание модели Seedream 3.0, которая генерирует изображения по текстовым запросам. Был момент, когда по замерам Artificial Analysis Arena она обогнала все существующие модели, сейчас — топ-2 модель, уступающая только OpenAI. Правда, с нестатзначимой разницей.&lt;br&gt;&lt;br&gt;Кажется, что создатели третьей версии Seedream проделали огромную техническую работу и потратили очень много человекочасов, разрабатывая свои модели. Статья вышла всего лишь через месяц после Seedream 2.0, так что её можно воспринимать как набор доработок к прошлой модели, уже неплохо показавшей себя.&lt;br&gt;&lt;br&gt;Обучая Seedream 3.0, авторы уделили много внимания специфике китайского языка — у многих моделей-конкурентов проблемы с рисованием иероглифов. В частности, обучающие датасеты Seedream обогатили набором объектов китайской культуры. Ещё одна интересная деталь: после первой стадии обучения на изображениях размером 256 пикселей, модель обучается уже на целом диапазоне разрешений — от 512 до 2048 пикселей. А чтобы выкидывать из обучающего датасета меньше картинок с дефектами и вотермарками, авторы просто маскируют в лоссе проблемные области.&lt;br&gt;&lt;br&gt;В статье упоминается, что авторы обучили собственный VAE, но деталей, к сожалению, нет. Диффузионный трансформер принимает на вход картинку и закодированный текст, но токены для них обрабатываются отдельными MLP. Собственная разработка авторов — расширение 2D RoPE, которое они назвали Scaling RoPE, позволяет генерировать изображения с размером, отличным от того, на чём обучали модель. Стабильность обучения обеспечивает QK-Norm.&lt;br&gt;&lt;br&gt;Текстовый энкодер дофайнтьюнили из LLM, тренируя её на парах текст-изображение. Так LLM лучше мэтчится с доменом картинок. Закодированные текстовые энкодеры она передаёт в диффузионный трансформер.&lt;br&gt;&lt;br&gt;Тексты, которые нужно зарендерить на картинке, обрабатывает ByT5 — модель работает на уровне Unicode. Не делит тексты на токены по несколько символов, а кодирует их как последовательность кодов Unicode, чтобы генерировать текст было проще. &lt;br&gt; &lt;br&gt;Кроме того, в Seedream 3.0 авторы использовали новую парадигму ускорения. Используя разнообразные техники, такие как квантование, консистентное зашумление и семплирование временных шагов с ранжированием по важности, они достигли существенного ускорения при сохранении качества изображения. А встроенный вывод изображений в высоком разрешении (до 2K), делает новую модель ещё более удобной и практичной. &lt;br&gt;&lt;br&gt;&lt;em&gt;Разбор подготовил &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt; Артём Конев&lt;br&gt;&lt;/em&gt;&lt;a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>Kimi-VL technical report</title><link>https://t.me/timeforcv/146</link><guid>https://t.me/timeforcv/146</guid><pubDate>Fri, 13 Jun 2025 09:06:13 +0000</pubDate><description>&lt;strong&gt;Kimi-VL technical report&lt;br&gt;&lt;/strong&gt;&lt;br&gt;Сегодня разбираем &lt;a href="https://arxiv.org/abs/2504.07491" rel="nofollow noopener noreferrer"&gt;статью&lt;/a&gt; про Kimi-VL — yet another VLM, интересная тем, что умеет понимать очень длинные контексты, активируя всего 2,8B параметров. Это не мешает ей получать результаты лучше, чем Qwen2.5-VL-7B, and Gemma-3-12B-IT и даже GPT-4o-mini на некоторых тасках.&lt;br&gt;&lt;br&gt;Kimi-VL под силу контексты размером в 128K токенов и работа с изображениями разного разрешения — для этого у неё под капотом специальный визуальный энкодер. Авторы говорят, что они разработали две версии нейросети: обычную и thinking, которая кроме всего вышеперечисленного справляется с reasoning — длинными рассуждениями. На картинке — сравнение Kimi-VL с другими популярными нейросетями: сколько параметров активируется на бенчмарке MathVision. &lt;br&gt;&lt;br&gt;Авторы считают, что будущее — за MoE и CoT (как у DeepSeek и других LLM), а плотная архитектура, которую использует большинство опенсорс-VLM (например, Qwen2.5-VL и Gemma-3), устарела. &lt;br&gt;&lt;br&gt;Kimi-VL, по их словам, догоняет по способностям LLM. На основе SigLIP-SO-400M они создали собственный визуальный энкодер — MoonViT. Он может обрабатывать картинки разного разрешения: по аналогии с текстовыми последовательностями разбирает их на батчи, вытягивает и превращает в 1D-векторы. Чем выше разрешение — тем больше векторов в последовательности. Каждый батч локализуют по ширине и по высоте. Энкодер и LLM соединяет двухслойный MLP. &lt;br&gt;  &lt;br&gt;Для претрейна используется много текстовых данных: судя по всему, именно это позволяет активировать меньше параметров для её работы. Само обучение состоит из нескольких частей: &lt;br&gt;&lt;br&gt;1. Предобучение энкодера ViT (2T + 0,1T токенов): MoonViT обучается работать с картинками на парах изображение+текст. &lt;br&gt;2. Joint Pre-training (1,4T токенов). Модель тренируется обрабатывать запросы на чисто текстовых данных.&lt;br&gt;3. Joint Cooldown (0,6T токенов). Оптимизация производительности модели: обучение на высококачественных языковых и мультимодальных наборах данных.&lt;br&gt;4. Joint Long-context (0,3T токенов). Увеличение длины контекста модели с 8K до 128K. Чтобы модель лучше понимала длинный контекст и одновременно хорошо работала с коротким, на каждом подэтапе этой стадии обучения авторы фильтруют и увеличивают соотношение длинных данных до 25%. &lt;br&gt;&lt;br&gt;Хотя текущая модель эффективно справляется со многими стандартными задачами, она всё ещё слишком мала для решения узкоспециализированных задач. Возможности рассуждений Kimi-VL ещё не достигли теоретического максимума, особенно для сложных задач, требующих многоступенчатых выводов или более глубокого контекстного понимания. Путь к преодолению этих сложностей — масштабирование модели и совершенствование алгоритмов обучения (в том числе обогащение и увеличение тренировочных датасетов). &lt;br&gt;&lt;br&gt;&lt;em&gt;Разбор подготовила &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt;  Дарья Виноградова&lt;br&gt;&lt;/em&gt;&lt;a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>No Pose, No Problem: Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images</title><link>https://t.me/timeforcv/144</link><guid>https://t.me/timeforcv/144</guid><pubDate>Tue, 10 Jun 2025 08:02:30 +0000</pubDate><description>&lt;strong&gt;No Pose, No Problem: Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images&lt;br&gt;&lt;/strong&gt;&lt;br&gt;Сегодня коротко разбираем работу &lt;a href="https://noposplat.github.io/" rel="nofollow noopener noreferrer"&gt;NoPoSplat&lt;/a&gt;, в которой предлагается метод 3D-реконструкции по RGB-изображениям без информации об их позах. Модель NoPoSplat выдаёт &lt;a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/" rel="nofollow noopener noreferrer"&gt;3D Gaussian Splatting&lt;/a&gt; (3DGS) облако, которое можно рендерить (novel view synthesis, NVS) и использовать для оценки относительного положения камер (relative pose estimation).&lt;br&gt;&lt;br&gt;Ключевое достижение статьи — демонстрация того, что простая архитектура, основанная на трансформерах (ViT, DPT), обученная исключительно с использованием фотометрических функций потерь, может решать задачи реконструкции за один прямой проход. Метод полагается на наличие внутренних параметров (intrinsics) камеры, однако обычно получить их легче, чем внешние (extrinsics).&lt;br&gt;&lt;br&gt;Это интересно потому, что традиционные методы 3D-реконструкции и синтеза изображений часто требуют большого числа изображений, информации о параметрах камер и полагаются на многоэтапные structure from motion-пайплайны. Если количество входных изображений ограничено — так называемый sparse view — возникает проблема плохого перекрытия контента. Методы, которые полагаются на геометрические прайоры, например, cost volumes (например, &lt;a href="https://arxiv.org/abs/2403.14627" rel="nofollow noopener noreferrer"&gt;MVSplat&lt;/a&gt;) или epipolar geometry (например, &lt;a href="https://arxiv.org/abs/2312.12337" rel="nofollow noopener noreferrer"&gt;PixelSplat&lt;/a&gt;), перестают работать. Знание поз камер — существенное ограничение для in-the-wild приложений, например, обработки user generated content.&lt;br&gt;&lt;br&gt;Архитектура модели «многобашенная» и состоит из трёх основных компонентов: ViT энкодера и декодера и DPT-голов, предсказывающих параметры 3DGS-облака, и повторяет широко известные &lt;a href="https://arxiv.org/abs/2312.14132" rel="nofollow noopener noreferrer"&gt;DUSt3R&lt;/a&gt; и &lt;a href="https://arxiv.org/abs/2406.09756" rel="nofollow noopener noreferrer"&gt;MASt3R&lt;/a&gt;. Веса энкодеров общие, а в декодерах применяется cross view attention.&lt;br&gt;&lt;br&gt;У модели две головы. Первая предсказывает центроиды гауссиан, а вторая — оставшиеся параметры: поворот, масштаб, цвет. Чтобы лучше предсказывать цвет, в модели есть RGB shortcut — вместе с токенами из декодера в голову &lt;a href="https://github.com/cvg/NoPoSplat/blob/9b04307ebe179d610c04208db8d69f7c3106d03b/src/model/encoder/heads/dpt_gs_head.py#L148" rel="nofollow noopener noreferrer"&gt;через свёртку пробрасывается&lt;/a&gt; патч из входного изображения. В качестве канонического пространства фиксируется система координат относительно первого входного изображения, и головы выдают параметры гауссиан в этой единой системе координат.&lt;br&gt;&lt;br&gt;Для решения проблемы неоднозначности масштаба NoPoSplat делают camera intrinsic embedding. Интринсики преобразуются в токен и конкатенируются в энкодере с токенами картиночных патчей. В статье рассматривается ещё два способа добавления интринсиков в модель, но они оказались немного хуже.&lt;br&gt;&lt;br&gt;Если описывать метод одним предложением, то можно сказать, что это MASt3R c примочками для предсказания 3DGS-облака.&lt;br&gt;&lt;br&gt;Обучение модели проводится с использованием MSE- и LPIPS-лоссов, то есть для супервизии используют только RGB-изображения. Обучаемая модель предсказывает параметры 3DGS по входным изображениям. Затем 3DGS отрисовывается дифференцируемым рендером в нескольких новых известных позах из обучающего датасета и рендеры сравниваются с GT-изображениями. Groundtruth-позы используются только для рендеринга в процессе обучения. Обучают на датасетах RealEstate10k, ACID и DL3DV. Они включают RGB-изображения, а положения камер оценены с помощью COLMAP.&lt;br&gt;&lt;br&gt;Модель может быть инициализирована случайно, но поскольку архитектура повторяет CroCoV2, DUSt3R и MASt3R, попробовали частично инициализировать веса из них и это дало лучшие результаты. Поскольку MASt3R был обучен на данных с GT-информацией о глубине, то нельзя сказать, что лучшая модель NoPoSplat обучена только на RGB-данных.&lt;br&gt;&lt;br&gt;Для решения задачи оценки относительной позы между входными изображениями сначала находят приближение с использованием PnP + RANSAC, затем &lt;a href="https://arxiv.org/abs/2312.06741" rel="nofollow noopener noreferrer"&gt;её уточняют, используя SSIM loss&lt;/a&gt; относительно предсказанного 3DGS-облака.&lt;br&gt;&lt;br&gt;Качество NVS зависит от количества картинок на входе и степени их взаимного пересечения, PSNR варьируется от 22 до 27. С одной стороны, не так уж много, а с другой — удивительно хорошо при такой постановке задачи.&lt;br&gt;&lt;br&gt;&lt;em&gt;Разбор подготовил &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt; Расим Ахунзянов&lt;br&gt;&lt;/em&gt;&lt;a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;&lt;br&gt;&lt;br&gt;#YaICLR</description></item><item><title>Improving the Diffusability of Autoencoders</title><link>https://t.me/timeforcv/143</link><guid>https://t.me/timeforcv/143</guid><pubDate>Wed, 04 Jun 2025 08:06:18 +0000</pubDate><description>&lt;strong&gt;Improving the Diffusability of Autoencoders&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Сегодня разбираем &lt;a href="https://arxiv.org/abs/2502.14831" rel="nofollow noopener noreferrer"&gt;статью&lt;/a&gt;, в которой обсуждается то, что авторы называют diffusability латентного пространства: насколько легко диффузионной модели учиться на латентах автоэнкодера.&lt;br&gt;&lt;br&gt;В латентных диффузионных моделях (например, Stable Diffusion) генерация происходит не в пикселях, а в сжатом представлении. Это ускоряет обучение, но вводит зависимость от свойств автоэнкодера. Обычно смотрят только на качество реконструкции: насколько хорошо декодер восстанавливает изображение. Но есть вторая характеристика — diffusability, и именно её авторы рассматривают в этой работе.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Что такое diffusability и почему это важно&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Если латенты имеют сложное распределение или содержат неинформативные шумовые компоненты, диффузии приходится подстраиваться под это распределение — обучаться дольше и потенциально упираться в потолок качества. Поэтому автоэнкодер задаёт не только качество реконструкции, но и удобство обучения вместе с последующей генерацией.&lt;br&gt;&lt;br&gt;Авторы смотрят на латенты от обычных автоэнкодеров и замечают, что они визуально шумные: в них много высокочастотных деталей, особенно в фоне. Чтобы разобраться, применяют дискретное косинусное преобразование (DCT), как в JPEG. Разбивают картинку или латент на блоки 8×8, считают DCT по каждому из них, усредняют спектры и строят частотный профиль.&lt;br&gt;&lt;br&gt;Выясняется, что латенты содержат больше высокочастотных компонентов, чем изображения, и это особенно заметно при увеличении числа каналов. Даже если латент визуально похож на картинку, его частотный профиль сильно отличается. А если обнулить высокие частоты и попробовать восстановить изображение, латент теряет качество гораздо сильнее, чем обычное изображение — там такие потери почти незаметны. Это говорит о том, что латенты слишком зависят от высокочастотной части  и не обладают масштабной эквивариантностью.&lt;br&gt;&lt;br&gt;Тогда авторы добавляют к лоссу автоэнкодера простую компоненту: берут исходное изображение и соответствующий латент, уменьшают их разрешение (в 2 или 4 раза), затем реконструируют картинку из сжатого латента и считают дополнительный лосс между даунскейленным изображением и полученной реконструкцией.&lt;br&gt;&lt;br&gt;Таким образом они обеспечивают соблюдения свойства масштабной инвариантности (потому что лосс буквально это и делает), что, в свою очередь, регуляризует латенты, убирая из них лишние высокие частоты. &lt;br&gt;&lt;br&gt;Результат — латенты становятся менее шумными, частотные профили ближе к тем, что у изображений. И, что важно, визуально структура латента сохраняется. Согласно метрикам, качество реконструкции почти не падает.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Эксперименты&lt;/strong&gt;&lt;br&gt;&lt;strong&gt;&lt;br&gt;&lt;/strong&gt;Метод протестировали на ImageNet-1K (изображения) и Kinetics-700 (видео). Сравнивали обучение диффузионной модели на обычных и исправленных латентах.&lt;br&gt;&lt;br&gt;В статье diffusability измеряют через скорость обучения: берут автоэнкодер, обучают на нём диффузионную модель и смотрят, насколько быстро растёт метрика качества (например, FID для изображений и FVD для видео). Сравнивались базовые модели и те же архитектуры, но обученные на автоэнкодерах с исходным и улучшенным diffusability. Оказалось, что последние учатся быстрее и дают лучшее финальное качество.&lt;br&gt;&lt;br&gt;Результаты:&lt;br&gt; — генерация изображений: FID улучшился на 19%;&lt;br&gt; — генерация видео: FVD улучшился на 44%;&lt;br&gt; — модели обучаются быстрее;&lt;br&gt; — PSNR немного растёт (за счёт блюра), но визуально картинки выглядят нормально.&lt;br&gt;&lt;br&gt;Визуализация того, как выглядят латенты до и после (&lt;em&gt;см. картинку&lt;/em&gt;), взята &lt;a href="https://arxiv.org/pdf/2502.09509" rel="nofollow noopener noreferrer"&gt;из другой работы&lt;/a&gt;, посвященной этой же теме: шум действительно уходит, но структура остаётся. Частотные кривые тоже приближаются к тем, что у изображений.&lt;br&gt;&lt;br&gt;В целом статья посвящена довольно локальной проблеме, но в ней есть понятная идея и измеримый эффект. &lt;br&gt;&lt;br&gt;&lt;em&gt;Разбор подготовил &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt; &lt;/em&gt;Сергей Кастрюлин&lt;br&gt;&lt;a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>Yandex Alchemist: открытый датасет для буста text-to-image генерации</title><link>https://t.me/timeforcv/140</link><guid>https://t.me/timeforcv/140</guid><pubDate>Tue, 27 May 2025 09:12:54 +0000</pubDate><description>&lt;strong&gt;Yandex Alchemist: открытый датасет для буста text-to-image генерации&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Раньше T2I-модели обучали в один этап — претрейн на большом, довольно грязном датасете интернет-данных. В 2023 году &lt;a href="https://arxiv.org/abs/2309.15807" rel="nofollow noopener noreferrer"&gt;Meta в техрепорте EMU&lt;/a&gt; предложили делать файнтюн на маленьком датасете исключительного качества и за счёт этого существенно бустить результат генерации. Правда, они ничего не сказали о том, как такой датасет собрать.&lt;br&gt;&lt;br&gt;Команда YandexART тоже занималась этой задачей, и сегодня мы делимся результатами своей работы — &lt;a href="https://huggingface.co/datasets/yandex/alchemist" rel="nofollow noopener noreferrer"&gt;датасетом Alchemist&lt;/a&gt;. Он состоит из 3 350 пар «картинка-текст» и имеет лицензию Apache 2.0, пользуйтесь.&lt;br&gt;&lt;br&gt;Alchemist сокращает дистанцию между крутыми потюненными закрытыми моделями и открытыми, для которых такой тюнинг недоступен. Ранее сообществу был доступен только пофильтрованный на эстетичность кусочек LAION и файнтюн-датасеты под узкий домен, например аниме или живопись. LAION часто не давал существенного прироста качества, а файнтюны под узкий домен ограничивали возможности генерации за его пределами.&lt;br&gt;&lt;br&gt;Ниже мы подробно рассказываем, как получить датасет уровня Alchemist, имея лишь сырой набор интернет-данных. Отметим, что весь пайплайн — про картинки. Мы считаем, что так правильно: тексты потом лучше сгенерировать синтетические. &lt;br&gt;&lt;br&gt;Итак, стартуя с датасета на 10 млрд примеров, мы выбрали картинки высокого разрешения без NSFW-контента и удалили те, что содержали вотермарки, имели низкое качество и были неэстетичны. Когда осталось примерно 300 млн изображений, дальнейшее выкручивание порогов фильтрации не помогало: модели недостаточно чувствительны, чтобы отделять хорошие картинки от великолепных. Выбирать руками лучшее из такого большого набора — тоже сомнительная затея. &lt;br&gt;&lt;br&gt;На этом этапе мы предположили, что предобученная диффузионка может сама знать, какие картинки хорошие, а какие — не очень. Пробовали подходы из области &lt;a href="https://arxiv.org/abs/2205.09329" rel="nofollow noopener noreferrer"&gt;dataset pruning&lt;/a&gt;, например, пропускать картинки через модель и смотреть на значение лосса. Оказалось, что так отбираются только самые простые изображения — абстрактные иллюстрации, вроде обоев на рабочий стол. В них немного деталей и их легко моделировать, но на файнтюне от них мало толку.&lt;br&gt;&lt;br&gt;В итоге нам пришлось придумать свой метод, суть которого в следующем. &lt;br&gt;&lt;br&gt;1. Возьмём 1000 картинок из наших 300 млн и разметим на условно плохие (LQ) и хорошие (HQ). Хорошими будем считать те, у которых высокие эстетичность и техническое качество, умеренная наполненность контентом. &lt;br&gt;2. Смастерим общий промт, который будет содержать перечисление желаемых характеристик: “aesthetic”, “high quality” и т. д. &lt;br&gt;3. Дальше будем брать LQ- и HQ-картинки, зашумлять их до какого-то t, подавать в нашу предобученую диффузионку вместе с промтом и смотреть, что происходит со значениями в cross-attention. &lt;br&gt;&lt;br&gt;Оказывается, что на основе нашей небольшой и грубой разметки можно выделить комбинации активаций в cross-attn и токенов, которые будут хорошо отделять изображения с нужными нам свойствами. Если просуммировать эти значения, получим скаляр, который и будет нашим скором качества изображения. Проскорив таким образом 300 млн картинок, мы выбрали топ-3350 — это картинки из нашего датасета. &lt;br&gt;&lt;br&gt;Дальше осталось сделать тексты — исходные из интернета могут быть ошибочны, содержать лишнюю или упускать нужную информацию. Наше наблюдение: лучше всего работают умеренно подробные промты, похожие на те, которые пишет скорее увлечённый пользователь, чем профессиональный промпт-инженер. YandexVLM как раз умеет подстраиваться под нужный формат. С её помощью мы сгенерировали тексты для каждой картинки, получив датасет Alchemist.&lt;br&gt;&lt;br&gt;Чтобы убедиться в обобщаемости датасета и метода, мы сделали и &lt;a href="https://huggingface.co/collections/yandex/alchemist-6825f7a16cbcc71128ee525f" rel="nofollow noopener noreferrer"&gt;выложили файнтюны&lt;/a&gt; SD 1.5, SD 2.1, SDXL-base 1.0, SD 3.5 Medium и Large. У всех файнтюнов растёт эстетичность и наполненность генераций, которую мы называем “image complexity”. Подробнее о методике и экспериментах &lt;a href="https://arxiv.org/abs/2505.19297" rel="nofollow noopener noreferrer"&gt;читайте в препринте&lt;/a&gt;. &lt;br&gt;&lt;br&gt;&lt;em&gt;Статью подготовили &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt; Валерий Старцев, Александр Устюжанин, Алексей Кириллов, Дмитрий Баранчук, Сергей Кастрюлин&lt;br&gt;&lt;/em&gt;&lt;br&gt;&lt;a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;&lt;br&gt;___&lt;br&gt;&lt;em&gt;Meta признана экстремистской организацией, а Facebook и Instagram запрещены на территории РФ&lt;/em&gt;</description></item><item><title>The Chosen One: Consistent Characters in Text-to-Image Diffusion Models</title><link>https://t.me/timeforcv/139</link><guid>https://t.me/timeforcv/139</guid><pubDate>Tue, 20 May 2025 08:04:31 +0000</pubDate><description>&lt;strong&gt;The Chosen One: Consistent Characters in Text-to-Image Diffusion Models&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Сегодня разбираем &lt;a href="https://arxiv.org/abs/2311.10093" rel="nofollow noopener noreferrer"&gt;статью&lt;/a&gt;, которая предлагает не самый практичный, но достаточно любопытный способ заставить генеративную модель выдавать одного и того же персонажа при разных промптах. Например, это важно для сторителлинга и комиксов, где герой должен сохранять идентичность во всех сценах.&lt;br&gt;&lt;br&gt;Основная идея статьи — добиться того, чтобы по одному текстовому промпту всегда генерировался один и тот же персонаж. При стандартной генерации «ёжика-альбиноса с фиолетовыми иголками» без подготовки получаются разные ёжики:  похожие, но отличающиеся в деталях. Обычно задачу решают через DreamBooth или текстовую инверсию на одной картинке, но это ведёт к жесткому переобучению и потере вариативности окружения.&lt;br&gt;&lt;br&gt;Авторы предлагают другой путь. Они не используют исходное изображение и работают только с текстом. Сначала генерируют 128 картинок по одному промпту (SDXL), затем извлекают эмбеддинги через DINOv2 и выполняют кластеризацию. Выбирают самый крупный и плотный кластер — там образ героя выглядит максимально однородно. На этом подмножестве проводят fine-tune модели с помощью LoRA и текстовой инверсии, после чего повторяют цикл генерации, кластеризации и обучения ещё четыре–пять раз. Процедура занимает около 24 минут на одной GPU.&lt;br&gt;&lt;br&gt;Так удаётся зафиксировать ключевые черты персонажа — цвет кожи, форму глаз, аксессуары и даже позу, хотя фон при этом остаётся неизменным. При смене промпта обучение придётся повторить: метод жёстко привязан к тексту.&lt;br&gt;&lt;br&gt;Сравнение с базовыми методами:&lt;br&gt;&lt;br&gt;- Vanilla Textual Inversion — образы слишком разнородны;&lt;br&gt;- DreamBooth full fine-tuning — модель переобучается на фон и перестаёт менять окружение;&lt;br&gt;- текстовая инверсия через LoRA: недообучается, даёт слабую консистентность.&lt;br&gt;&lt;br&gt;В итоге этот метод («Sauce») позволяет получить баланс между соответствием промту и стабильностью образа. Auto-метрика CLIP-Score и оценки на Amazon MTurk подтвердили, что согласованность растёт без серьёзных потерь в точности при сохранении разнообразия фонов и поз.&lt;br&gt;&lt;br&gt;Абляционный анализ показывает, что без кластеризации модели не сохраняют образ. Одна итерация обучения даёт малозаметный эффект, а при реинициализации весов каждую итерацию результаты ухудшаются. &lt;br&gt;&lt;br&gt;Метод совместим с другими техниками: при генерации истории из четырёх промптов герой остаётся постоянным; с ControlNet можно задать новую позу, сохранив лицо, а сочетание с DreamBooth и LoRA улучшает детализацию.&lt;br&gt;&lt;br&gt;Основные ограничения связаны с тем, что кластер может захватить фон или часто встречающиеся детали — котик может «прилипнуть» к листикам, а позы и окружение мешают выделить только лицо героя. Авторы предлагают предоставить пользователю выбор из нескольких кластеров.&lt;br&gt;&lt;br&gt;В перспективе авторы хотят расширить подход для работы с реальными фотографиями: сначала получить текстовое описание через captioning, затем применить тот же цикл генерации, кластеризации и дообучения.&lt;br&gt;&lt;br&gt;Немного технических деталей: 128 изображений, 500 шагов обучения с AdamW, порог плотности кластера — 0,8 от медианной дистанции с адаптивным подбором на первой итерации.&lt;br&gt;&lt;br&gt;В заключение можно подметить, что метод хоть и интересный, но на практике требует много времени и ресурсов, а результат всё же далёк от идеала. Но сама идея итеративной кластеризации и дообучения модели заслуживает внимания.&lt;br&gt;&lt;br&gt;&lt;em&gt;Разбор подготовил &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt; Григорий Лившиц&lt;/em&gt;&lt;br&gt;&lt;a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>HunyuanVideo: A Systematic Framework For Large Video Generative Models</title><link>https://t.me/timeforcv/137</link><guid>https://t.me/timeforcv/137</guid><pubDate>Mon, 12 May 2025 08:05:59 +0000</pubDate><description>&lt;strong&gt;HunyuanVideo: A Systematic Framework For Large Video Generative Models&lt;/strong&gt;&lt;br&gt;&lt;strong&gt;&lt;br&gt;&lt;/strong&gt;Сегодня разбираем &lt;a href="https://arxiv.org/abs/2412.03603" rel="nofollow noopener noreferrer"&gt;статью&lt;/a&gt; от команды Tencent о HunyuanVideo — большой генеративной модели для видео. Работа во многом напоминает &lt;a href="https://arxiv.org/abs/2410.13720" rel="nofollow noopener noreferrer"&gt;MovieGen&lt;/a&gt;, но есть некоторые важные отличия. А главное — веса модели выложены в открытый доступ, что редкость для видеомоделей.&lt;br&gt;&lt;br&gt;Обучение начинается с картинок 256×256, потом разрешение повышают до 512×512. При этом 256×256 всё ещё поддерживается — чтобы не терять навык генерации на этом уровне. Сначала учат только на изображениях, потом добавляют видео. &lt;br&gt;&lt;br&gt;Генерация стартует с нормального распределения, стандартного для диффузионок. Но вместо линейно-квадратичного расписания шагов из MovieGen, здесь применяется «сдвинутое» специальным образом расписание. Авторы говорят, что такой сдвиг даёт лучшее качество, чем квадратичное расписание, особенно при уменьшении количества шагов инференса.&lt;br&gt;&lt;br&gt;Видео для обучения берут из датасета WebVid. Чтобы сбалансировать данные, авторы находят 10 000 центроид и сэмплируют из них так, чтобы равномерно распределить количество примеров между центроидами. Если в одну центроиду попадает слишком много данных, часть отбрасывают. Так датасет получается разнообразнее.&lt;br&gt;&lt;br&gt;У модели несколько видов параллелизма: тензорный (делят слои и FF-блоки между GPU), контекстный (делят токены между процессами) и параллелизм по данным. Это помогает обрабатывать длинные последовательности, возникающие при генерации в высоком разрешении.&lt;br&gt;&lt;br&gt;Также модель поддерживает CFG и guidance distillation — учитель и ученик, как обычно. Ученик учится повторять учителя по результатам генерации. Для переписывания промптов используют Hunyuan Large Language Model — особенно если исходный текст слишком технический.&lt;br&gt;&lt;br&gt;Есть отдельная аудиомодель, которая по сгенерённому видео создаёт музыку. Она учится на спектрограммах и работает в духе AudioGen.&lt;br&gt;&lt;br&gt;Ещё есть возможности персонализации: можно подать референс-картинку и получить видео. Модель справляется с аватарами, движущимися портретами и анимацией объектов.&lt;br&gt;&lt;br&gt;Авторы собрали свой бенчмарк из 1533 промптов и сравнились с пятью сильными бейзлайнами. Публикуют не всё: выкладывают 600 промптов. Смотрят на соответствие тексту, движение, визуальное качество и общее впечатление. Их модель лидирует, но не с гигантским отрывом. Оценки FLOPs — без подробностей, так что сравнивать с другими моделями сложно.&lt;br&gt;&lt;br&gt;&lt;em&gt;Разбор подготовил &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt; Денис Кузнеделев&lt;/em&gt;&lt;br&gt;&lt;a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt;</description></item><item><title>Ещё немного этих мягких французских постеров с ICLR</title><link>https://t.me/timeforcv/129</link><guid>https://t.me/timeforcv/129</guid><pubDate>Mon, 28 Apr 2025 15:46:04 +0000</pubDate><description>&lt;strong&gt;Ещё немного этих &lt;del&gt;мягких французских&lt;/del&gt; постеров с ICLR&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Наши инженеры и исследователи продолжают делиться своими находками на тему CV — а мы несём их вам, чтобы обеспечить полезным чтением в короткую праздничную неделю.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2409.02574" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Solving Video Inverse Problems Using Image Diffusion Models&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;Авторы предлагают разбить генерацию видео с помощью диффузионных моделей на этапы: сначала покадровая генерация, затем синхронизация кадров по времени. Говорят, что получается быстрее и с хорошим качеством.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2412.11350" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Deep Random Features for Scalable Interpolation of Spatiotemporal Data&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;Работа напоминает NeRF, но для remote sensing данных. Орбитальные спутники не дают плотную картинку по пространству и времени, поэтому авторы предлагают научиться генерации по координатам «пространство-время», которые измерил бы спутник в этот момент.&lt;br&gt;&lt;br&gt;&lt;a href="https://openreview.net/forum?id=1KLBvrYz3V" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Century: A Framework and Dataset for Evaluating Ethical Contextualisation of Sensitive Images&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;Исследователи из DeepMind предлагают новый бенчмарк для оценки понимания моделями разных исторических событий, стратифицированных по типам связанных сущностей (люди, места и прочее) и по типу входных данных.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2407.02687" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;No Training, No Problem: Rethinking Classifier-Free Guidance for Diffusion Models&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;Пара годных хаков для улучшения Classifier-Free Guidance (CFG): &lt;br&gt;- unconditional-эмбеддинги можно заменить на рандомные текстовые токены;&lt;br&gt;- можно делать negative guidance на рандомные таймстемпы.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2410.22376" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;Rare-to-Frequent: Unlocking Compositional Generation Power of Diffusion Models on Rare Concepts with LLM Guidance&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;В этой работе помогают диффузионной модели лучше генерировать редкие концепты. Для этого с помощью LLM находят похожий, но более частый концепт и во время генерации используют информацию от обоих: редкого и частого.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2411.02780" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;How much is a noisy image worth? Data Scaling Laws for Ambient Diffusion&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;Авторы переформулируют лосс для зашумлённых изображений в диффузии, чтобы не отбрасывать данные и использовать их для обучения. Сейчас они готовят продолжение работы с разбором гиперпараметров.&lt;br&gt;&lt;strong&gt;&lt;br&gt;&lt;/strong&gt;&lt;a href="https://arxiv.org/abs/2403.08632" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;A Decade’s Battle on Dataset Bias: Are We There Yet?&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;Забавный факт: имея классификатор с 7 тысячами параметров, можно с высокой точностью определить, к какому датасету принадлежит фотография. Размер базы — более 3 миллиардов изображений.&lt;br&gt;&lt;br&gt;&lt;a href="https://arxiv.org/abs/2312.14091" rel="nofollow noopener noreferrer"&gt;&lt;strong&gt;HD-Painter: High-Resolution and Prompt-Faithful Text-Guided Image Inpainting with Diffusion Models&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;Работа от PicsArt с улучшением инпеинтинга. Решают проблему того, что диффузионка сильнее опирается на картинку, чем на промпт. Для этого «перевешивают» аттеншн-мапы в селф-аттеншн по аттеншн-мапам из кросс-аттеншна. Говорят, работает очень робастно.&lt;br&gt;&lt;br&gt;&lt;em&gt;Работы отобрали и прокомментировали &lt;/em&gt;&lt;em&gt;❣&lt;/em&gt;&lt;em&gt; Пётр Вытовтов, Алексей Спасёнов, Сергей Овчаренко, Александр Шишеня, Евгений Ляпустин, Иван Балашов&lt;/em&gt;&lt;br&gt;&lt;br&gt;&lt;a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer"&gt;CV Time &lt;/a&gt;&lt;br&gt;&lt;br&gt;#YaICLR</description></item><item><title>ICLR 2025 выходит на финишную прямую!</title><link>https://t.me/timeforcv/128</link><guid>https://t.me/timeforcv/128</guid><pubDate>Sun, 27 Apr 2025 08:05:47 +0000</pubDate><description>&lt;strong&gt;ICLR 2025 выходит на финишную прямую!&lt;br&gt;&lt;/strong&gt;&lt;br&gt;Мы внимательно следили за работами на конференции и собрали в одном посте все наши обзоры:&lt;br&gt;&lt;br&gt;- &lt;a href="https://t.me/timeforcv/95" rel="nofollow noopener noreferrer"&gt;Приветственный пост от ребят из CV-команды&lt;br&gt;-&lt;/a&gt; &lt;a href="https://t.me/timeforcv/98?single" rel="nofollow noopener noreferrer"&gt;Подборка интересных работ. Часть 1&lt;/a&gt;&lt;br&gt;- &lt;a href="https://t.me/timeforcv/106" rel="nofollow noopener noreferrer"&gt;Репортаж с первого Invited Talk &lt;/a&gt;&lt;br&gt;- &lt;a href="https://t.me/timeforcv/111" rel="nofollow noopener noreferrer"&gt;Немного атмосферных фото и видео&lt;/a&gt;&lt;br&gt;- &lt;a href="https://t.me/timeforcv/116" rel="nofollow noopener noreferrer"&gt;Подборка интересных работ. Часть 2&lt;/a&gt;&lt;br&gt;- &lt;a href="https://t.me/timeforcv/123" rel="nofollow noopener noreferrer"&gt;Подборка интересных работ. Часть 3&lt;/a&gt;&lt;br&gt;&lt;br&gt;Оставайтесь с нами, впереди более подробные разборы. А на видео — ещё немного Сингапура.&lt;br&gt;&lt;br&gt;&lt;em&gt;Больше разборов, интересных постеров, фото и видео с ICLR вы найдёте в наших других каналах: &lt;/em&gt;&lt;em&gt;@RecSysChannel&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;@MLunderhood&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;@stuffyNLP&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;@speechinfo&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;br&gt;&lt;br&gt;&lt;a href="https://t.me/+955SbPNeKC5kMTAy" rel="nofollow noopener noreferrer"&gt;CV Time&lt;/a&gt; &lt;br&gt;&lt;br&gt;#YaICLR</description></item></channel></rss>